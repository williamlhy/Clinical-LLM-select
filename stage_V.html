<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Hongyi Li">
	
	<title>Stage V | Clinical LLM selector</title>

	<link rel="shortcut icon" href="assets/images/gt_favicon.png">
	
	<!-- Bootstrap -->
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	<!-- Icon font -->
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	<!-- Fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	<!-- Custom styles -->
	<link rel="stylesheet" href="assets/css/styles.css">

	<!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
</head>
<body>

    <header id="header">
        <div id="head" class="parallax" parallax-speed="2">
            <h1 id="logo" class="text-center">
                <img class="img-circle" src="assets/images/me.jpg" alt="">
                <span class="title">Hongyi Li</span>
                <span class="tagline">Zhejiang University<br>
                    <a href="">12135029@zju.edu.cn</a></span>
            </h1>
        </div>
    
        <nav class="navbar navbar-default navbar-sticky">
            <div class="container-fluid">
                
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
                </div>
                
                <div class="navbar-collapse collapse">
                    
                    <ul class="nav navbar-nav">
                        <li class="active"><a href="index.html">Home</a></li>
                        <li><a href="about.html">About</a></li>
                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Clinical LLM selector <b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="intro.html">Introduction</a></li>
                                <li><a href="guideline.html">Guideline</a></li>
                                <li><a href="stage_I.html">Stage_I</a></li>
                                <li><a href="stage_II.html">Stage_II</a></li>
                                <li><a href="stage_III.html">Stage_III</a></li>
                                <li><a href="stage_IV.html">Stage_IV</a></li>
                                <li><a href="stage_V.html">Stage_V</a></li>
                            </ul>
                        </li>
                        <!-- <li><a href="blog.html">Blog</a></li> -->
                    </ul>
                
                </div><!--/.nav-collapse -->			
            </div>	
        </nav>
    </header>

<main id="main">
    <div class="container">
		<div class="row topspace">
			<div class="col-sm-8 col-sm-offset-2">
															
				<article class="post">
					<header class="entry-header">
						<div class="entry-meta"> 
							<!-- <span class="posted-on"><time class="entry-date published" date="2024-10-29">October 29, 2024</time></span>			 -->
						</div>
						<h1 class="entry-title"><a href="single.html" rel="bookmark">Clinical LLM srlector for stage V</a></h1>
					</header>
					<div class="entry-content">
						<p><b>Stage V: discharge and follow-up.</b>
                        <br>Next, answer the following questions to find the most suitable LLM for your use case. If an answer has an LLM followed by an `(*)' sign, this indicates that papers reported (not necessarily the same papers) that the LLM was the best performing LLM in the selected condition. e.g. there may be three different papers reporting the same LLM as the best performing model in three different clinical tasks, so when `Yes' is selected for all three clinical tasks, the answer will have the name of that LLM followed by the `(*)'. Those without the `(*)' are simply LLMs that satisfy the selected condition.<br>
                        <span style="color: blue;">Blue text</span> that appears in a question indicates that a specific explanation is available by clicking on that text. The explanation will appear at the bottom of the page and can be closed by clicking red `close' in the explanation.<br>
                        <span style="color: rgb(141, 40, 224);">Any links</span> present in the answer can be accessed by clicking on them.</p>
					</div>
                    <h5>Please answer the following questions on a case-by-case basis.</h5>
                    <!-- <div class="buttonContainer">
                        <button onclick="loadContent('LLM.html')">Return to the initial screen </button>
                    </div> -->
                
                    <div id="questionContainer"></div>
                    <div id="descriptionContainer">
                        <p id="descriptionText"></p>
                        <span id="closeButton">Close</span>
                    </div>
                    <p></p>
				</article>
			</div> 
		</div>


</main>

<footer id="footer">
	<div class="container">
		<div class="row">
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
						<a href="mailto:#">12135029@zju.edu.cn</a><br>
						<br>
						Zhejiang University, HangZhou, China
					</p>	
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
						<a href="https://github.com/williamlhy"><i class="fa fa-github fa-2"></i></a>
					</p>
				</div>
			</div>

			<!-- <div class="col-md-3 widget">
				<h3 class="widget-title">Text widget</h3>
				<div class="widget-body">
					<p>Require to fill</p>
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Form widget</h3>
				<div class="widget-body">
					<p>+234 23 9873237<br>
						<a href="mailto:#">some.email@somewhere.com</a><br>
						<br>
						234 Hidden Pond Road, Ashland City, TN 37015
					</p>	
				</div>
			</div> -->

		</div> <!-- /row of widgets -->
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">
			
			<div class="col-md-6 widget">
				<div class="widget-body">
					<p>Zhejiang University, HangZhou, China </p>
				</div>
			</div>

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2024, Hongyi Li</p>
				</div>
			</div>

		</div> <!-- /row of widgets -->
	</div>
</footer>


<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script>document.addEventListener('DOMContentLoaded', function () {
    const data = {
        "0": {
            "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "1",
            "no": "2"
        },
        "1": {
            "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "3",
            "no": "4"
        },
        "2": {
            "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "5",
            "no": "6"
        },
        "3": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "7",
            "no": "8"
        },
        "4": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "9",
            "no": "10"
        },
        "5": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "11",
            "no": "12"
        },
        "6": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "13",
            "no": "14"
        },
        "7": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "15",
            "no": "16"
        },
        "8": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "17",
            "no": "18"
        },
        "9": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "19",
            "no": "20"
        },
        "10": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "21",
            "no": "22"
        },
        "11": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "23",
            "no": "24"
        },
        "12": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "25",
            "no": "26"
        },
        "13": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "27",
            "no": "28"
        },
        "14": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "29",
            "no": "30"
        },
        "15": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "31",
            "no": "32"
        },
        "16": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "33",
            "no": "34"
        },
        "17": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "35",
            "no": "36"
        },
        "18": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "37",
            "no": "38"
        },
        "19": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "39",
            "no": "40"
        },
        "20": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "41",
            "no": "42"
        },
        "21": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "43",
            "no": "44"
        },
        "22": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "45",
            "no": "46"
        },
        "23": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "47",
            "no": "48"
        },
        "24": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "49",
            "no": "50"
        },
        "25": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "51",
            "no": "52"
        },
        "26": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "53",
            "no": "54"
        },
        "27": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "55",
            "no": "56"
        },
        "28": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "57",
            "no": "58"
        },
        "29": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "59",
            "no": "60"
        },
        "30": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "61",
            "no": "62"
        },
        "31": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "63",
            "no": "64"
        },
        "32": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "65",
            "no": "66"
        },
        "33": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "67",
            "no": "68"
        },
        "34": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "69",
            "no": "70"
        },
        "35": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "71",
            "no": "72"
        },
        "36": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "73",
            "no": "74"
        },
        "37": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "75",
            "no": "76"
        },
        "38": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "77",
            "no": "78"
        },
        "39": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "79",
            "no": "80"
        },
        "40": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "81",
            "no": "82"
        },
        "41": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "83",
            "no": "84"
        },
        "42": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "85",
            "no": "86"
        },
        "43": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "87",
            "no": "88"
        },
        "44": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "89",
            "no": "90"
        },
        "45": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "91",
            "no": "92"
        },
        "46": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "93",
            "no": "94"
        },
        "47": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "95",
            "no": "96"
        },
        "48": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "97",
            "no": "98"
        },
        "49": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "99",
            "no": "100"
        },
        "50": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "101",
            "no": "102"
        },
        "51": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "103",
            "no": "104"
        },
        "52": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "105",
            "no": "106"
        },
        "53": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "107",
            "no": "108"
        },
        "54": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "109",
            "no": "110"
        },
        "55": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "111",
            "no": "112"
        },
        "56": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "113",
            "no": "114"
        },
        "57": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "115",
            "no": "116"
        },
        "58": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "117",
            "no": "118"
        },
        "59": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "119",
            "no": "120"
        },
        "60": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "121",
            "no": "122"
        },
        "61": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "123",
            "no": "124"
        },
        "62": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>?",
            "yes": "125",
            "no": "126"
        },
        "63": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "127",
            "no": "128"
        },
        "64": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "129",
            "no": "130"
        },
        "65": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "131",
            "no": "132"
        },
        "66": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "133",
            "no": "134"
        },
        "67": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "135",
            "no": "136"
        },
        "68": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "137",
            "no": "138"
        },
        "69": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "139",
            "no": "140"
        },
        "70": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "141",
            "no": "142"
        },
        "71": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "143",
            "no": "144"
        },
        "72": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "145",
            "no": "146"
        },
        "73": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "147",
            "no": "148"
        },
        "74": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "149",
            "no": "150"
        },
        "75": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "151",
            "no": "152"
        },
        "76": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "153",
            "no": "154"
        },
        "77": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "155",
            "no": "156"
        },
        "78": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "157",
            "no": "158"
        },
        "79": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "159",
            "no": "160"
        },
        "80": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "161",
            "no": "162"
        },
        "81": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "163",
            "no": "164"
        },
        "82": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "165",
            "no": "166"
        },
        "83": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "167",
            "no": "168"
        },
        "84": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "169",
            "no": "170"
        },
        "85": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "171",
            "no": "172"
        },
        "86": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "173",
            "no": "174"
        },
        "87": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "175",
            "no": "176"
        },
        "88": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "177",
            "no": "178"
        },
        "89": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "179",
            "no": "180"
        },
        "90": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "181",
            "no": "182"
        },
        "91": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "183",
            "no": "184"
        },
        "92": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "185",
            "no": "186"
        },
        "93": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "187",
            "no": "188"
        },
        "94": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "189",
            "no": "190"
        },
        "95": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "191",
            "no": "192"
        },
        "96": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "193",
            "no": "194"
        },
        "97": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "195",
            "no": "196"
        },
        "98": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "197",
            "no": "198"
        },
        "99": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "199",
            "no": "200"
        },
        "100": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "201",
            "no": "202"
        },
        "101": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "203",
            "no": "204"
        },
        "102": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "205",
            "no": "206"
        },
        "103": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "207",
            "no": "208"
        },
        "104": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "209",
            "no": "210"
        },
        "105": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "211",
            "no": "212"
        },
        "106": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "213",
            "no": "214"
        },
        "107": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "215",
            "no": "216"
        },
        "108": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "217",
            "no": "218"
        },
        "109": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "219",
            "no": "220"
        },
        "110": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "221",
            "no": "222"
        },
        "111": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "223",
            "no": "224"
        },
        "112": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "225",
            "no": "226"
        },
        "113": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "227",
            "no": "228"
        },
        "114": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "229",
            "no": "230"
        },
        "115": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "231",
            "no": "232"
        },
        "116": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "233",
            "no": "234"
        },
        "117": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "235",
            "no": "236"
        },
        "118": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "237",
            "no": "238"
        },
        "119": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "239",
            "no": "240"
        },
        "120": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "241",
            "no": "242"
        },
        "121": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "243",
            "no": "244"
        },
        "122": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "245",
            "no": "246"
        },
        "123": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "247",
            "no": "248"
        },
        "124": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "249",
            "no": "250"
        },
        "125": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "251",
            "no": "252"
        },
        "126": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "253",
            "no": "254"
        },
        "127": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "128": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "257",
            "no": "258"
        },
        "129": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "130": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "261",
            "no": "262"
        },
        "131": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "132": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "265",
            "no": "266"
        },
        "133": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "134": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "269",
            "no": "270"
        },
        "135": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "271",
            "no": "272"
        },
        "136": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "273",
            "no": "274"
        },
        "137": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "275",
            "no": "276"
        },
        "138": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "277",
            "no": "278"
        },
        "139": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "279",
            "no": "280"
        },
        "140": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "281",
            "no": "282"
        },
        "141": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "283",
            "no": "284"
        },
        "142": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option"
        },
        "143": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "144": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "289",
            "no": "290"
        },
        "145": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "146": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "293",
            "no": "294"
        },
        "147": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "148": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "297",
            "no": "298"
        },
        "149": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "150": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "301",
            "no": "302"
        },
        "151": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "152": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "305",
            "no": "306"
        },
        "153": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "154": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "309",
            "no": "310"
        },
        "155": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "156": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "313",
            "no": "314"
        },
        "157": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "158": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "159": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "160": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "321",
            "no": "322"
        },
        "161": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "162": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "325",
            "no": "326"
        },
        "163": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "164": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "329",
            "no": "330"
        },
        "165": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "166": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "333",
            "no": "334"
        },
        "167": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "335",
            "no": "336"
        },
        "168": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "337",
            "no": "338"
        },
        "169": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "339",
            "no": "340"
        },
        "170": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "341",
            "no": "342"
        },
        "171": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "343",
            "no": "344"
        },
        "172": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "345",
            "no": "346"
        },
        "173": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "347",
            "no": "348"
        },
        "174": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "175": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "176": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "353",
            "no": "354"
        },
        "177": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "178": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "357",
            "no": "358"
        },
        "179": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "180": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "361",
            "no": "362"
        },
        "181": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "182": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "365",
            "no": "366"
        },
        "183": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "184": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "369",
            "no": "370"
        },
        "185": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "186": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "373",
            "no": "374"
        },
        "187": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "188": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "377",
            "no": "378"
        },
        "189": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "190": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "191": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "192": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "385",
            "no": "386"
        },
        "193": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "194": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "389",
            "no": "390"
        },
        "195": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "196": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "393",
            "no": "394"
        },
        "197": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "198": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "397",
            "no": "398"
        },
        "199": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "399",
            "no": "400"
        },
        "200": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "401",
            "no": "402"
        },
        "201": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "403",
            "no": "404"
        },
        "202": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "405",
            "no": "406"
        },
        "203": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "407",
            "no": "408"
        },
        "204": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "409",
            "no": "410"
        },
        "205": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "411",
            "no": "412"
        },
        "206": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "207": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "208": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "417",
            "no": "418"
        },
        "209": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "210": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "421",
            "no": "422"
        },
        "211": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "212": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "425",
            "no": "426"
        },
        "213": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "214": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "429",
            "no": "430"
        },
        "215": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "216": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "433",
            "no": "434"
        },
        "217": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "218": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "437",
            "no": "438"
        },
        "219": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "220": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "441",
            "no": "442"
        },
        "221": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "222": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "223": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "224": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "449",
            "no": "450"
        },
        "225": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "226": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "453",
            "no": "454"
        },
        "227": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "228": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "457",
            "no": "458"
        },
        "229": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "230": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "461",
            "no": "462"
        },
        "231": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "463",
            "no": "464"
        },
        "232": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "465",
            "no": "466"
        },
        "233": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "467",
            "no": "468"
        },
        "234": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "469",
            "no": "470"
        },
        "235": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "471",
            "no": "472"
        },
        "236": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "473",
            "no": "474"
        },
        "237": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "475",
            "no": "476"
        },
        "238": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "239": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "240": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "481",
            "no": "482"
        },
        "241": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "242": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "485",
            "no": "486"
        },
        "243": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "244": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "489",
            "no": "490"
        },
        "245": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "246": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "493",
            "no": "494"
        },
        "247": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "248": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "497",
            "no": "498"
        },
        "249": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "250": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "501",
            "no": "502"
        },
        "251": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "252": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "505",
            "no": "506"
        },
        "253": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "254": {
            "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option."
        },
        "257": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "258": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "261": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "262": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "265": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "266": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "269": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "270": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "271": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "272": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "273": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "274": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "275": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "276": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "277": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "278": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "279": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "280": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "281": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "282": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank' class='custom-link'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "283": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "284": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "289": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "290": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "293": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "294": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "297": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "298": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "301": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "302": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "305": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "306": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "309": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "310": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "313": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "314": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "321": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "322": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "325": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600"
        },
        "326": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "329": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "330": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "333": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "334": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "335": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "336": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "337": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "338": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "339": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "340": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "341": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "342": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "343": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "344": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "345": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "346": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank' class='custom-link'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "347": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "348": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "353": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "354": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "357": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "358": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "361": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "362": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "365": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "366": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "369": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "370": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "373": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "374": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "377": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "378": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "385": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "386": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "389": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "390": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "393": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "394": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "397": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "398": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "399": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "400": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "401": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "402": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "403": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "404": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "405": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "406": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "407": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "408": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "409": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "410": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank' class='custom-link'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "411": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "412": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "417": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "418": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "421": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "422": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "425": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "426": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "429": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "430": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "433": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "434": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "437": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "438": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "441": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "442": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "449": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "450": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "453": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "454": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "457": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "458": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "461": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "462": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "463": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "464": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "465": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "466": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "467": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "468": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "469": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "470": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "471": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "472": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "473": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "474": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank' class='custom-link'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "475": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "476": {
            "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "481": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "482": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "485": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "486": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "489": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "490": {
            "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "493": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "494": {
            "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "497": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "498": {
            "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "501": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300."
        },
        "502": {
            "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "505": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "506": {
            "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        }
    };
    const descriptions = {
        "textual data": "It refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
        "perform tasks related to disease prediction":"It refers to the ability to perform at least one similar task, such as the prediction of disease recurrence and prediction of disease diagnosis-related groups, etc.",
        "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
        "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
        "extract information from clinical text":"It refers to the ability to perform at least one similar task, such as extracting disease phenotypic information, disease feature information and other key information from clinical texts.",
        "answer medical questions":"It refers to open-ended questions related to hospital discharge. Open questions are questions that require a free-form text answer. These questions usually require more in-depth thinking and personalized answers rather than choosing from a limited number of options. These questions may pertain to discharge instructions, recovery recommendations, etc.",
        "generate discharge-related text":"The text may include discharge summaries, clinical records after de-identification, etc."
    };

    let currentNode = "0"; // Starting node
    let questionHistory = []; // Keeps track of answered questions

    // Function to display a question based on the node
    function displayQuestion(node, questionIndex) {
        const questionContainer = document.getElementById('questionContainer');
        // Clear questions beyond this index
        questionContainer.innerHTML = '';

        // Display history
        questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

        // Display the current question
        appendQuestion(node, null, questionIndex + 1);
    }

    function appendQuestion(node, selectedChoice, questionNumber) {
        const questionContainer = document.getElementById('questionContainer');

        if (data[node] && data[node].question) {
            const questionDiv = document.createElement('div');
            questionDiv.classList.add('question');

            // Display fixed question number
            const questionNumberElem = document.createElement('p');
            questionNumberElem.classList.add('questionNumber');
            questionNumberElem.innerText = `Question ${questionNumber}:`;
            questionDiv.appendChild(questionNumberElem);

            // Display question text (with clickable keywords)
            const questionText = document.createElement('p');
            questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
            questionDiv.appendChild(questionText);

            // Create Yes button
            const yesButton = document.createElement('button');
            yesButton.innerText = 'Yes';
            yesButton.classList.toggle('selected', selectedChoice === 'yes');
            yesButton.onclick = function () {
                updateHistory(node, 'yes', questionNumber);
                currentNode = data[node].yes;
                displayQuestion(currentNode, questionNumber);
            };
            questionDiv.appendChild(yesButton);

            // Create No button
            const noButton = document.createElement('button');
            noButton.innerText = 'No';
            noButton.classList.toggle('selected', selectedChoice === 'no');
            noButton.onclick = function () {
                updateHistory(node, 'no', questionNumber);
                currentNode = data[node].no;
                displayQuestion(currentNode, questionNumber);
            };
            questionDiv.appendChild(noButton);

            // Append the question block to the container
            questionContainer.appendChild(questionDiv);

            // Attach event listeners to keywords
            const highlightElements = questionDiv.querySelectorAll('.highlight');
            highlightElements.forEach(function (element) {
                element.addEventListener('click', function () {
                    const keyword = element.getAttribute('data-keyword');
                    showDescription(keyword);
                });
            });

        } else if (data[node] && data[node].answer) {
            // Create a div for the final answer
            const answerDiv = document.createElement('div');
            answerDiv.classList.add('answer');

            const answerText = document.createElement('p');
            answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
            answerDiv.appendChild(answerText);

            // Append the answer to the container
            questionContainer.appendChild(answerDiv);
        } else {
            // Handle if no more questions or no data found
            const noMoreQuestions = document.createElement('p');
            noMoreQuestions.innerText = "No more questions available.";
            questionContainer.appendChild(noMoreQuestions);
        }
    }

    function showDescription(keyword) {
        const descriptionText = descriptions[keyword];
        if (descriptionText) {
            document.getElementById('descriptionText').innerText = descriptionText;
            document.getElementById('descriptionContainer').style.display = 'block';
        }
    }

    document.getElementById('closeButton').addEventListener('click', function () {
        document.getElementById('descriptionContainer').style.display = 'none';
    });

    function updateHistory(node, choice, questionNumber) {
        // Check if we are revisiting a question and modify history accordingly
        const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
        if (historyIndex !== -1) {
            questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
        }

        // Add current question to history
        questionHistory.push({ node, choice, number: questionNumber });
    }

    // Start the first question
    displayQuestion(currentNode, 0);
});</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
</body>
</html>
