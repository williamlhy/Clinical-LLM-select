<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Container for the two-column layout -->
    <div class="container">
        <!-- Left Sidebar -->
        <aside class="sidebar">
            <nav>
                <ul>
                    <div style="display: flex;">
                        <img src="zju.png" width="100" height="100" style="flex: 1;" >
                        <img src="mochi.png" width="100" height="100" style="flex: 1;" >
                    </div>
                    <li><a href="#" onclick="loadContent('home')">Home</a>
                    <li><a href="#" onclick="loadContent('about.html')">About</a>
                    <li><a href="#" onclick="loadContent('projects.html')">Projects</a>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content" id="content">
            <!-- Default Content -->
            <h1>Introduction</h1>
            <p>Hello, I'm Hongyi Li. This is my personal website where I share projects I've been working on.</p>
        </main>
    </div>

    <!-- Footer Section -->
    <footer>
        <p>&copy; 2024 Your Name. All rights reserved.</p>
    </footer>

    <!-- <script src="scripts.js"></script> -->
     <script> // Function to load content dynamically
        function loadContent(page) {
            if (page === 'home') {
                document.getElementById('content').innerHTML = `
                    <h1>Welcome to My Website</h1>
                    <p>This is the home page. Click the links on the left to navigate to other sections.</p>
                `;
            } else {
                const xhr = new XMLHttpRequest();
                xhr.open('GET', page, true);
                
                xhr.onreadystatechange = function() {
                    if (xhr.readyState === 4) {
                        if (xhr.status === 200) {
                            document.getElementById('content').innerHTML = xhr.responseText;
                            
                            // Check if the loaded page is the Q&A page
                            if (page === 's2.html') {
                                // Initialize Q&A logic after loading the HTML
                                const script = document.createElement('script');
                                script.innerHTML = `

            var currentNode = "0"; // Starting node
            var questionHistory = []; // Keeps track of answered questions

            // Function to display a question based on the node
            function displayQuestion(node, questionIndex) {
                const questionContainer = document.getElementById('questionContainer');
                // Clear questions beyond this index
                questionContainer.innerHTML = '';

                // Display history
                questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

                // Display the current question
                appendQuestion(node, null, questionIndex + 1);
            }

            function appendQuestion(node, selectedChoice, questionNumber) {
                const questionContainer = document.getElementById('questionContainer');
                const data = {
                "0": {
                    "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "1",
                    "no": "2"
                },
                "1": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "3",
                    "no": "4"
                },
                "2": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "5",
                    "no": "6"
                },
                "3": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "7",
                    "no": "8"
                },
                "4": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "9",
                    "no": "10"
                },
                "5": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "11",
                    "no": "12"
                },
                "6": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "13",
                    "no": "14"
                },
                "7": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "15",
                    "no": "16"
                },
                "8": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "17",
                    "no": "18"
                },
                "9": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "19",
                    "no": "20"
                },
                "10": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "21",
                    "no": "22"
                },
                "11": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "23",
                    "no": "24"
                },
                "12": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "25",
                    "no": "26"
                },
                "13": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "27",
                    "no": "28"
                },
                "14": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to radiology'>generate text related to radiology</span>?",
                    "yes": "29",
                    "no": "30"
                },
                "15": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "31",
                    "no": "32"
                },
                "16": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "33",
                    "no": "34"
                },
                "17": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "35",
                    "no": "36"
                },
                "18": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "37",
                    "no": "38"
                },
                "19": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "39",
                    "no": "40"
                },
                "20": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "41",
                    "no": "42"
                },
                "21": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "43",
                    "no": "44"
                },
                "22": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "45",
                    "no": "46"
                },
                "23": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "47",
                    "no": "48"
                },
                "24": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "49",
                    "no": "50"
                },
                "25": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "51",
                    "no": "52"
                },
                "26": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "53",
                    "no": "54"
                },
                "27": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "55",
                    "no": "56"
                },
                "28": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "57",
                    "no": "58"
                },
                "29": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "59",
                    "no": "60"
                },
                "30": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize text related to radiology reports'>summarize text related to radiology reports</span>?",
                    "yes": "61",
                    "no": "62"
                },
                "31": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 31",
                    "yes": "63",
                    "no": "64"
                },
                "32": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 32",
                    "yes": "65",
                    "no": "66"
                },
                "33": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 33",
                    "yes": "67",
                    "no": "68"
                },
                "34": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 34"
                },
                "35": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 35",
                    "yes": "71",
                    "no": "72"
                },
                "36": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 36",
                    "yes": "73",
                    "no": "74"
                },
                "37": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 37",
                    "yes": "75",
                    "no": "76"
                },
                "38": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 38"
                },
                "39": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 39",
                    "yes": "79",
                    "no": "80"
                },
                "40": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 40",
                    "yes": "81",
                    "no": "82"
                },
                "41": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 41",
                    "yes": "83",
                    "no": "84"
                },
                "42": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 42"
                },
                "43": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 43",
                    "yes": "87",
                    "no": "88"
                },
                "44": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 44",
                    "yes": "89",
                    "no": "90"
                },
                "45": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 45",
                    "yes": "91",
                    "no": "92"
                },
                "46": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 46"
                },
                "47": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 47",
                    "yes": "95",
                    "no": "96"
                },
                "48": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 48",
                    "yes": "97",
                    "no": "98"
                },
                "49": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 49",
                    "yes": "99",
                    "no": "100"
                },
                "50": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 50",
                    "yes": "101",
                    "no": "102"
                },
                "51": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 51",
                    "yes": "103",
                    "no": "104"
                },
                "52": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 52",
                    "yes": "105",
                    "no": "106"
                },
                "53": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 53",
                    "yes": "107",
                    "no": "108"
                },
                "54": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 54",
                    "yes": "109",
                    "no": "110"
                },
                "55": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 55",
                    "yes": "111",
                    "no": "112"
                },
                "56": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 56",
                    "yes": "113",
                    "no": "114"
                },
                "57": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 57",
                    "yes": "115",
                    "no": "116"
                },
                "58": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 58",
                    "yes": "117",
                    "no": "118"
                },
                "59": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 59",
                    "yes": "119",
                    "no": "120"
                },
                "60": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 60",
                    "yes": "121",
                    "no": "122"
                },
                "61": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 61",
                    "yes": "123",
                    "no": "124"
                },
                "62": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='process medical images'>process medical images</span> 62",
                    "yes": "125",
                    "no": "126"
                },
                "63": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>."
                },
                "64": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, RaDialog-7B. Here are the required resources: <br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>."
                },
                "65": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>."
                },
                "66": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), Bing Chat (*), IT5-220M (*), RaDialog-7B (*), accGPT (*), Glass AI (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>accGPT: can be accessed via link <a href='https://github.com/maxrusse/accGPT' target='_blank'>accGPT GitHub</a>.<br><br>Glass AI: can be accessed via link <a href='https://glass.health' target='_blank'>glassAI GitHub</a>."
                },
                "67": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>."
                },
                "68": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>."
                },
                "71": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 71"
                },
                "72": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 72"
                },
                "73": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: is a LLM fine-tuned from T5-220M, and can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 73"
                },
                "74": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 74"
                },
                "75": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 75"
                },
                "76": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 76"
                },
                "79": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 79"
                },
                "80": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, RaDialog-7B. Here are the required resources: <br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 80"
                },
                "81": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: is a LLM fine-tuned from T5-220M, and can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 81"
                },
                "82": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), Bing Chat (*), IT5-220M (*), RaDialog-7B (*), accGPT (*), Glass AI (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>accGPT: can be accessed via link <a href='https://github.com/maxrusse/accGPT' target='_blank'>accGPT GitHub</a>.<br><br>Glass AI: can be accessed via link <a href='https://glass.health' target='_blank'>glassAI GitHub</a>. 82"
                },
                "83": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 83"
                },
                "84": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 84"
                },
                "87": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 87"
                },
                "88": {
                    "answer": "The LLM we recommend you use is: RaDialog-7B. Here are the required resources: <br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 88"
                },
                "89": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: is a LLM fine-tuned from T5-220M, and can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 89"
                },
                "90": {
                    "answer": "The LLMs we recommend you use are: IT5-220M (*), RaDialog-7B (*). Here are the required resources: <br><br>IT5-220M: is a LLM fine-tuned from T5-220M, and can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 90"
                },
                "91": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 91"
                },
                "92": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 92"
                },
                "95": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 95",
                    "yes": "191",
                    "no": "192"
                },
                "96": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 96",
                    "yes": "193",
                    "no": "194"
                },
                "97": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 97",
                    "yes": "195",
                    "no": "196"
                },
                "98": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 98",
                    "yes": "197",
                    "no": "198"
                },
                "99": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 99",
                    "yes": "199",
                    "no": "200"
                },
                "100": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 100",
                    "yes": "201",
                    "no": "202"
                },
                "101": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 101",
                    "yes": "203",
                    "no": "204"
                },
                "102": {
                    "answer": "Insufficient information to recommend LLMs 102"
                },
                "103": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 103",
                    "yes": "207",
                    "no": "208"
                },
                "104": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 104",
                    "yes": "209",
                    "no": "210"
                },
                "105": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 105",
                    "yes": "211",
                    "no": "212"
                },
                "106": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 106",
                    "yes": "213",
                    "no": "214"
                },
                "107": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 107",
                    "yes": "215",
                    "no": "216"
                },
                "108": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 108",
                    "yes": "217",
                    "no": "218"
                },
                "109": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 109",
                    "yes": "219",
                    "no": "220"
                },
                "110": {
                    "answer": "Insufficient information to recommend LLMs 110"
                },
                "111": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 111",
                    "yes": "223",
                    "no": "224"
                },
                "112": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 112",
                    "yes": "225",
                    "no": "226"
                },
                "113": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 113",
                    "yes": "227",
                    "no": "228"
                },
                "114": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 114",
                    "yes": "229",
                    "no": "230"
                },
                "115": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 115",
                    "yes": "231",
                    "no": "232"
                },
                "116": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 116",
                    "yes": "233",
                    "no": "234"
                },
                "117": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 117",
                    "yes": "235",
                    "no": "236"
                },
                "118": {
                    "answer": "Insufficient information to recommend LLMs 118"
                },
                "119": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 119",
                    "yes": "239",
                    "no": "240"
                },
                "120": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 120",
                    "yes": "241",
                    "no": "242"
                },
                "121": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 121",
                    "yes": "243",
                    "no": "244"
                },
                "122": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 122",
                    "yes": "245",
                    "no": "246"
                },
                "123": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 123",
                    "yes": "247",
                    "no": "248"
                },
                "124": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 124",
                    "yes": "249",
                    "no": "250"
                },
                "125": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 125",
                    "yes": "251",
                    "no": "252"
                },
                "126": {
                    "answer": "Insufficient information to recommend LLMs 126"
                },
                "191": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br>191"
                },
                "192": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 192"
                },
                "193": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 193"
                },
                "194": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), RaDialog-7B (*) , GPT-3.5, PaLM-E-84B. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 194"
                },
                "195": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 195"
                },
                "196": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 196"
                },
                "197": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The total thermal design power of the GPU is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 197"
                },
                "198": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GPT-4V (*), RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*), Google-Bard (*), Bing Chat (*), accGPT (*), Glass AI (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>accGPT: can be accessed via link <a href='https://github.com/maxrusse/accGPT' target='_blank'>accGPT GitHub</a>.<br><br>Glass AI: can be accessed via link <a href='https://glass.health' target='_blank'>glassAI GitHub</a>. 198"
                },
                "199": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 199"
                },
                "200": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 200"
                },
                "201": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 201"
                },
                "202": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 202"
                },
                "203": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 203"
                },
                "204": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 204"
                },
                "207": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 207"
                },
                "208": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 208"
                },
                "209": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 209"
                },
                "210": {
                    "answer": "The LLMs we recommend you use are:RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 210"
                },
                "211": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 211"
                },
                "212": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 212"
                },
                "213": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The total thermal design power of the GPU is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 213"
                },
                "214": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 214"
                },
                "215": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 215"
                },
                "216": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 216"
                },
                "217": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 217"
                },
                "218": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. 218"
                },
                "219": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 219"
                },
                "220": {
                    "answer": "The LLMs we recommend you use are: Gemini Ultra, PaLM-E-84B. Here are the required resources:<br><br>Gemini Ultra: available for individual users only at $19.99 per month.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 220"
                },
                "223": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 223"
                },
                "224": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 224"
                },
                "225": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 225"
                },
                "226": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), RaDialog-7B (*) , GPT-3.5, PaLM-E-84B. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 226"
                },
                "227": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 227"
                },
                "228": {
                    "answer": "The LLMs we recommend you use are: GPT-4V, PaLM-E-84B. Here are the required resources:<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 228"
                },
                "229": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The total thermal design power of the GPU is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 229"
                },
                "230": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GPT-4V (*), RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*), Google-Bard (*), Bing Chat (*), accGPT (*), Glass AI (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>accGPT: can be accessed via link <a href='https://github.com/maxrusse/accGPT' target='_blank'>accGPT GitHub</a>.<br><br>Glass AI: can be accessed via link <a href='https://glass.health' target='_blank'>glassAI GitHub</a>. 230"
                },
                "231": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 231"
                },
                "232": {
                    "answer": "The LLMs we recommend you use are: GPT-4V, PaLM-E-84B. Here are the required resources:<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 232"
                },
                "233": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 233"
                },
                "234": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 234"
                },
                "235": {
                    "answer": "The LLM we recommend you use is: OphGLM-6.2B (*), PaLM-E-84B. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 235"
                },
                "236": {
                    "answer": "The LLMs we recommend you use are: OphGLM-6.2B (*), Gemini Ultra, PaLM-E-84B, GPT-4V. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>Gemini Ultra: available for individual users only at $19.99 per month.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 236"
                },
                "239": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 239"
                },
                "240": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 240"
                },
                "241": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 241"
                },
                "242": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), PaLM-E-84B. Here are the required resources:<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 242"
                },
                "243": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 243"
                },
                "244": {
                    "answer": "The LLMs we recommend you use are: GPT-4V, PaLM-E-84B. Here are the required resources:<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 244"
                },
                "245": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The total thermal design power of the GPU is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 245"
                },
                "246": {
                    "answer": "The LLMs we recommend you use are: RaDialog-7B (*), RadFM-14B (*), CXR-LLAVA-7B (*), M3D-LaMed (*), IT5-220M (*). Here are the required resources:<br> <br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>CXR-LLAVA-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/ECOFRI/CXR_LLAVA' target='_blank'>CXRLLAVA GitHub</a>.<br><br>M3D-LaMed: pre-train requires 8 NVIDIA TESLA A100-80GB GPUS. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/BAAI-DCAI/M3D' target='_blank'>M3DLaMed GitHub</a>.<br><br>IT5-220M: can be accessed via link <a href='https://github.com/bmi-labmedinfo/radiology-qa-transformer' target='_blank'>IT5 GitHub</a>. 246"
                },
                "247": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 247"
                },
                "248": {
                    "answer": "The LLMs we recommend you use are: GPT-4V, PaLM-E-84B. Here are the required resources:<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 248"
                },
                "249": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 249"
                },
                "250": {
                    "answer": "The LLMs we recommend you use are: PEGASUS-568M (*), LLaMA2-7B (*), Flan-T5-770M (*), Clinical-T5-770M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), RaDialog-7B (*). Here are the required resources:<br><br>PEGASUS-568M: fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>It can be accessed via link <a href='https://github.com/xtie97/PET-Report-Summarization' target='_blank'>PEGASUS GitHub</a>.<br><br>LLaMA2-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Clinical-T5-770M: pre-train requires 1 TPU v3.8 cluster and the price is $1800.<br><br>Fine-tuning requires 1 NVIDIA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>. 250"
                },
                "251": {
                    "answer": "The LLM we recommend you use is: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. 251"
                },
                "252": {
                    "answer": "The LLMs we recommend you use are: PaLM-E-84B. Here are the required resources:<br><br>PaLM-E-84B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. 252"
                }
            };


                if (data[node] && data[node].question) {
                    const questionDiv = document.createElement('div');
                    questionDiv.classList.add('question');

                    // Display fixed question number
                    const questionNumberElem = document.createElement('p');
                    questionNumberElem.classList.add('questionNumber');
                    questionNumberElem.innerText = \`Question \${questionNumber}:\`;
                    questionDiv.appendChild(questionNumberElem);

                    // Display question text (with clickable keywords)
                    const questionText = document.createElement('p');
                    questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
                    questionDiv.appendChild(questionText);

                    // Create Yes button
                    const yesButton = document.createElement('button');
                    yesButton.innerText = 'Yes';
                    yesButton.classList.toggle('selected', selectedChoice === 'yes');
                    yesButton.onclick = function () {
                        updateHistory(node, 'yes', questionNumber);
                        currentNode = data[node].yes;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(yesButton);

                    // Create No button
                    const noButton = document.createElement('button');
                    noButton.innerText = 'No';
                    noButton.classList.toggle('selected', selectedChoice === 'no');
                    noButton.onclick = function () {
                        updateHistory(node, 'no', questionNumber);
                        currentNode = data[node].no;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(noButton);

                    // Append the question block to the container
                    questionContainer.appendChild(questionDiv);

                    // Attach event listeners to keywords
                    const highlightElements = questionDiv.querySelectorAll('.highlight');
                    highlightElements.forEach(function (element) {
                        element.addEventListener('click', function () {
                            const keyword = element.getAttribute('data-keyword');
                            showDescription(keyword);
                        });
                    });

                } else if (data[node] && data[node].answer) {
                    // Create a div for the final answer
                    const answerDiv = document.createElement('div');
                    answerDiv.classList.add('answer');

                    const answerText = document.createElement('p');
                    answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
                    answerDiv.appendChild(answerText);

                    // Append the answer to the container
                    questionContainer.appendChild(answerDiv);
                } else {
                    // Handle if no more questions or no data found
                    const noMoreQuestions = document.createElement('p');
                    noMoreQuestions.innerText = "No more questions available.";
                    questionContainer.appendChild(noMoreQuestions);
                }
            }

            function showDescription(keyword) {
            const descriptions = {
                "textual data": "Textual data refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
                "generate text related to radiology": "It refers to the ability to perform at least one similar task, such as generating precautions for radiology examination, generating the finding portion of a radiology report, converting free text into a standardized radiology report, radiology report error checking, radiology report simplification, etc.",
                "summarize text related to radiology reports":"It refers to the ability to perform at least one similar task, such as summarizing the impression portion of a radiology report, summarizing questions posed by the patient related to the radiology report, etc.",
                "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
                "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
                "process medical images":"It refers to the ability to perform at least one similar task, such as medical image classification, adding medical image captions and medical image segmentation."
            };
                const descriptionText = descriptions[keyword];
                if (descriptionText) {
                    document.getElementById('descriptionText').innerText = descriptionText;
                    document.getElementById('descriptionContainer').style.display = 'block';
                }
            }

            document.getElementById('closeButton').addEventListener('click', function () {
                document.getElementById('descriptionContainer').style.display = 'none';
            });

            function updateHistory(node, choice, questionNumber) {
                // Check if we are revisiting a question and modify history accordingly
                const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
                if (historyIndex !== -1) {
                    questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
                }

                // Add current question to history
                questionHistory.push({ node, choice, number: questionNumber });
            }

            // Start the first question
            displayQuestion(currentNode, 0);
                                `;
                                document.body.appendChild(script);
                            } else if (page === 's3.html') {
                                // Initialize Q&A logic after loading the HTML
                                const script = document.createElement('script');
                                script.innerHTML = `

            var currentNode = "0"; // Starting node
            var questionHistory = []; // Keeps track of answered questions

            // Function to display a question based on the node
            function displayQuestion(node, questionIndex) {
                const questionContainer = document.getElementById('questionContainer');
                // Clear questions beyond this index
                questionContainer.innerHTML = '';

                // Display history
                questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

                // Display the current question
                appendQuestion(node, null, questionIndex + 1);
            }

            function appendQuestion(node, selectedChoice, questionNumber) {
                const questionContainer = document.getElementById('questionContainer');
                const data = {
                "0": {
                    "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "1",
                    "no": "2"
                },
                "1": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "3",
                    "no": "4"
                },
                "2": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "5",
                    "no": "6"
                },
                "3": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "7",
                    "no": "8"
                },
                "4": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "9",
                    "no": "10"
                },
                "5": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "11",
                    "no": "12"
                },
                "6": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "13",
                    "no": "14"
                },
                "7": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 7",
                    "yes": "15",
                    "no": "16"
                },
                "8": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 8",
                    "yes": "17",
                    "no": "18"
                },
                "9": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 9",
                    "yes": "19",
                    "no": "20"
                },
                "10": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 10",
                    "yes": "21",
                    "no": "22"
                },
                "11": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 11",
                    "yes": "23",
                    "no": "24"
                },
                "12": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 12",
                    "yes": "25",
                    "no": "26"
                },
                "13": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 13",
                    "yes": "27",
                    "no": "28"
                },
                "14": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease diagnosis'>perform tasks related to disease diagnosis</span>? 14",
                    "yes": "29",
                    "no": "30"
                },
                "15": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 15",
                    "yes": "31",
                    "no": "32"
                },
                "16": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 16",
                    "yes": "33",
                    "no": "34"
                },
                "17": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 17",
                    "yes": "35",
                    "no": "36"
                },
                "18": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 18",
                    "yes": "37",
                    "no": "38"
                },
                "19": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 19",
                    "yes": "39",
                    "no": "40"
                },
                "20": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 20",
                    "yes": "41",
                    "no": "42"
                },
                "21": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 21",
                    "yes": "43",
                    "no": "44"
                },
                "22": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 22",
                    "yes": "45",
                    "no": "46"
                },
                "23": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 23",
                    "yes": "47",
                    "no": "48"
                },
                "24": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 24",
                    "yes": "49",
                    "no": "50"
                },
                "25": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 25",
                    "yes": "51",
                    "no": "52"
                },
                "26": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 26",
                    "yes": "53",
                    "no": "54"
                },
                "27": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 27",
                    "yes": "55",
                    "no": "56"
                },
                "28": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 28",
                    "yes": "57",
                    "no": "58"
                },
                "29": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 29",
                    "yes": "59",
                    "no": "60"
                },
                "30": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract medical information'>extract medical information</span>? 30",
                    "yes": "61",
                    "no": "62"
                },
                "31": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 31",
                    "yes": "63",
                    "no": "64"
                },
                "32": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 32",
                    "yes": "65",
                    "no": "66"
                },
                "33": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 33",
                    "yes": "67",
                    "no": "68"
                },
                "34": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 34",
                    "yes": "69",
                    "no": "70"
                },
                "35": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 35",
                    "yes": "71",
                    "no": "72"
                },
                "36": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 36",
                    "yes": "73",
                    "no": "74"
                },
                "37": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 37",
                    "yes": "75",
                    "no": "76"
                },
                "38": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 38",
                    "yes": "77",
                    "no": "78"
                },
                "39": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 39",
                    "yes": "79",
                    "no": "80"
                },
                "40": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 40",
                    "yes": "81",
                    "no": "82"
                },
                "41": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 41",
                    "yes": "83",
                    "no": "84"
                },
                "42": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 42",
                    "yes": "85",
                    "no": "86"
                },
                "43": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 43",
                    "yes": "87",
                    "no": "88"
                },
                "44": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 44",
                    "yes": "89",
                    "no": "90"
                },
                "45": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 45",
                    "yes": "91",
                    "no": "92"
                },
                "46": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 46",
                    "yes": "93",
                    "no": "94"
                },
                "47": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 47",
                    "yes": "95",
                    "no": "96"
                },
                "48": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 48",
                    "yes": "97",
                    "no": "98"
                },
                "49": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 49",
                    "yes": "99",
                    "no": "100"
                },
                "50": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 50",
                    "yes": "101",
                    "no": "102"
                },
                "51": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 51",
                    "yes": "103",
                    "no": "104"
                },
                "52": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 52",
                    "yes": "105",
                    "no": "106"
                },
                "53": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 53",
                    "yes": "107",
                    "no": "108"
                },
                "54": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 54",
                    "yes": "109",
                    "no": "110"
                },
                "55": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 55",
                    "yes": "111",
                    "no": "112"
                },
                "56": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 56",
                    "yes": "113",
                    "no": "114"
                },
                "57": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 57",
                    "yes": "115",
                    "no": "116"
                },
                "58": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 58",
                    "yes": "117",
                    "no": "118"
                },
                "59": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 59",
                    "yes": "119",
                    "no": "120"
                },
                "60": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 60",
                    "yes": "121",
                    "no": "122"
                },
                "61": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 61",
                    "yes": "123",
                    "no": "124"
                },
                "62": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 62",
                    "yes": "125",
                    "no": "126"
                },
                "63": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 63",
                    "yes": "127",
                    "no": "128"
                },
                "64": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 64",
                    "yes": "129",
                    "no": "130"
                },
                "65": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 65",
                    "yes": "131",
                    "no": "132"
                },
                "66": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 66",
                    "yes": "133",
                    "no": "134"
                },
                "67": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 67",
                    "yes": "135",
                    "no": "136"
                },
                "68": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 68",
                    "yes": "137",
                    "no": "138"
                },
                "69": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 69",
                    "yes": "139",
                    "no": "140"
                },
                "70": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 70",
                    "yes": "141",
                    "no": "142"
                },
                "71": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 71",
                    "yes": "143",
                    "no": "144"
                },
                "72": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 72",
                    "yes": "145",
                    "no": "146"
                },
                "73": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 73",
                    "yes": "147",
                    "no": "148"
                },
                "74": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 74",
                    "yes": "149",
                    "no": "150"
                },
                "75": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 75",
                    "yes": "151",
                    "no": "152"
                },
                "76": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 76",
                    "yes": "153",
                    "no": "154"
                },
                "77": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 77",
                    "yes": "155",
                    "no": "156"
                },
                "78": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 78",
                    "yes": "157",
                    "no": "158"
                },
                "79": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 79",
                    "yes": "159",
                    "no": "160"
                },
                "80": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 80",
                    "yes": "161",
                    "no": "162"
                },
                "81": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 81",
                    "yes": "163",
                    "no": "164"
                },
                "82": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 82",
                    "yes": "165",
                    "no": "166"
                },
                "83": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 83",
                    "yes": "167",
                    "no": "168"
                },
                "84": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 84",
                    "yes": "169",
                    "no": "170"
                },
                "85": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 85",
                    "yes": "171",
                    "no": "172"
                },
                "86": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 86",
                    "yes": "173",
                    "no": "174"
                },
                "87": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 87",
                    "yes": "175",
                    "no": "176"
                },
                "88": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 88",
                    "yes": "177",
                    "no": "178"
                },
                "89": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 89",
                    "yes": "179",
                    "no": "180"
                },
                "90": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 90",
                    "yes": "181",
                    "no": "182"
                },
                "91": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 91",
                    "yes": "183",
                    "no": "184"
                },
                "92": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 92",
                    "yes": "185",
                    "no": "186"
                },
                "93": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 93",
                    "yes": "187",
                    "no": "188"
                },
                "94": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 94",
                    "yes": "189",
                    "no": "190"
                },
                "95": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 95",
                    "yes": "191",
                    "no": "192"
                },
                "96": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 96",
                    "yes": "193",
                    "no": "194"
                },
                "97": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 97",
                    "yes": "195",
                    "no": "196"
                },
                "98": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 98",
                    "yes": "197",
                    "no": "198"
                },
                "99": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 99",
                    "yes": "199",
                    "no": "200"
                },
                "100": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 100",
                    "yes": "201",
                    "no": "202"
                },
                "101": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 101",
                    "yes": "203",
                    "no": "204"
                },
                "102": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 102",
                    "yes": "205",
                    "no": "206"
                },
                "103": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 103",
                    "yes": "207",
                    "no": "208"
                },
                "104": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 104",
                    "yes": "209",
                    "no": "210"
                },
                "105": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 105",
                    "yes": "211",
                    "no": "212"
                },
                "106": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 106",
                    "yes": "213",
                    "no": "214"
                },
                "107": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 107",
                    "yes": "215",
                    "no": "216"
                },
                "108": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 108",
                    "yes": "217",
                    "no": "218"
                },
                "109": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 109",
                    "yes": "219",
                    "no": "220"
                },
                "110": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 110",
                    "yes": "221",
                    "no": "222"
                },
                "111": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 111",
                    "yes": "223",
                    "no": "224"
                },
                "112": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 112",
                    "yes": "225",
                    "no": "226"
                },
                "113": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 113",
                    "yes": "227",
                    "no": "228"
                },
                "114": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 114",
                    "yes": "229",
                    "no": "230"
                },
                "115": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 115",
                    "yes": "231",
                    "no": "232"
                },
                "116": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 116",
                    "yes": "233",
                    "no": "234"
                },
                "117": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 117",
                    "yes": "235",
                    "no": "236"
                },
                "118": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 118",
                    "yes": "237",
                    "no": "238"
                },
                "119": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 119",
                    "yes": "239",
                    "no": "240"
                },
                "120": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 120",
                    "yes": "241",
                    "no": "242"
                },
                "121": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 121",
                    "yes": "243",
                    "no": "244"
                },
                "122": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 122",
                    "yes": "245",
                    "no": "246"
                },
                "123": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 123",
                    "yes": "247",
                    "no": "248"
                },
                "124": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 124",
                    "yes": "249",
                    "no": "250"
                },
                "125": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 125",
                    "yes": "251",
                    "no": "252"
                },
                "126": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate text related to medical decision making'>generate text related to medical decision making</span>? 126",
                    "yes": "253",
                    "no": "254"
                },
                "127": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 127",
                    "yes": "255",
                    "no": "256"
                },
                "128": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 128",
                    "yes": "257",
                    "no": "258"
                },
                "129": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 129",
                    "yes": "259",
                    "no": "260"
                },
                "130": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 130",
                    "yes": "261",
                    "no": "262"
                },
                "131": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 131",
                    "yes": "263",
                    "no": "264"
                },
                "132": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 132",
                    "yes": "265",
                    "no": "266"
                },
                "133": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 133",
                    "yes": "267",
                    "no": "268"
                },
                "134": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 134",
                    "yes": "269",
                    "no": "270"
                },
                "135": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 135",
                    "yes": "271",
                    "no": "272"
                },
                "136": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 136",
                    "yes": "273",
                    "no": "274"
                },
                "137": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 137",
                    "yes": "275",
                    "no": "276"
                },
                "138": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 138",
                    "yes": "277",
                    "no": "278"
                },
                "139": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 139",
                    "yes": "279",
                    "no": "280"
                },
                "140": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 140",
                    "yes": "281",
                    "no": "282"
                },
                "141": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 141",
                    "yes": "283",
                    "no": "284"
                },
                "142": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 142"
                },
                "143": {
                    "answer": "Sorry, no matching LLMs were found. 143"
                },
                "144": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 144",
                    "yes": "289",
                    "no": "290"
                },
                "145": {
                    "answer": "Sorry, no matching LLMs were found. 145"
                },
                "146": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 146",
                    "yes": "293",
                    "no": "294"
                },
                "147": {
                    "answer": "Sorry, no matching LLMs were found. 147"
                },
                "148": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 148",
                    "yes": "297",
                    "no": "298"
                },
                "149": {
                    "answer": "Sorry, no matching LLMs were found. 149"
                },
                "150": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 150",
                    "yes": "301",
                    "no": "302"
                },
                "151": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 151",
                    "yes": "303",
                    "no": "304"
                },
                "152": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 152",
                    "yes": "305",
                    "no": "306"
                },
                "153": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 153",
                    "yes": "307",
                    "no": "308"
                },
                "154": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 154",
                    "yes": "309",
                    "no": "310"
                },
                "155": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 155",
                    "yes": "311",
                    "no": "312"
                },
                "156": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 156",
                    "yes": "313",
                    "no": "314"
                },
                "157": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 157",
                    "yes": "315",
                    "no": "316"
                },
                "158": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 158"
                },
                "159": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 159",
                    "yes": "319",
                    "no": "320"
                },
                "160": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 160",
                    "yes": "321",
                    "no": "322"
                },
                "161": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 161",
                    "yes": "323",
                    "no": "324"
                },
                "162": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 162",
                    "yes": "325",
                    "no": "326"
                },
                "163": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 163",
                    "yes": "327",
                    "no": "328"
                },
                "164": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 164",
                    "yes": "329",
                    "no": "330"
                },
                "165": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 165",
                    "yes": "331",
                    "no": "332"
                },
                "166": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 166",
                    "yes": "333",
                    "no": "334"
                },
                "167": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 167",
                    "yes": "335",
                    "no": "336"
                },
                "168": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 168",
                    "yes": "337",
                    "no": "338"
                },
                "169": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 169",
                    "yes": "339",
                    "no": "340"
                },
                "170": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 170",
                    "yes": "341",
                    "no": "342"
                },
                "171": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 171",
                    "yes": "343",
                    "no": "344"
                },
                "172": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 172",
                    "yes": "345",
                    "no": "346"
                },
                "173": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 173",
                    "yes": "347",
                    "no": "348"
                },
                "174": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 174"
                },
                "175": {
                    "answer": "Sorry, no matching LLMs were found. 175"
                },
                "176": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 176",
                    "yes": "353",
                    "no": "354"
                },
                "177": {
                    "answer": "Sorry, no matching LLMs were found. 177"
                },
                "178": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 178",
                    "yes": "357",
                    "no": "358"
                },
                "179": {
                    "answer": "Sorry, no matching LLMs were found. 179"
                },
                "180": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 180",
                    "yes": "361",
                    "no": "362"
                },
                "181": {
                    "answer": "Sorry, no matching LLMs were found. 181"
                },
                "182": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 182",
                    "yes": "365",
                    "no": "366"
                },
                "183": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 183",
                    "yes": "367",
                    "no": "368"
                },
                "184": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 184",
                    "yes": "369",
                    "no": "370"
                },
                "185": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 185",
                    "yes": "371",
                    "no": "372"
                },
                "186": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 186",
                    "yes": "373",
                    "no": "374"
                },
                "187": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 187",
                    "yes": "375",
                    "no": "376"
                },
                "188": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 188",
                    "yes": "377",
                    "no": "378"
                },
                "189": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 189",
                    "yes": "379",
                    "no": "380"
                },
                "190": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 190"
                },
                "191": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 191",
                    "yes": "383",
                    "no": "384"
                },
                "192": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 192",
                    "yes": "385",
                    "no": "386"
                },
                "193": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 193",
                    "yes": "387",
                    "no": "388"
                },
                "194": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 194",
                    "yes": "389",
                    "no": "390"
                },
                "195": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 195",
                    "yes": "391",
                    "no": "392"
                },
                "196": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 196",
                    "yes": "393",
                    "no": "394"
                },
                "197": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 197",
                    "yes": "395",
                    "no": "396"
                },
                "198": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 198",
                    "yes": "397",
                    "no": "398"
                },
                "199": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 199",
                    "yes": "399",
                    "no": "400"
                },
                "200": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 200",
                    "yes": "401",
                    "no": "402"
                },
                "201": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 201",
                    "yes": "403",
                    "no": "404"
                },
                "202": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 202",
                    "yes": "405",
                    "no": "406"
                },
                "203": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 203",
                    "yes": "407",
                    "no": "408"
                },
                "204": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 204",
                    "yes": "409",
                    "no": "410"
                },
                "205": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 205",
                    "yes": "411",
                    "no": "412"
                },
                "206": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 206"
                },
                "207": {
                    "answer": "Sorry, no matching LLMs were found. 207"
                },
                "208": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 208",
                    "yes": "417",
                    "no": "418"
                },
                "209": {
                    "answer": "Sorry, no matching LLMs were found. 209"
                },
                "210": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 210",
                    "yes": "421",
                    "no": "422"
                },
                "211": {
                    "answer": "Sorry, no matching LLMs were found. 211"
                },
                "212": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 212",
                    "yes": "425",
                    "no": "426"
                },
                "213": {
                    "answer": "Sorry, no matching LLMs were found. 213"
                },
                "214": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 214",
                    "yes": "429",
                    "no": "430"
                },
                "215": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 215",
                    "yes": "431",
                    "no": "432"
                },
                "216": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 216",
                    "yes": "433",
                    "no": "434"
                },
                "217": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 217",
                    "yes": "435",
                    "no": "436"
                },
                "218": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 218",
                    "yes": "437",
                    "no": "438"
                },
                "219": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 219",
                    "yes": "439",
                    "no": "440"
                },
                "220": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 220",
                    "yes": "441",
                    "no": "442"
                },
                "221": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 221",
                    "yes": "443",
                    "no": "444"
                },
                "222": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 222"
                },
                "223": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 223",
                    "yes": "447",
                    "no": "448"
                },
                "224": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 224",
                    "yes": "449",
                    "no": "450"
                },
                "225": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 225",
                    "yes": "451",
                    "no": "452"
                },
                "226": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 226",
                    "yes": "453",
                    "no": "454"
                },
                "227": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 227",
                    "yes": "455",
                    "no": "456"
                },
                "228": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 228",
                    "yes": "457",
                    "no": "458"
                },
                "229": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 229",
                    "yes": "459",
                    "no": "460"
                },
                "230": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 230",
                    "yes": "461",
                    "no": "462"
                },
                "231": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 231",
                    "yes": "463",
                    "no": "464"
                },
                "232": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 232",
                    "yes": "465",
                    "no": "466"
                },
                "233": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 233",
                    "yes": "467",
                    "no": "468"
                },
                "234": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 234",
                    "yes": "469",
                    "no": "470"
                },
                "235": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 235",
                    "yes": "471",
                    "no": "472"
                },
                "236": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 236",
                    "yes": "473",
                    "no": "474"
                },
                "237": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 237",
                    "yes": "475",
                    "no": "476"
                },
                "238": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 238"
                },
                "239": {
                    "answer": "Sorry, no matching LLMs were found. 239"
                },
                "240": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 240",
                    "yes": "481",
                    "no": "482"
                },
                "241": {
                    "answer": "Sorry, no matching LLMs were found. 241"
                },
                "242": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 242",
                    "yes": "485",
                    "no": "486"
                },
                "243": {
                    "answer": "Sorry, no matching LLMs were found. 243"
                },
                "244": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 244",
                    "yes": "489",
                    "no": "490"
                },
                "245": {
                    "answer": "Sorry, no matching LLMs were found. 245"
                },
                "246": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 246",
                    "yes": "493",
                    "no": "494"
                },
                "247": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 247",
                    "yes": "495",
                    "no": "496"
                },
                "248": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 248",
                    "yes": "497",
                    "no": "498"
                },
                "249": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 249",
                    "yes": "499",
                    "no": "500"
                },
                "250": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 250",
                    "yes": "501",
                    "no": "502"
                },
                "251": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 251",
                    "yes": "503",
                    "no": "504"
                },
                "252": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 252",
                    "yes": "505",
                    "no": "506"
                },
                "253": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 253",
                    "yes": "507",
                    "no": "508"
                },
                "254": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 254"
                },
                "255": {
                    "answer": "Sorry, no matching LLMs were found. 255"
                },
                "256": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 256"
                },
                "257": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 257"
                },
                "258": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), Google Bard, GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br> <br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 258"
                },
                "259": {
                    "answer": "Sorry, no matching LLMs were found. 259"
                },
                "260": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br>  260"
                },
                "261": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 261"
                },
                "262": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 262"
                },
                "263": {
                    "answer": "Sorry, no matching LLMs were found. 263"
                },
                "264": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br> 264"
                },
                "265": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br> 265"
                },
                "266": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br> 266"
                },
                "267": {
                    "answer": "Sorry, no matching LLMs were found. 267"
                },
                "268": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br>   268"
                },
                "269": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 269"
                },
                "270": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 270"
                },
                "271": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 271"
                },
                "272": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 272"
                },
                "273": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 273"
                },
                "274": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 274"
                },
                "275": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 275"
                },
                "276": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 276"
                },
                "277": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 277"
                },
                "278": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*), FraCGPT-4 (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>FraCGPT-4: can be accessed via link <a href='https://github.com/maxrusse/fracchat' target='_blank'>FraCGPT4 GitHub</a>.<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>.  278"
                },
                "279": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 279"
                },
                "280": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 280"
                },
                "281": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 281"
                },
                "282": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Bing Chat (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 282"
                },
                "283": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 283"
                },
                "284": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Claude-instant-v1.0 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Claude-instant-v1.0: the input price is $0.0008/1k tokens and the output price is $0.0024/1k tokens.<br> <br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 284"
                },
                "289": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 289"
                },
                "290": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 290"
                },
                "293": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 293"
                },
                "294": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 294"
                },
                "297": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 297"
                },
                "298": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 298"
                },
                "301": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 301"
                },
                "302": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.  302"
                },
                "303": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.  303"
                },
                "304": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  304"
                },
                "305": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 305"
                },
                "306": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 306"
                },
                "307": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  307"
                },
                "308": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  308"
                },
                "309": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 309"
                },
                "310": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 310"
                },
                "311": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 311"
                },
                "312": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 312"
                },
                "313": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 313"
                },
                "314": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 314"
                },
                "315": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 315"
                },
                "316": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 316"
                },
                "319": {
                    "answer": "Sorry, no matching LLMs were found. 319"
                },
                "320": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 320"
                },
                "321": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 321"
                },
                "322": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), Google Bard, GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br> <br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 322"
                },
                "323": {
                    "answer": "Sorry, no matching LLMs were found. 323"
                },
                "324": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 324"
                },
                "325": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 325"
                },
                "326": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 326"
                },
                "327": {
                    "answer": "Sorry, no matching LLMs were found. 327"
                },
                "328": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 328"
                },
                "329": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 329"
                },
                "330": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 330"
                },
                "331": {
                    "answer": "Sorry, no matching LLMs were found. 331"
                },
                "332": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 332"
                },
                "333": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.  333"
                },
                "334": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 334"
                },
                "335": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 335"
                },
                "336": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 336"
                },
                "337": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 337"
                },
                "338": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 338"
                },
                "339": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 339"
                },
                "340": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 340"
                },
                "341": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 341"
                },
                "342": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*), FraCGPT-4 (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>FraCGPT-4: can be accessed via link <a href='https://github.com/maxrusse/fracchat' target='_blank'>FraCGPT4 GitHub</a>.<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 342"
                },
                "343": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 343"
                },
                "344": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 344"
                },
                "345": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 345"
                },
                "346": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Bing Chat (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 346"
                },
                "347": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 347"
                },
                "348": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Claude-instant-v1.0 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Claude-instant-v1.0: the input price is $0.0008/1k tokens and the output price is $0.0024/1k tokens.<br> <br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 348"
                },
                "353": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 353"
                },
                "354": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 354"
                },
                "357": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 357"
                },
                "358": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  358"
                },
                "361": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 361"
                },
                "362": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 362"
                },
                "365": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 365"
                },
                "366": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 366"
                },
                "367": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.  367"
                },
                "368": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  368"
                },
                "369": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 369"
                },
                "370": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 370"
                },
                "371": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  371"
                },
                "372": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  372"
                },
                "373": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 373"
                },
                "374": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 374"
                },
                "375": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 375"
                },
                "376": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 376"
                },
                "377": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 377"
                },
                "378": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>. 378"
                },
                "379": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.  379"
                },
                "380": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.  380"
                },
                "383": {
                    "answer": "Sorry, no matching LLMs were found. 383"
                },
                "384": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.  384"
                },
                "385": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 385"
                },
                "386": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 386"
                },
                "387": {
                    "answer": "Sorry, no matching LLMs were found. 387"
                },
                "388": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 388"
                },
                "389": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 389"
                },
                "390": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 390"
                },
                "391": {
                    "answer": "Sorry, no matching LLMs were found. 391"
                },
                "392": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 392"
                },
                "393": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-7B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>Med-Flamingo-7B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 393"
                },
                "394": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 394"
                },
                "395": {
                    "answer": "Sorry, no matching LLMs were found. 395"
                },
                "396": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 396"
                },
                "397": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*), OphGLM-6.2B (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 397"
                },
                "398": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), GatorTron-345M (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*), OphGLM-6.2B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 398"
                },
                "399": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 399"
                },
                "400": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 400"
                },
                "401": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> 401"
                },
                "402": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 402"
                },
                "403": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 403"
                },
                "404": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 404"
                },
                "405": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 405"
                },
                "406": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*), FraCGPT-4 (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>FraCGPT-4: can be accessed via link <a href='https://github.com/maxrusse/fracchat' target='_blank'>FraCGPT4 GitHub</a>.<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 406"
                },
                "407": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 407"
                },
                "408": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 408"
                },
                "409": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 409"
                },
                "410": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Bing Chat (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 410"
                },
                "411": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 411"
                },
                "412": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Claude-instant-v1.0 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Claude-instant-v1.0: the input price is $0.0008/1k tokens and the output price is $0.0024/1k tokens.<br> <br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 412"
                },
                "417": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 417"
                },
                "418": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 418"
                },
                "421": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 421"
                },
                "422": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 422"
                },
                "425": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-7B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 425"
                },
                "426": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 426"
                },
                "429": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 429"
                },
                "430": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 430"
                },
                "431": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.  431"
                },
                "432": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  432"
                },
                "433": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 433"
                },
                "434": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 434"
                },
                "435": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  435"
                },
                "436": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  436"
                },
                "437": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 437"
                },
                "438": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 438"
                },
                "439": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 439"
                },
                "440": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 440"
                },
                "441": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992. 441"
                },
                "442": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h. 442"
                },
                "443": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 443"
                },
                "444": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 444"
                },
                "447": {
                    "answer": "Sorry, no matching LLMs were found. 447"
                },
                "448": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.  448"
                },
                "449": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 449"
                },
                "450": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.  450"
                },
                "451": {
                    "answer": "Sorry, no matching LLMs were found. 451"
                },
                "452": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 452"
                },
                "453": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 453"
                },
                "454": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  454"
                },
                "455": {
                    "answer": "Sorry, no matching LLMs were found. 455"
                },
                "456": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 456"
                },
                "457": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-7B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 457"
                },
                "458": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 458"
                },
                "459": {
                    "answer": "Sorry, no matching LLMs were found. 459"
                },
                "460": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens. 460"
                },
                "461": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*), OphGLM-6.2B (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 461"
                },
                "462": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard (*), GatorTron-345M (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*), OphGLM-6.2B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 462"
                },
                "463": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.  463"
                },
                "464": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 464"
                },
                "465": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 465"
                },
                "466": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 466"
                },
                "467": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU. 467"
                },
                "468": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), LLaMA-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h. 468"
                },
                "469": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 469"
                },
                "470": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*), FraCGPT-4 (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>FraCGPT-4: can be accessed via link <a href='https://github.com/maxrusse/fracchat' target='_blank'>FraCGPT4 GitHub</a>.<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 470"
                },
                "471": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 471"
                },
                "472": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 472"
                },
                "473": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 473"
                },
                "474": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), GatorTron-8.9B (*), Bing Chat (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), GPT-4V (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*), OphGLM-6.2B (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is more than 36000W and the total price is more than $2160000.<br><br>Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>GPT-4V: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>. 474"
                },
                "475": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 475"
                },
                "476": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Claude-instant-v1.0 (*), Google Bard, LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Claude-instant-v1.0: the input price is $0.0008/1k tokens and the output price is $0.0024/1k tokens.<br> <br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro.<br><br>The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 476"
                },
                "481": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 481"
                },
                "482": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, BioBERT, BioMegatron, ClinicalBERT-110M, RadioLOGIC. Here are the required resources:<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 482"
                },
                "485": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 485"
                },
                "486": {
                    "answer": "The LLMs we recommend you use are: RadioLOGIC (*), Clinical-longformer-150M (*), GatorTron-345M, GatorTron-3.9B, GatorTron-8.9B, LLaMA2-70B, BERT, DeBERTa, RoBERTa, BioBERT, BioMegatron, ClinicalBERT-110M. Here are the required resources:<br>  <br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTron-3.9B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA2-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>DeBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioMegatron: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 486"
                },
                "489": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-7B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 489"
                },
                "490": {
                    "answer": "The LLMs we recommend you use are: Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*). Here are the required resources:<br> <br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 490"
                },
                "493": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 500W and the total price is $13998.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 493"
                },
                "494": {
                    "answer": "The LLMs we recommend you use are: GatorTron-345M (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*),  GestaltGPT-7B (*), Clinical-longformer-150M (*), BioClinRoBERTa-345M (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BioBERT (*), GPT2-1.5B (*), BioLinkBERT (*), ClinicalBERT-110M (*), RadioLOGIC (*). Here are the required resources:<br><br>GatorTron-345M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting this type of GPU using cloud services is $4.05/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>GestaltGPT-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPU. The price of renting 2 this type of GPU using cloud services is $8.1/1h.<br> <br>It can be accessed via link <a href='https://github.com/WGLab/GestaltMML-GestaltGPT' target='_blank'>GestaltGPT GitHub</a>.<br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>GPT2-1.5B: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>. 494"
                },
                "495": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.  495"
                },
                "496": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  496"
                },
                "497": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 497"
                },
                "498": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*). Here are the required resources:<br><br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>. 498"
                },
                "499": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  499"
                },
                "500": {
                    "answer": "The LLM we recommend you use is: LLaMA-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.  500"
                },
                "501": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>GatorTronS-20B: no specific resource requirements for these models, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br> <br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $4680.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 501"
                },
                "502": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), LLaMA-7B (*), Vicuna-13B (*),  Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), Alpaca-7B (*), RadioLOGIC (*), PhenoBCBERT-110M (*), PhenoGPT-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $128.6/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>RadioLOGIC: pre-train requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/Netherlands-Cancer-Institute/RadioLOGIC_NLP' target='_blank'>RadioLOGIC GitHub</a>.<br><br>PhenoBCBERT-110M: fine-tuning requires 1 2GB GPU. The price of renting a GPU with 2GB of memory using a cloud service is $0.16/1h.<br><br>PhenoGPT-7B: fine-tuning requires 1 70GB GPU. The price of renting a GPU with 70GB of memory using a cloud service is $5.47/1h.<br><br>It can be accessed via link <a href='https://github.com/WGLab/PhenoGPT' target='_blank'>PhenoGPT GitHub</a>. 502"
                },
                "503": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 503"
                },
                "504": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br> <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 504"
                },
                "505": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 297600W and the total price is $17856000. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $144000. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The total thermal design power of the GPU is 300W and the total price is $7799.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $20571.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The total thermal design power of the GPUs is 300W and the total price is $18000.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 9600W and the total price is $576000.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992. 505"
                },
                "506": {
                    "answer": "The LLMs we recommend you use are: GatorTron-8.9B (*), MedAlpaca-13B (*), Clinical-longformer-150M (*), GatorTronGPT-20B (*), GatorTronS-20B (*), BioClinRoBERTa-345M (*), Galactica-120B (*), Me LLaMA-Chat-13B (*), Me LLaMA-Chat-70B (*), Me LLaMA-13B (*), Me LLaMA-70B (*), RaDialog-7B (*), ChatDoctor-7B (*), MedChatZH-7B (*), Med-Flamingo-9B (*), SkinGPT-4-40B (*), RadFM-14B (*), LLaVA-Med-7B (*). Here are the required resources:<br> <br>GatorTron-8.9B: pre-train requires 992 NVIDIA TESLA A100-80GB GPUs. The price of renting 992 this type of GPU using cloud services is $6200/1h. The data is only reported in the paper, and an LLM of that size may only require a few dozen GPUs of the same type.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. <br><br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GatorTronS-20B: no specific resource requirements for this model, refer to resources for LLMs of similar size (20B). Pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h.<br><br>Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br> <br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Galactica-120B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires more than 120 NVIDIA TESLA A100-80GB GPUs. The price of renting 120 this type of GPU using cloud services is $750/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Me LLaMA-Chat-13B, Me LLaMA-13B, Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 GPUs of this type using cloud services is $1760/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 GPUs of this type using cloud services is $88/1h.<br><br>They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>RaDialog-7B: fine-tuning requires 1 NVIDIA A40-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>It can be accessed via link <a href='https://github.com/ChantalMP/RaDialog' target='_blank'>RaDialog GitHub</a>.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>.<br><br>MedChatZH-7B: pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Fine-tuning  requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>It can be accessed via link <a href='https://github.com/tyang816/MedChatZH' target='_blank'>MedChatZH GitHub</a>.<br><br>Med-Flamingo-9B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>It can be accessed via link <a href='https://github.com/snap-stanford/med-flamingo' target='_blank'>MedFlamingo GitHub</a>.<br><br>SkinGPT-4-40B: pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>Inference requires 1 NVIDIA TESLA A100-80GB GPU. The price of renting 1 this type of GPU using cloud services is $6.25/1h.<br><br>It can be accessed via link <a href='https://github.com/JoshuaChou2018/SkinGPT-4' target='_blank'>SkinGPT4 GitHub</a>.<br><br>RadFM-14B: pre-train requires 32 NVIDIA TESLA A100-80GB GPUs. The price of renting 32 this type of GPU using cloud services is $200/1h.<br><br>It can be accessed via link <a href='https://github.com/chaoyi-wu/RadFM' target='_blank'>RadFM GitHub</a>.<br><br>LLaVA-Med-7B: fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h. 506"
                },
                "507": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000.<br><br>Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15998.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $41994.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 507"
                },
                "508": {
                    "answer": "The LLMs we recommend you use are: LLaMA-7B, LLaMA-70B, ChatDoctor-7B. Here are the required resources:<br>  <br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>LLaMA-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h.<br><br>Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h.<br><br>Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>ChatDoctor-7B: fine-tuning requires 6 NVIDIA TESLA A100-40GB GPUs. The price of renting 6 this type of GPU using cloud services is $24.3/1h.<br><br>It can be accessed via link <a href='https://github.com/Kent0n-Li/ChatDoctor' target='_blank'>ChatDoctor GitHub</a>. 508"
                }
            };


                if (data[node] && data[node].question) {
                    const questionDiv = document.createElement('div');
                    questionDiv.classList.add('question');

                    // Display fixed question number
                    const questionNumberElem = document.createElement('p');
                    questionNumberElem.classList.add('questionNumber');
                    questionNumberElem.innerText = \`Question \${questionNumber}:\`;
                    questionDiv.appendChild(questionNumberElem);

                    // Display question text (with clickable keywords)
                    const questionText = document.createElement('p');
                    questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
                    questionDiv.appendChild(questionText);

                    // Create Yes button
                    const yesButton = document.createElement('button');
                    yesButton.innerText = 'Yes';
                    yesButton.classList.toggle('selected', selectedChoice === 'yes');
                    yesButton.onclick = function () {
                        updateHistory(node, 'yes', questionNumber);
                        currentNode = data[node].yes;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(yesButton);

                    // Create No button
                    const noButton = document.createElement('button');
                    noButton.innerText = 'No';
                    noButton.classList.toggle('selected', selectedChoice === 'no');
                    noButton.onclick = function () {
                        updateHistory(node, 'no', questionNumber);
                        currentNode = data[node].no;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(noButton);

                    // Append the question block to the container
                    questionContainer.appendChild(questionDiv);

                    // Attach event listeners to keywords
                    const highlightElements = questionDiv.querySelectorAll('.highlight');
                    highlightElements.forEach(function (element) {
                        element.addEventListener('click', function () {
                            const keyword = element.getAttribute('data-keyword');
                            showDescription(keyword);
                        });
                    });

                } else if (data[node] && data[node].answer) {
                    // Create a div for the final answer
                    const answerDiv = document.createElement('div');
                    answerDiv.classList.add('answer');

                    const answerText = document.createElement('p');
                    answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
                    answerDiv.appendChild(answerText);

                    // Append the answer to the container
                    questionContainer.appendChild(answerDiv);
                } else {
                    // Handle if no more questions or no data found
                    const noMoreQuestions = document.createElement('p');
                    noMoreQuestions.innerText = "No more questions available.";
                    questionContainer.appendChild(noMoreQuestions);
                }
            }

            function showDescription(keyword) {
            const descriptions = {
                "textual data": "It refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
                "perform tasks related to disease diagnosis": "It refers to the ability to perform at least one relevant task, including generating diagnostic conclusions based on patient descriptions, examination reports, clinical records or medical images (if non-textual input), disease classification tasks, etc.",
                "extract medical information":"It refers to the ability to perform at least one similar task, such as summarizing the impression portion of a radiology report, summarizing questions posed by the patient related to the radiology report, etc.",
                "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
                "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
                "answer medical questions":"medical questions includes multiple-choice and open-ended questions (and may also include visual questions, i.e., where there are medical images in the input) on tasks related to the disease diagnosis and treatment planning. Open questions are questions that require a free-form text answer. These questions usually require more in-depth thinking and personalized answers rather than choosing from a limited number of options. For example, these questions may involve descriptions of specific symptoms of the patient, detailed documentation of past medical history, preferences or limitations of specific treatment options, etc., information that is not fully accessible from traditional multiple-choice questions.",
                "generate text related to medical decision making":"It refers to the ability to perform at least one similar task, including the generation of patient treatment plans, improved clinical decision support recommendations, etc."
            };
                const descriptionText = descriptions[keyword];
                if (descriptionText) {
                    document.getElementById('descriptionText').innerText = descriptionText;
                    document.getElementById('descriptionContainer').style.display = 'block';
                }
            }

            document.getElementById('closeButton').addEventListener('click', function () {
                document.getElementById('descriptionContainer').style.display = 'none';
            });

            function updateHistory(node, choice, questionNumber) {
                // Check if we are revisiting a question and modify history accordingly
                const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
                if (historyIndex !== -1) {
                    questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
                }

                // Add current question to history
                questionHistory.push({ node, choice, number: questionNumber });
            }

            // Start the first question
            displayQuestion(currentNode, 0);
                                `;
                                document.body.appendChild(script);
                            } else if (page === 's4.html') {
                                // Initialize Q&A logic after loading the HTML
                                const script = document.createElement('script');
                                script.innerHTML = `

            var currentNode = "0"; // Starting node
            var questionHistory = []; // Keeps track of answered questions

            // Function to display a question based on the node
            function displayQuestion(node, questionIndex) {
                const questionContainer = document.getElementById('questionContainer');
                // Clear questions beyond this index
                questionContainer.innerHTML = '';

                // Display history
                questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

                // Display the current question
                appendQuestion(node, null, questionIndex + 1);
            }

            function appendQuestion(node, selectedChoice, questionNumber) {
                const questionContainer = document.getElementById('questionContainer');
                const data = {
                "0": {
                    "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "1",
                    "no": "2"
                },
                "1": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "3",
                    "no": "4"
                },
                "2": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "5",
                    "no": "6"
                },
                "3": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "7",
                    "no": "8"
                },
                "4": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "9",
                    "no": "10"
                },
                "5": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "11",
                    "no": "12"
                },
                "6": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "13",
                    "no": "14"
                },
                "7": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 7",
                    "yes": "15",
                    "no": "16"
                },
                "8": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 8",
                    "yes": "17",
                    "no": "18"
                },
                "9": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 9",
                    "yes": "19",
                    "no": "20"
                },
                "10": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 10",
                    "yes": "21",
                    "no": "22"
                },
                "11": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 11",
                    "yes": "23",
                    "no": "24"
                },
                "12": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 12",
                    "yes": "25",
                    "no": "26"
                },
                "13": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 13",
                    "yes": "27",
                    "no": "28"
                },
                "14": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>? 14",
                    "yes": "29",
                    "no": "30"
                },
                "15": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 15",
                    "yes": "31",
                    "no": "32"
                },
                "16": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 16",
                    "yes": "33",
                    "no": "34"
                },
                "17": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 17",
                    "yes": "35",
                    "no": "36"
                },
                "18": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 18",
                    "yes": "37",
                    "no": "38"
                },
                "19": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 19",
                    "yes": "39",
                    "no": "40"
                },
                "20": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 20",
                    "yes": "41",
                    "no": "42"
                },
                "21": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 21",
                    "yes": "43",
                    "no": "44"
                },
                "22": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 22",
                    "yes": "45",
                    "no": "46"
                },
                "23": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 23",
                    "yes": "47",
                    "no": "48"
                },
                "24": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 24",
                    "yes": "49",
                    "no": "50"
                },
                "25": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 25",
                    "yes": "51",
                    "no": "52"
                },
                "26": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 26",
                    "yes": "53",
                    "no": "54"
                },
                "27": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 27",
                    "yes": "55",
                    "no": "56"
                },
                "28": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 28",
                    "yes": "57",
                    "no": "58"
                },
                "29": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 29",
                    "yes": "59",
                    "no": "60"
                },
                "30": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 30",
                    "yes": "61",
                    "no": "62"
                },
                "31": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 31",
                    "yes": "63",
                    "no": "64"
                },
                "32": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 32",
                    "yes": "65",
                    "no": "66"
                },
                "33": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 33",
                    "yes": "67",
                    "no": "68"
                },
                "34": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 34",
                    "yes": "69",
                    "no": "70"
                },
                "35": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 35",
                    "yes": "71",
                    "no": "72"
                },
                "36": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 36",
                    "yes": "73",
                    "no": "74"
                },
                "37": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 37",
                    "yes": "75",
                    "no": "76"
                },
                "38": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 38",
                    "yes": "77",
                    "no": "78"
                },
                "39": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 39",
                    "yes": "79",
                    "no": "80"
                },
                "40": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 40",
                    "yes": "81",
                    "no": "82"
                },
                "41": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 41",
                    "yes": "83",
                    "no": "84"
                },
                "42": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 42",
                    "yes": "85",
                    "no": "86"
                },
                "43": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 43",
                    "yes": "87",
                    "no": "88"
                },
                "44": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 44",
                    "yes": "89",
                    "no": "90"
                },
                "45": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 45",
                    "yes": "91",
                    "no": "92"
                },
                "46": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 46",
                    "yes": "93",
                    "no": "94"
                },
                "47": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 47",
                    "yes": "95",
                    "no": "96"
                },
                "48": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 48",
                    "yes": "97",
                    "no": "98"
                },
                "49": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 49",
                    "yes": "99",
                    "no": "100"
                },
                "50": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 50",
                    "yes": "101",
                    "no": "102"
                },
                "51": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 51",
                    "yes": "103",
                    "no": "104"
                },
                "52": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 52",
                    "yes": "105",
                    "no": "106"
                },
                "53": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 53",
                    "yes": "107",
                    "no": "108"
                },
                "54": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 54",
                    "yes": "109",
                    "no": "110"
                },
                "55": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 55",
                    "yes": "111",
                    "no": "112"
                },
                "56": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 56",
                    "yes": "113",
                    "no": "114"
                },
                "57": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 57",
                    "yes": "115",
                    "no": "116"
                },
                "58": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 58",
                    "yes": "117",
                    "no": "118"
                },
                "59": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 59",
                    "yes": "119",
                    "no": "120"
                },
                "60": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 60",
                    "yes": "121",
                    "no": "122"
                },
                "61": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 61",
                    "yes": "123",
                    "no": "124"
                },
                "62": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 62",
                    "yes": "125",
                    "no": "126"
                },
                "63": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 63",
                    "yes": "127",
                    "no": "128"
                },
                "64": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 64",
                    "yes": "129",
                    "no": "130"
                },
                "65": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 65",
                    "yes": "131",
                    "no": "132"
                },
                "66": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 66",
                    "yes": "133",
                    "no": "134"
                },
                "67": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 67",
                    "yes": "135",
                    "no": "136"
                },
                "68": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 68",
                    "yes": "137",
                    "no": "138"
                },
                "69": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 69",
                    "yes": "139",
                    "no": "140"
                },
                "70": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 70",
                    "yes": "141",
                    "no": "142"
                },
                "71": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 71",
                    "yes": "143",
                    "no": "144"
                },
                "72": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 72",
                    "yes": "145",
                    "no": "146"
                },
                "73": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 73",
                    "yes": "147",
                    "no": "148"
                },
                "74": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 74",
                    "yes": "149",
                    "no": "150"
                },
                "75": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 75",
                    "yes": "151",
                    "no": "152"
                },
                "76": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 76",
                    "yes": "153",
                    "no": "154"
                },
                "77": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 77",
                    "yes": "155",
                    "no": "156"
                },
                "78": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 78",
                    "yes": "157",
                    "no": "158"
                },
                "79": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 79",
                    "yes": "159",
                    "no": "160"
                },
                "80": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 80",
                    "yes": "161",
                    "no": "162"
                },
                "81": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 81",
                    "yes": "163",
                    "no": "164"
                },
                "82": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 82",
                    "yes": "165",
                    "no": "166"
                },
                "83": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 83",
                    "yes": "167",
                    "no": "168"
                },
                "84": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 84",
                    "yes": "169",
                    "no": "170"
                },
                "85": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 85",
                    "yes": "171",
                    "no": "172"
                },
                "86": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 86",
                    "yes": "173",
                    "no": "174"
                },
                "87": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 87",
                    "yes": "175",
                    "no": "176"
                },
                "88": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 88",
                    "yes": "177",
                    "no": "178"
                },
                "89": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 89",
                    "yes": "179",
                    "no": "180"
                },
                "90": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 90",
                    "yes": "181",
                    "no": "182"
                },
                "91": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 91",
                    "yes": "183",
                    "no": "184"
                },
                "92": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 92",
                    "yes": "185",
                    "no": "186"
                },
                "93": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 93",
                    "yes": "187",
                    "no": "188"
                },
                "94": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 94",
                    "yes": "189",
                    "no": "190"
                },
                "95": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 95",
                    "yes": "191",
                    "no": "192"
                },
                "96": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 96",
                    "yes": "193",
                    "no": "194"
                },
                "97": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 97",
                    "yes": "195",
                    "no": "196"
                },
                "98": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 98",
                    "yes": "197",
                    "no": "198"
                },
                "99": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 99",
                    "yes": "199",
                    "no": "200"
                },
                "100": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 100",
                    "yes": "201",
                    "no": "202"
                },
                "101": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 101",
                    "yes": "203",
                    "no": "204"
                },
                "102": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 102",
                    "yes": "205",
                    "no": "206"
                },
                "103": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 103",
                    "yes": "207",
                    "no": "208"
                },
                "104": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 104",
                    "yes": "209",
                    "no": "210"
                },
                "105": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 105",
                    "yes": "211",
                    "no": "212"
                },
                "106": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 106",
                    "yes": "213",
                    "no": "214"
                },
                "107": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 107",
                    "yes": "215",
                    "no": "216"
                },
                "108": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 108",
                    "yes": "217",
                    "no": "218"
                },
                "109": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 109",
                    "yes": "219",
                    "no": "220"
                },
                "110": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 110",
                    "yes": "221",
                    "no": "222"
                },
                "111": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 111",
                    "yes": "223",
                    "no": "224"
                },
                "112": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 112",
                    "yes": "225",
                    "no": "226"
                },
                "113": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 113",
                    "yes": "227",
                    "no": "228"
                },
                "114": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 114",
                    "yes": "229",
                    "no": "230"
                },
                "115": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 115",
                    "yes": "231",
                    "no": "232"
                },
                "116": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 116",
                    "yes": "233",
                    "no": "234"
                },
                "117": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 117",
                    "yes": "235",
                    "no": "236"
                },
                "118": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 118",
                    "yes": "237",
                    "no": "238"
                },
                "119": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 119",
                    "yes": "239",
                    "no": "240"
                },
                "120": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 120",
                    "yes": "241",
                    "no": "242"
                },
                "121": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 121",
                    "yes": "243",
                    "no": "244"
                },
                "122": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 122",
                    "yes": "245",
                    "no": "246"
                },
                "123": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 123",
                    "yes": "247",
                    "no": "248"
                },
                "124": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 124",
                    "yes": "249",
                    "no": "250"
                },
                "125": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 125",
                    "yes": "251",
                    "no": "252"
                },
                "126": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 126",
                    "yes": "253",
                    "no": "254"
                },
                "127": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 127",
                    "yes": "255",
                    "no": "256"
                },
                "128": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 128",
                    "yes": "257",
                    "no": "258"
                },
                "129": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 129",
                    "yes": "259",
                    "no": "260"
                },
                "130": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 130",
                    "yes": "261",
                    "no": "262"
                },
                "131": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 131",
                    "yes": "263",
                    "no": "264"
                },
                "132": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 132",
                    "yes": "265",
                    "no": "266"
                },
                "133": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 133",
                    "yes": "267",
                    "no": "268"
                },
                "134": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 134",
                    "yes": "269",
                    "no": "270"
                },
                "135": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 135",
                    "yes": "271",
                    "no": "272"
                },
                "136": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 136",
                    "yes": "273",
                    "no": "274"
                },
                "137": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 137",
                    "yes": "275",
                    "no": "276"
                },
                "138": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 138",
                    "yes": "277",
                    "no": "278"
                },
                "139": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 139",
                    "yes": "279",
                    "no": "280"
                },
                "140": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 140",
                    "yes": "281",
                    "no": "282"
                },
                "141": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 141",
                    "yes": "283",
                    "no": "284"
                },
                "142": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 142",
                    "yes": "285",
                    "no": "286"
                },
                "143": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 143",
                    "yes": "287",
                    "no": "288"
                },
                "144": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 144",
                    "yes": "289",
                    "no": "290"
                },
                "145": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 145",
                    "yes": "291",
                    "no": "292"
                },
                "146": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 146",
                    "yes": "293",
                    "no": "294"
                },
                "147": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 147",
                    "yes": "295",
                    "no": "296"
                },
                "148": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 148",
                    "yes": "297",
                    "no": "298"
                },
                "149": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 149",
                    "yes": "299",
                    "no": "300"
                },
                "150": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 150",
                    "yes": "301",
                    "no": "302"
                },
                "151": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 151",
                    "yes": "303",
                    "no": "304"
                },
                "152": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 152",
                    "yes": "305",
                    "no": "306"
                },
                "153": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 153",
                    "yes": "307",
                    "no": "308"
                },
                "154": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 154",
                    "yes": "309",
                    "no": "310"
                },
                "155": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 155",
                    "yes": "311",
                    "no": "312"
                },
                "156": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 156",
                    "yes": "313",
                    "no": "314"
                },
                "157": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 157",
                    "yes": "315",
                    "no": "316"
                },
                "158": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 158",
                    "yes": "317",
                    "no": "318"
                },
                "159": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 159",
                    "yes": "319",
                    "no": "320"
                },
                "160": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 160",
                    "yes": "321",
                    "no": "322"
                },
                "161": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 161",
                    "yes": "323",
                    "no": "324"
                },
                "162": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 162",
                    "yes": "325",
                    "no": "326"
                },
                "163": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 163",
                    "yes": "327",
                    "no": "328"
                },
                "164": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 164",
                    "yes": "329",
                    "no": "330"
                },
                "165": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 165",
                    "yes": "331",
                    "no": "332"
                },
                "166": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 166",
                    "yes": "333",
                    "no": "334"
                },
                "167": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 167",
                    "yes": "335",
                    "no": "336"
                },
                "168": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 168",
                    "yes": "337",
                    "no": "338"
                },
                "169": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 169",
                    "yes": "339",
                    "no": "340"
                },
                "170": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 170",
                    "yes": "341",
                    "no": "342"
                },
                "171": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 171",
                    "yes": "343",
                    "no": "344"
                },
                "172": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 172",
                    "yes": "345",
                    "no": "346"
                },
                "173": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 173",
                    "yes": "347",
                    "no": "348"
                },
                "174": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 174",
                    "yes": "349",
                    "no": "350"
                },
                "175": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 175",
                    "yes": "351",
                    "no": "352"
                },
                "176": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 176",
                    "yes": "353",
                    "no": "354"
                },
                "177": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 177",
                    "yes": "355",
                    "no": "356"
                },
                "178": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 178",
                    "yes": "357",
                    "no": "358"
                },
                "179": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 179",
                    "yes": "359",
                    "no": "360"
                },
                "180": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 180",
                    "yes": "361",
                    "no": "362"
                },
                "181": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 181",
                    "yes": "363",
                    "no": "364"
                },
                "182": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 182",
                    "yes": "365",
                    "no": "366"
                },
                "183": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 183",
                    "yes": "367",
                    "no": "368"
                },
                "184": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 184",
                    "yes": "369",
                    "no": "370"
                },
                "185": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 185",
                    "yes": "371",
                    "no": "372"
                },
                "186": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 186",
                    "yes": "373",
                    "no": "374"
                },
                "187": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 187",
                    "yes": "375",
                    "no": "376"
                },
                "188": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 188",
                    "yes": "377",
                    "no": "378"
                },
                "189": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 189",
                    "yes": "379",
                    "no": "380"
                },
                "190": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 190",
                    "yes": "381",
                    "no": "382"
                },
                "191": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 191",
                    "yes": "383",
                    "no": "384"
                },
                "192": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 192",
                    "yes": "385",
                    "no": "386"
                },
                "193": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 193",
                    "yes": "387",
                    "no": "388"
                },
                "194": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 194",
                    "yes": "389",
                    "no": "390"
                },
                "195": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 195",
                    "yes": "391",
                    "no": "392"
                },
                "196": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 196",
                    "yes": "393",
                    "no": "394"
                },
                "197": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 197",
                    "yes": "395",
                    "no": "396"
                },
                "198": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 198",
                    "yes": "397",
                    "no": "398"
                },
                "199": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 199",
                    "yes": "399",
                    "no": "400"
                },
                "200": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 200",
                    "yes": "401",
                    "no": "402"
                },
                "201": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 201",
                    "yes": "403",
                    "no": "404"
                },
                "202": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 202",
                    "yes": "405",
                    "no": "406"
                },
                "203": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 203",
                    "yes": "407",
                    "no": "408"
                },
                "204": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 204",
                    "yes": "409",
                    "no": "410"
                },
                "205": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 205",
                    "yes": "411",
                    "no": "412"
                },
                "206": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 206",
                    "yes": "413",
                    "no": "414"
                },
                "207": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 207",
                    "yes": "415",
                    "no": "416"
                },
                "208": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 208",
                    "yes": "417",
                    "no": "418"
                },
                "209": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 209",
                    "yes": "419",
                    "no": "420"
                },
                "210": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 210",
                    "yes": "421",
                    "no": "422"
                },
                "211": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 211",
                    "yes": "423",
                    "no": "424"
                },
                "212": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 212",
                    "yes": "425",
                    "no": "426"
                },
                "213": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 213",
                    "yes": "427",
                    "no": "428"
                },
                "214": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 214",
                    "yes": "429",
                    "no": "430"
                },
                "215": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 215",
                    "yes": "431",
                    "no": "432"
                },
                "216": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 216",
                    "yes": "433",
                    "no": "434"
                },
                "217": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 217",
                    "yes": "435",
                    "no": "436"
                },
                "218": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 218",
                    "yes": "437",
                    "no": "438"
                },
                "219": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 219",
                    "yes": "439",
                    "no": "440"
                },
                "220": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 220",
                    "yes": "441",
                    "no": "442"
                },
                "221": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 221",
                    "yes": "443",
                    "no": "444"
                },
                "222": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 222",
                    "yes": "445",
                    "no": "446"
                },
                "223": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 223",
                    "yes": "447",
                    "no": "448"
                },
                "224": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 224",
                    "yes": "449",
                    "no": "450"
                },
                "225": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 225",
                    "yes": "451",
                    "no": "452"
                },
                "226": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 226",
                    "yes": "453",
                    "no": "454"
                },
                "227": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 227",
                    "yes": "455",
                    "no": "456"
                },
                "228": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 228",
                    "yes": "457",
                    "no": "458"
                },
                "229": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 229",
                    "yes": "459",
                    "no": "460"
                },
                "230": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 230",
                    "yes": "461",
                    "no": "462"
                },
                "231": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 231",
                    "yes": "463",
                    "no": "464"
                },
                "232": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 232",
                    "yes": "465",
                    "no": "466"
                },
                "233": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 233",
                    "yes": "467",
                    "no": "468"
                },
                "234": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 234",
                    "yes": "469",
                    "no": "470"
                },
                "235": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 235",
                    "yes": "471",
                    "no": "472"
                },
                "236": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 236",
                    "yes": "473",
                    "no": "474"
                },
                "237": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 237",
                    "yes": "475",
                    "no": "476"
                },
                "238": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 238",
                    "yes": "477",
                    "no": "478"
                },
                "239": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 239",
                    "yes": "479",
                    "no": "480"
                },
                "240": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 240",
                    "yes": "481",
                    "no": "482"
                },
                "241": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 241",
                    "yes": "483",
                    "no": "484"
                },
                "242": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 242",
                    "yes": "485",
                    "no": "486"
                },
                "243": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 243",
                    "yes": "487",
                    "no": "488"
                },
                "244": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 244",
                    "yes": "489",
                    "no": "490"
                },
                "245": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 245",
                    "yes": "491",
                    "no": "492"
                },
                "246": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 246",
                    "yes": "493",
                    "no": "494"
                },
                "247": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 247",
                    "yes": "495",
                    "no": "496"
                },
                "248": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 248",
                    "yes": "497",
                    "no": "498"
                },
                "249": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 249",
                    "yes": "499",
                    "no": "500"
                },
                "250": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 250",
                    "yes": "501",
                    "no": "502"
                },
                "251": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 251",
                    "yes": "503",
                    "no": "504"
                },
                "252": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 252",
                    "yes": "505",
                    "no": "506"
                },
                "253": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 253",
                    "yes": "507",
                    "no": "508"
                },
                "254": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>? 254",
                    "yes": "509",
                    "no": "510"
                },
                "255": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 255",
                    "yes": "511",
                    "no": "512"
                },
                "256": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 256",
                    "yes": "513",
                    "no": "514"
                },
                "257": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 257",
                    "yes": "515",
                    "no": "516"
                },
                "258": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 258",
                    "yes": "517",
                    "no": "518"
                },
                "259": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 259",
                    "yes": "519",
                    "no": "520"
                },
                "260": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 260",
                    "yes": "521",
                    "no": "522"
                },
                "261": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 261",
                    "yes": "523",
                    "no": "524"
                },
                "262": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 262",
                    "yes": "525",
                    "no": "526"
                },
                "263": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 263",
                    "yes": "527",
                    "no": "528"
                },
                "264": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 264",
                    "yes": "529",
                    "no": "530"
                },
                "265": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 265",
                    "yes": "531",
                    "no": "532"
                },
                "266": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 266",
                    "yes": "533",
                    "no": "534"
                },
                "267": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 267",
                    "yes": "535",
                    "no": "536"
                },
                "268": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 268",
                    "yes": "537",
                    "no": "538"
                },
                "269": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 269",
                    "yes": "539",
                    "no": "540"
                },
                "270": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 270",
                    "yes": "541",
                    "no": "542"
                },
                "271": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 271",
                    "yes": "543",
                    "no": "544"
                },
                "272": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 272",
                    "yes": "545",
                    "no": "546"
                },
                "273": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 273",
                    "yes": "547",
                    "no": "548"
                },
                "274": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 274",
                    "yes": "549",
                    "no": "550"
                },
                "275": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 275",
                    "yes": "551",
                    "no": "552"
                },
                "276": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 276",
                    "yes": "553",
                    "no": "554"
                },
                "277": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 277",
                    "yes": "555",
                    "no": "556"
                },
                "278": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 278",
                    "yes": "557",
                    "no": "558"
                },
                "279": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 279",
                    "yes": "559",
                    "no": "560"
                },
                "280": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 280",
                    "yes": "561",
                    "no": "562"
                },
                "281": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 281",
                    "yes": "563",
                    "no": "564"
                },
                "282": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 282",
                    "yes": "565",
                    "no": "566"
                },
                "283": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 283",
                    "yes": "567",
                    "no": "568"
                },
                "284": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 284",
                    "yes": "569",
                    "no": "570"
                },
                "285": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 285",
                    "yes": "571",
                    "no": "572"
                },
                "286": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 286"
                },
                "287": {
                    "answer": "Sorry, no matching LLMs were found. 287"
                },
                "288": {
                    "answer": "Sorry, no matching LLMs were found. 288"
                },
                "289": {
                    "answer": "Sorry, no matching LLMs were found. 289"
                },
                "290": {
                    "answer": "Sorry, no matching LLMs were found. 290"
                },
                "291": {
                    "answer": "Sorry, no matching LLMs were found. 291"
                },
                "292": {
                    "answer": "Sorry, no matching LLMs were found. 292"
                },
                "293": {
                    "answer": "Sorry, no matching LLMs were found. 293"
                },
                "294": {
                    "answer": "Sorry, no matching LLMs were found. 294"
                },
                "295": {
                    "answer": "Sorry, no matching LLMs were found. 295"
                },
                "296": {
                    "answer": "Sorry, no matching LLMs were found. 296"
                },
                "297": {
                    "answer": "Sorry, no matching LLMs were found. 297"
                },
                "298": {
                    "answer": "Sorry, no matching LLMs were found. 298"
                },
                "299": {
                    "answer": "Sorry, no matching LLMs were found. 299"
                },
                "300": {
                    "answer": "Sorry, no matching LLMs were found. 300"
                },
                "301": {
                    "answer": "Sorry, no matching LLMs were found. 301"
                },
                "302": {
                    "answer": "Sorry, no matching LLMs were found. 302"
                },
                "303": {
                    "answer": "Sorry, no matching LLMs were found. 303"
                },
                "304": {
                    "answer": "Sorry, no matching LLMs were found. 304"
                },
                "305": {
                    "answer": "Sorry, no matching LLMs were found. 305"
                },
                "306": {
                    "answer": "Sorry, no matching LLMs were found. 306"
                },
                "307": {
                    "answer": "Sorry, no matching LLMs were found. 307"
                },
                "308": {
                    "answer": "Sorry, no matching LLMs were found. 308"
                },
                "309": {
                    "answer": "Sorry, no matching LLMs were found. 309"
                },
                "310": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 310",
                    "yes": "621",
                    "no": "622"
                },
                "311": {
                    "answer": "Sorry, no matching LLMs were found. 311"
                },
                "312": {
                    "answer": "Sorry, no matching LLMs were found. 312"
                },
                "313": {
                    "answer": "Sorry, no matching LLMs were found. 313"
                },
                "314": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 314",
                    "yes": "629",
                    "no": "630"
                },
                "315": {
                    "answer": "Sorry, no matching LLMs were found. 315"
                },
                "316": {
                    "answer": "Sorry, no matching LLMs were found. 316"
                },
                "317": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 317",
                    "yes": "635",
                    "no": "636"
                },
                "318": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 318"
                },
                "319": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 319",
                    "yes": "639",
                    "no": "640"
                },
                "320": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 320",
                    "yes": "641",
                    "no": "642"
                },
                "321": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 321",
                    "yes": "643",
                    "no": "644"
                },
                "322": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 322",
                    "yes": "645",
                    "no": "646"
                },
                "323": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 323",
                    "yes": "647",
                    "no": "648"
                },
                "324": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 324",
                    "yes": "649",
                    "no": "650"
                },
                "325": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 325",
                    "yes": "651",
                    "no": "652"
                },
                "326": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 326",
                    "yes": "653",
                    "no": "654"
                },
                "327": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 327",
                    "yes": "655",
                    "no": "656"
                },
                "328": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 328",
                    "yes": "657",
                    "no": "658"
                },
                "329": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 329",
                    "yes": "659",
                    "no": "660"
                },
                "330": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 330",
                    "yes": "661",
                    "no": "662"
                },
                "331": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 331",
                    "yes": "663",
                    "no": "664"
                },
                "332": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 332",
                    "yes": "665",
                    "no": "666"
                },
                "333": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 333",
                    "yes": "667",
                    "no": "668"
                },
                "334": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 334",
                    "yes": "669",
                    "no": "670"
                },
                "335": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 335",
                    "yes": "671",
                    "no": "672"
                },
                "336": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 336",
                    "yes": "673",
                    "no": "674"
                },
                "337": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 337",
                    "yes": "675",
                    "no": "676"
                },
                "338": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 338",
                    "yes": "677",
                    "no": "678"
                },
                "339": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 339",
                    "yes": "679",
                    "no": "680"
                },
                "340": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 340",
                    "yes": "681",
                    "no": "682"
                },
                "341": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 341",
                    "yes": "683",
                    "no": "684"
                },
                "342": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 342",
                    "yes": "685",
                    "no": "686"
                },
                "343": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 343",
                    "yes": "687",
                    "no": "688"
                },
                "344": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 344",
                    "yes": "689",
                    "no": "690"
                },
                "345": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 345",
                    "yes": "691",
                    "no": "692"
                },
                "346": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 346",
                    "yes": "693",
                    "no": "694"
                },
                "347": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 347",
                    "yes": "695",
                    "no": "696"
                },
                "348": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 348",
                    "yes": "697",
                    "no": "698"
                },
                "349": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 349",
                    "yes": "699",
                    "no": "700"
                },
                "350": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 350"
                },
                "351": {
                    "answer": "Sorry, no matching LLMs were found. 351"
                },
                "352": {
                    "answer": "Sorry, no matching LLMs were found. 352"
                },
                "353": {
                    "answer": "Sorry, no matching LLMs were found. 353"
                },
                "354": {
                    "answer": "Sorry, no matching LLMs were found. 354"
                },
                "355": {
                    "answer": "Sorry, no matching LLMs were found. 355"
                },
                "356": {
                    "answer": "Sorry, no matching LLMs were found. 356"
                },
                "357": {
                    "answer": "Sorry, no matching LLMs were found. 357"
                },
                "358": {
                    "answer": "Sorry, no matching LLMs were found. 358"
                },
                "359": {
                    "answer": "Sorry, no matching LLMs were found. 359"
                },
                "360": {
                    "answer": "Sorry, no matching LLMs were found. 360"
                },
                "361": {
                    "answer": "Sorry, no matching LLMs were found. 361"
                },
                "362": {
                    "answer": "Sorry, no matching LLMs were found. 362"
                },
                "363": {
                    "answer": "Sorry, no matching LLMs were found. 363"
                },
                "364": {
                    "answer": "Sorry, no matching LLMs were found. 364"
                },
                "365": {
                    "answer": "Sorry, no matching LLMs were found. 365"
                },
                "366": {
                    "answer": "Sorry, no matching LLMs were found. 366"
                },
                "367": {
                    "answer": "Sorry, no matching LLMs were found. 367"
                },
                "368": {
                    "answer": "Sorry, no matching LLMs were found. 368"
                },
                "369": {
                    "answer": "Sorry, no matching LLMs were found. 369"
                },
                "370": {
                    "answer": "Sorry, no matching LLMs were found. 370"
                },
                "371": {
                    "answer": "Sorry, no matching LLMs were found. 371"
                },
                "372": {
                    "answer": "Sorry, no matching LLMs were found. 372"
                },
                "373": {
                    "answer": "Sorry, no matching LLMs were found. 373"
                },
                "374": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 374",
                    "yes": "749",
                    "no": "750"
                },
                "375": {
                    "answer": "Sorry, no matching LLMs were found. 375"
                },
                "376": {
                    "answer": "Sorry, no matching LLMs were found. 376"
                },
                "377": {
                    "answer": "Sorry, no matching LLMs were found. 377"
                },
                "378": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 378",
                    "yes": "757",
                    "no": "758"
                },
                "379": {
                    "answer": "Sorry, no matching LLMs were found. 379"
                },
                "380": {
                    "answer": "Sorry, no matching LLMs were found. 380"
                },
                "381": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 381",
                    "yes": "763",
                    "no": "764"
                },
                "382": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 382"
                },
                "383": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 383",
                    "yes": "767",
                    "no": "768"
                },
                "384": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 384",
                    "yes": "769",
                    "no": "770"
                },
                "385": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 385",
                    "yes": "771",
                    "no": "772"
                },
                "386": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 386",
                    "yes": "773",
                    "no": "774"
                },
                "387": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 387",
                    "yes": "775",
                    "no": "776"
                },
                "388": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 388",
                    "yes": "777",
                    "no": "778"
                },
                "389": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 389",
                    "yes": "779",
                    "no": "780"
                },
                "390": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 390",
                    "yes": "781",
                    "no": "782"
                },
                "391": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 391",
                    "yes": "783",
                    "no": "784"
                },
                "392": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 392",
                    "yes": "785",
                    "no": "786"
                },
                "393": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 393",
                    "yes": "787",
                    "no": "788"
                },
                "394": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 394",
                    "yes": "789",
                    "no": "790"
                },
                "395": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 395",
                    "yes": "791",
                    "no": "792"
                },
                "396": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 396",
                    "yes": "793",
                    "no": "794"
                },
                "397": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 397",
                    "yes": "795",
                    "no": "796"
                },
                "398": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 398",
                    "yes": "797",
                    "no": "798"
                },
                "399": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 399",
                    "yes": "799",
                    "no": "800"
                },
                "400": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 400",
                    "yes": "801",
                    "no": "802"
                },
                "401": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 401",
                    "yes": "803",
                    "no": "804"
                },
                "402": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 402",
                    "yes": "805",
                    "no": "806"
                },
                "403": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 403",
                    "yes": "807",
                    "no": "808"
                },
                "404": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 404",
                    "yes": "809",
                    "no": "810"
                },
                "405": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 405",
                    "yes": "811",
                    "no": "812"
                },
                "406": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 406",
                    "yes": "813",
                    "no": "814"
                },
                "407": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 407",
                    "yes": "815",
                    "no": "816"
                },
                "408": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 408",
                    "yes": "817",
                    "no": "818"
                },
                "409": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 409",
                    "yes": "819",
                    "no": "820"
                },
                "410": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 410",
                    "yes": "821",
                    "no": "822"
                },
                "411": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 411",
                    "yes": "823",
                    "no": "824"
                },
                "412": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 412",
                    "yes": "825",
                    "no": "826"
                },
                "413": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 413",
                    "yes": "827",
                    "no": "828"
                },
                "414": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 414"
                },
                "415": {
                    "answer": "Sorry, no matching LLMs were found. 415"
                },
                "416": {
                    "answer": "Sorry, no matching LLMs were found. 416"
                },
                "417": {
                    "answer": "Sorry, no matching LLMs were found. 417"
                },
                "418": {
                    "answer": "Sorry, no matching LLMs were found. 418"
                },
                "419": {
                    "answer": "Sorry, no matching LLMs were found. 419"
                },
                "420": {
                    "answer": "Sorry, no matching LLMs were found. 420"
                },
                "421": {
                    "answer": "Sorry, no matching LLMs were found. 421"
                },
                "422": {
                    "answer": "Sorry, no matching LLMs were found. 422"
                },
                "423": {
                    "answer": "Sorry, no matching LLMs were found. 423"
                },
                "424": {
                    "answer": "Sorry, no matching LLMs were found. 424"
                },
                "425": {
                    "answer": "Sorry, no matching LLMs were found. 425"
                },
                "426": {
                    "answer": "Sorry, no matching LLMs were found. 426"
                },
                "427": {
                    "answer": "Sorry, no matching LLMs were found. 427"
                },
                "428": {
                    "answer": "Sorry, no matching LLMs were found. 428"
                },
                "429": {
                    "answer": "Sorry, no matching LLMs were found. 429"
                },
                "430": {
                    "answer": "Sorry, no matching LLMs were found. 430"
                },
                "431": {
                    "answer": "Sorry, no matching LLMs were found. 431"
                },
                "432": {
                    "answer": "Sorry, no matching LLMs were found. 432"
                },
                "433": {
                    "answer": "Sorry, no matching LLMs were found. 433"
                },
                "434": {
                    "answer": "Sorry, no matching LLMs were found. 434"
                },
                "435": {
                    "answer": "Sorry, no matching LLMs were found. 435"
                },
                "436": {
                    "answer": "Sorry, no matching LLMs were found. 436"
                },
                "437": {
                    "answer": "Sorry, no matching LLMs were found. 437"
                },
                "438": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 438",
                    "yes": "877",
                    "no": "878"
                },
                "439": {
                    "answer": "Sorry, no matching LLMs were found. 439"
                },
                "440": {
                    "answer": "Sorry, no matching LLMs were found. 440"
                },
                "441": {
                    "answer": "Sorry, no matching LLMs were found. 441"
                },
                "442": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 442",
                    "yes": "885",
                    "no": "886"
                },
                "443": {
                    "answer": "Sorry, no matching LLMs were found. 443"
                },
                "444": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 444",
                    "yes": "889",
                    "no": "890"
                },
                "445": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 445",
                    "yes": "891",
                    "no": "892"
                },
                "446": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 446"
                },
                "447": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 447",
                    "yes": "895",
                    "no": "896"
                },
                "448": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 448",
                    "yes": "897",
                    "no": "898"
                },
                "449": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 449",
                    "yes": "899",
                    "no": "900"
                },
                "450": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 450",
                    "yes": "901",
                    "no": "902"
                },
                "451": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 451",
                    "yes": "903",
                    "no": "904"
                },
                "452": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 452",
                    "yes": "905",
                    "no": "906"
                },
                "453": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 453",
                    "yes": "907",
                    "no": "908"
                },
                "454": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 454",
                    "yes": "909",
                    "no": "910"
                },
                "455": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 455",
                    "yes": "911",
                    "no": "912"
                },
                "456": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 456",
                    "yes": "913",
                    "no": "914"
                },
                "457": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 457",
                    "yes": "915",
                    "no": "916"
                },
                "458": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 458",
                    "yes": "917",
                    "no": "918"
                },
                "459": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 459",
                    "yes": "919",
                    "no": "920"
                },
                "460": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 460",
                    "yes": "921",
                    "no": "922"
                },
                "461": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 461",
                    "yes": "923",
                    "no": "924"
                },
                "462": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 462",
                    "yes": "925",
                    "no": "926"
                },
                "463": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 463",
                    "yes": "927",
                    "no": "928"
                },
                "464": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 464",
                    "yes": "929",
                    "no": "930"
                },
                "465": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 465",
                    "yes": "931",
                    "no": "932"
                },
                "466": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 466",
                    "yes": "933",
                    "no": "934"
                },
                "467": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 467",
                    "yes": "935",
                    "no": "936"
                },
                "468": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 468",
                    "yes": "937",
                    "no": "938"
                },
                "469": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 469",
                    "yes": "939",
                    "no": "940"
                },
                "470": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 470",
                    "yes": "941",
                    "no": "942"
                },
                "471": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 471",
                    "yes": "943",
                    "no": "944"
                },
                "472": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 472",
                    "yes": "945",
                    "no": "946"
                },
                "473": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 473",
                    "yes": "947",
                    "no": "948"
                },
                "474": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 474",
                    "yes": "949",
                    "no": "950"
                },
                "475": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 475",
                    "yes": "951",
                    "no": "952"
                },
                "476": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 476",
                    "yes": "953",
                    "no": "954"
                },
                "477": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 477",
                    "yes": "955",
                    "no": "956"
                },
                "478": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 478"
                },
                "479": {
                    "answer": "Sorry, no matching LLMs were found. 479"
                },
                "480": {
                    "answer": "Sorry, no matching LLMs were found. 480"
                },
                "481": {
                    "answer": "Sorry, no matching LLMs were found. 481"
                },
                "482": {
                    "answer": "Sorry, no matching LLMs were found. 482"
                },
                "483": {
                    "answer": "Sorry, no matching LLMs were found. 483"
                },
                "484": {
                    "answer": "Sorry, no matching LLMs were found. 484"
                },
                "485": {
                    "answer": "Sorry, no matching LLMs were found. 485"
                },
                "486": {
                    "answer": "Sorry, no matching LLMs were found. 486"
                },
                "487": {
                    "answer": "Sorry, no matching LLMs were found. 487"
                },
                "488": {
                    "answer": "Sorry, no matching LLMs were found. 488"
                },
                "489": {
                    "answer": "Sorry, no matching LLMs were found. 489"
                },
                "490": {
                    "answer": "Sorry, no matching LLMs were found. 490"
                },
                "491": {
                    "answer": "Sorry, no matching LLMs were found. 491"
                },
                "492": {
                    "answer": "Sorry, no matching LLMs were found. 492"
                },
                "493": {
                    "answer": "Sorry, no matching LLMs were found. 493"
                },
                "494": {
                    "answer": "Sorry, no matching LLMs were found. 494"
                },
                "495": {
                    "answer": "Sorry, no matching LLMs were found. 495"
                },
                "496": {
                    "answer": "Sorry, no matching LLMs were found. 496"
                },
                "497": {
                    "answer": "Sorry, no matching LLMs were found. 497"
                },
                "498": {
                    "answer": "Sorry, no matching LLMs were found. 498"
                },
                "499": {
                    "answer": "Sorry, no matching LLMs were found. 499"
                },
                "500": {
                    "answer": "Sorry, no matching LLMs were found. 500"
                },
                "501": {
                    "answer": "Sorry, no matching LLMs were found. 501"
                },
                "502": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 502",
                    "yes": "1005",
                    "no": "1006"
                },
                "503": {
                    "answer": "Sorry, no matching LLMs were found. 503"
                },
                "504": {
                    "answer": "Sorry, no matching LLMs were found. 504"
                },
                "505": {
                    "answer": "Sorry, no matching LLMs were found. 505"
                },
                "506": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 506",
                    "yes": "1013",
                    "no": "1014"
                },
                "507": {
                    "answer": "Sorry, no matching LLMs were found. 507"
                },
                "508": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 508",
                    "yes": "1017",
                    "no": "1018"
                },
                "509": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 509",
                    "yes": "1019",
                    "no": "1020"
                },
                "510": {
                    "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform. 510"
                },
                "511": {
                    "answer": "Sorry, no matching LLMs were found. 511"
                },
                "512": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 512"
                },
                "513": {
                    "answer": "Sorry, no matching LLMs were found. 513"
                },
                "514": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 514"
                },
                "515": {
                    "answer": "Sorry, no matching LLMs were found. 515"
                },
                "516": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 516"
                },
                "517": {
                    "answer": "Sorry, no matching LLMs were found. 517"
                },
                "518": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 518"
                },
                "519": {
                    "answer": "Sorry, no matching LLMs were found. 519"
                },
                "520": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 520"
                },
                "521": {
                    "answer": "Sorry, no matching LLMs were found. 521"
                },
                "522": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 522"
                },
                "523": {
                    "answer": "Sorry, no matching LLMs were found. 523"
                },
                "524": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 524"
                },
                "525": {
                    "answer": "Sorry, no matching LLMs were found. 525"
                },
                "526": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 526"
                },
                "527": {
                    "answer": "Sorry, no matching LLMs were found. 527"
                },
                "528": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 528"
                },
                "529": {
                    "answer": "Sorry, no matching LLMs were found. 529"
                },
                "530": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 530"
                },
                "531": {
                    "answer": "Sorry, no matching LLMs were found. 531"
                },
                "532": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 532"
                },
                "533": {
                    "answer": "Sorry, no matching LLMs were found. 533"
                },
                "534": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 534"
                },
                "535": {
                    "answer": "Sorry, no matching LLMs were found. 535"
                },
                "536": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 536"
                },
                "537": {
                    "answer": "Sorry, no matching LLMs were found. 537"
                },
                "538": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 538"
                },
                "539": {
                    "answer": "Sorry, no matching LLMs were found. 539"
                },
                "540": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 540"
                },
                "541": {
                    "answer": "Sorry, no matching LLMs were found. 541"
                },
                "542": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 542"
                },
                "543": {
                    "answer": "Sorry, no matching LLMs were found. 543"
                },
                "544": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 544"
                },
                "545": {
                    "answer": "Sorry, no matching LLMs were found. 545"
                },
                "546": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 546"
                },
                "547": {
                    "answer": "Sorry, no matching LLMs were found. 547"
                },
                "548": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 548"
                },
                "549": {
                    "answer": "Sorry, no matching LLMs were found. 549"
                },
                "550": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 550"
                },
                "551": {
                    "answer": "Sorry, no matching LLMs were found. 551"
                },
                "552": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 552"
                },
                "553": {
                    "answer": "Sorry, no matching LLMs were found. 553"
                },
                "554": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 554"
                },
                "555": {
                    "answer": "Sorry, no matching LLMs were found. 555"
                },
                "556": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 556"
                },
                "557": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 557"
                },
                "558": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  558"
                },
                "559": {
                    "answer": "Sorry, no matching LLMs were found. 559"
                },
                "560": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 560"
                },
                "561": {
                    "answer": "Sorry, no matching LLMs were found. 561"
                },
                "562": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 562"
                },
                "563": {
                    "answer": "Sorry, no matching LLMs were found. 563"
                },
                "564": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 564"
                },
                "565": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 565"
                },
                "566": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 566"
                },
                "567": {
                    "answer": "Sorry, no matching LLMs were found. 567"
                },
                "568": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 568"
                },
                "569": {
                    "answer": "Sorry, no matching LLMs were found. 569"
                },
                "570": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 570"
                },
                "571": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 571"
                },
                "572": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 572"
                },
                "621": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  621"
                },
                "622": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 622"
                },
                "629": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 629"
                },
                "630": {
                    "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 630"
                },
                "635": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 635"
                },
                "636": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.  636"
                },
                "639": {
                    "answer": "Sorry, no matching LLMs were found. 639"
                },
                "640": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 640"
                },
                "641": {
                    "answer": "Sorry, no matching LLMs were found. 641"
                },
                "642": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 642"
                },
                "643": {
                    "answer": "Sorry, no matching LLMs were found. 643"
                },
                "644": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 644"
                },
                "645": {
                    "answer": "Sorry, no matching LLMs were found. 645"
                },
                "646": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 646"
                },
                "647": {
                    "answer": "Sorry, no matching LLMs were found. 647"
                },
                "648": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 648"
                },
                "649": {
                    "answer": "Sorry, no matching LLMs were found. 649"
                },
                "650": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 650"
                },
                "651": {
                    "answer": "Sorry, no matching LLMs were found. 651"
                },
                "652": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 652"
                },
                "653": {
                    "answer": "Sorry, no matching LLMs were found.  653"
                },
                "654": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 654"
                },
                "655": {
                    "answer": "Sorry, no matching LLMs were found. 655"
                },
                "656": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 656"
                },
                "657": {
                    "answer": "Sorry, no matching LLMs were found. 657"
                },
                "658": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 658"
                },
                "659": {
                    "answer": "Sorry, no matching LLMs were found. 659"
                },
                "660": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 660"
                },
                "661": {
                    "answer": "Sorry, no matching LLMs were found. 661"
                },
                "662": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 662"
                },
                "663": {
                    "answer": "Sorry, no matching LLMs were found. 663"
                },
                "664": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 664"
                },
                "665": {
                    "answer": "Sorry, no matching LLMs were found. 665"
                },
                "666": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 666"
                },
                "667": {
                    "answer": "Sorry, no matching LLMs were found. 667"
                },
                "668": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 668"
                },
                "669": {
                    "answer": "Sorry, no matching LLMs were found. 669"
                },
                "670": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 670"
                },
                "671": {
                    "answer": "Sorry, no matching LLMs were found. 671"
                },
                "672": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 672"
                },
                "673": {
                    "answer": "Sorry, no matching LLMs were found. 673"
                },
                "674": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 674"
                },
                "675": {
                    "answer": "Sorry, no matching LLMs were found. 675"
                },
                "676": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 676"
                },
                "677": {
                    "answer": "Sorry, no matching LLMs were found. 677"
                },
                "678": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 678"
                },
                "679": {
                    "answer": "Sorry, no matching LLMs were found. 679"
                },
                "680": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 680"
                },
                "681": {
                    "answer": "Sorry, no matching LLMs were found. 681"
                },
                "682": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 682"
                },
                "683": {
                    "answer": "Sorry, no matching LLMs were found.  683"
                },
                "684": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  684"
                },
                "685": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 685"
                },
                "686": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 686"
                },
                "687": {
                    "answer": "Sorry, no matching LLMs were found. 687"
                },
                "688": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 688"
                },
                "689": {
                    "answer": "Sorry, no matching LLMs were found. 689"
                },
                "690": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 690"
                },
                "691": {
                    "answer": "Sorry, no matching LLMs were found. 691"
                },
                "692": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 692"
                },
                "693": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 693"
                },
                "694": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 694"
                },
                "695": {
                    "answer": "Sorry, no matching LLMs were found. 695"
                },
                "696": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 696"
                },
                "697": {
                    "answer": "Sorry, no matching LLMs were found. 697"
                },
                "698": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 698"
                },
                "699": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 699"
                },
                "700": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 700"
                },
                "749": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 749"
                },
                "750": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 750"
                },
                "757": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 757"
                },
                "758": {
                    "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 758"
                },
                "763": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 763"
                },
                "764": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 764"
                },
                "767": {
                    "answer": "Sorry, no matching LLMs were found. 767"
                },
                "768": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 768"
                },
                "769": {
                    "answer": "Sorry, no matching LLMs were found. 769"
                },
                "770": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 770"
                },
                "771": {
                    "answer": "Sorry, no matching LLMs were found. 771"
                },
                "772": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 772"
                },
                "773": {
                    "answer": "Sorry, no matching LLMs were found. 773"
                },
                "774": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 774"
                },
                "775": {
                    "answer": "Sorry, no matching LLMs were found. 775"
                },
                "776": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 776"
                },
                "777": {
                    "answer": "Sorry, no matching LLMs were found. 777"
                },
                "778": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 778"
                },
                "779": {
                    "answer": "Sorry, no matching LLMs were found. 779"
                },
                "780": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 780"
                },
                "781": {
                    "answer": "Sorry, no matching LLMs were found. 781"
                },
                "782": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 782"
                },
                "783": {
                    "answer": "Sorry, no matching LLMs were found. 783"
                },
                "784": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 784"
                },
                "785": {
                    "answer": "Sorry, no matching LLMs were found. 785"
                },
                "786": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 786"
                },
                "787": {
                    "answer": "Sorry, no matching LLMs were found. 787"
                },
                "788": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 788"
                },
                "789": {
                    "answer": "Sorry, no matching LLMs were found. 789"
                },
                "790": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 790"
                },
                "791": {
                    "answer": "Sorry, no matching LLMs were found. 791"
                },
                "792": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 792"
                },
                "793": {
                    "answer": "Sorry, no matching LLMs were found.  793"
                },
                "794": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 794"
                },
                "795": {
                    "answer": "Sorry, no matching LLMs were found. 795"
                },
                "796": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 796"
                },
                "797": {
                    "answer": "Sorry, no matching LLMs were found. 797"
                },
                "798": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 798"
                },
                "799": {
                    "answer": "Sorry, no matching LLMs were found. 799"
                },
                "800": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 800"
                },
                "801": {
                    "answer": "Sorry, no matching LLMs were found. 801"
                },
                "802": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 802"
                },
                "803": {
                    "answer": "Sorry, no matching LLMs were found.  803"
                },
                "804": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 804"
                },
                "805": {
                    "answer": "Sorry, no matching LLMs were found. 805"
                },
                "806": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 806"
                },
                "807": {
                    "answer": "Sorry, no matching LLMs were found. 807"
                },
                "808": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 808"
                },
                "809": {
                    "answer": "Sorry, no matching LLMs were found. 809"
                },
                "810": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  810"
                },
                "811": {
                    "answer": "Sorry, no matching LLMs were found.  811"
                },
                "812": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 812"
                },
                "813": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 813"
                },
                "814": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 814"
                },
                "815": {
                    "answer": "Sorry, no matching LLMs were found. 815"
                },
                "816": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 816"
                },
                "817": {
                    "answer": "Sorry, no matching LLMs were found. 817"
                },
                "818": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 818"
                },
                "819": {
                    "answer": "Sorry, no matching LLMs were found. 819"
                },
                "820": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 820"
                },
                "821": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.  821"
                },
                "822": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 822"
                },
                "823": {
                    "answer": "Sorry, no matching LLMs were found. 823"
                },
                "824": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 824"
                },
                "825": {
                    "answer": "The LLM we recommend you use is: OphGLM-6.2B (*). Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br> 825"
                },
                "826": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 826"
                },
                "827": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 827"
                },
                "828": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 828"
                },
                "877": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 877"
                },
                "878": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 878"
                },
                "885": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 885"
                },
                "886": {
                    "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 886"
                },
                "889": {
                    "answer": "The LLM we recommend you use is: OphGLM-6.2B (*). Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br> 889"
                },
                "890": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 890"
                },
                "891": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 891"
                },
                "892": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.  892"
                },
                "895": {
                    "answer": "Sorry, no matching LLMs were found. 895"
                },
                "896": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 896"
                },
                "897": {
                    "answer": "Sorry, no matching LLMs were found.  897"
                },
                "898": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 898"
                },
                "899": {
                    "answer": "Sorry, no matching LLMs were found. 899"
                },
                "900": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  900"
                },
                "901": {
                    "answer": "Sorry, no matching LLMs were found. 901"
                },
                "902": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 902"
                },
                "903": {
                    "answer": "Sorry, no matching LLMs were found. 903"
                },
                "904": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 904"
                },
                "905": {
                    "answer": "Sorry, no matching LLMs were found. 905"
                },
                "906": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 906"
                },
                "907": {
                    "answer": "Sorry, no matching LLMs were found. 907"
                },
                "908": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 908"
                },
                "909": {
                    "answer": "Sorry, no matching LLMs were found. 909"
                },
                "910": {
                    "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 910"
                },
                "911": {
                    "answer": "Sorry, no matching LLMs were found. 911"
                },
                "912": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 912"
                },
                "913": {
                    "answer": "Sorry, no matching LLMs were found. 913"
                },
                "914": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 914"
                },
                "915": {
                    "answer": "Sorry, no matching LLMs were found. 915"
                },
                "916": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 916"
                },
                "917": {
                    "answer": "Sorry, no matching LLMs were found. 917"
                },
                "918": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 918"
                },
                "919": {
                    "answer": "Sorry, no matching LLMs were found. 919"
                },
                "920": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 920"
                },
                "921": {
                    "answer": "Sorry, no matching LLMs were found. 921"
                },
                "922": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 922"
                },
                "923": {
                    "answer": "Sorry, no matching LLMs were found.  923"
                },
                "924": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.924"
                },
                "925": {
                    "answer": "Sorry, no matching LLMs were found. 925"
                },
                "926": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.  926"
                },
                "927": {
                    "answer": "Sorry, no matching LLMs were found. 927"
                },
                "928": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 928"
                },
                "929": {
                    "answer": "Sorry, no matching LLMs were found. 929"
                },
                "930": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  930"
                },
                "931": {
                    "answer": "Sorry, no matching LLMs were found. 931"
                },
                "932": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 932"
                },
                "933": {
                    "answer": "Sorry, no matching LLMs were found. 933"
                },
                "934": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  934"
                },
                "935": {
                    "answer": "Sorry, no matching LLMs were found. 935"
                },
                "936": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 936"
                },
                "937": {
                    "answer": "Sorry, no matching LLMs were found. 937"
                },
                "938": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  938"
                },
                "939": {
                    "answer": "Sorry, no matching LLMs were found.  939"
                },
                "940": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 940"
                },
                "941": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 941"
                },
                "942": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 942"
                },
                "943": {
                    "answer": "Sorry, no matching LLMs were found. 943"
                },
                "944": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.  944"
                },
                "945": {
                    "answer": "Sorry, no matching LLMs were found. 945"
                },
                "946": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 946"
                },
                "947": {
                    "answer": "Sorry, no matching LLMs were found. 947"
                },
                "948": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 948"
                },
                "949": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 949"
                },
                "950": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 950"
                },
                "951": {
                    "answer": "Sorry, no matching LLMs were found. 951"
                },
                "952": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens. 952"
                },
                "953": {
                    "answer": "The LLM we recommend you use is: OphGLM-6.2B (*). Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br> 953"
                },
                "954": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 954"
                },
                "955": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 955"
                },
                "956": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 956"
                },
                "1005": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 1005"
                },
                "1006": {
                    "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 1006"
                },
                "1013": {
                    "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40 GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 1013"
                },
                "1014": {
                    "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40 GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank'>MeLLaMA GitHub</a>. 1014"
                },
                "1017": {
                    "answer": "The LLM we recommend you use is: OphGLM-6.2B (*). Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br> 1017"
                },
                "1018": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually. 1018"
                },
                "1019": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. 1019"
                },
                "1020": {
                    "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 1020"
                }
            };
                if (data[node] && data[node].question) {
                    const questionDiv = document.createElement('div');
                    questionDiv.classList.add('question');

                    // Display fixed question number
                    const questionNumberElem = document.createElement('p');
                    questionNumberElem.classList.add('questionNumber');
                    questionNumberElem.innerText = \`Question \${questionNumber}:\`;
                    questionDiv.appendChild(questionNumberElem);

                    // Display question text (with clickable keywords)
                    const questionText = document.createElement('p');
                    questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
                    questionDiv.appendChild(questionText);

                    // Create Yes button
                    const yesButton = document.createElement('button');
                    yesButton.innerText = 'Yes';
                    yesButton.classList.toggle('selected', selectedChoice === 'yes');
                    yesButton.onclick = function () {
                        updateHistory(node, 'yes', questionNumber);
                        currentNode = data[node].yes;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(yesButton);

                    // Create No button
                    const noButton = document.createElement('button');
                    noButton.innerText = 'No';
                    noButton.classList.toggle('selected', selectedChoice === 'no');
                    noButton.onclick = function () {
                        updateHistory(node, 'no', questionNumber);
                        currentNode = data[node].no;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(noButton);

                    // Append the question block to the container
                    questionContainer.appendChild(questionDiv);

                    // Attach event listeners to keywords
                    const highlightElements = questionDiv.querySelectorAll('.highlight');
                    highlightElements.forEach(function (element) {
                        element.addEventListener('click', function () {
                            const keyword = element.getAttribute('data-keyword');
                            showDescription(keyword);
                        });
                    });

                } else if (data[node] && data[node].answer) {
                    // Create a div for the final answer
                    const answerDiv = document.createElement('div');
                    answerDiv.classList.add('answer');

                    const answerText = document.createElement('p');
                    answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
                    answerDiv.appendChild(answerText);

                    // Append the answer to the container
                    questionContainer.appendChild(answerDiv);
                } else {
                    // Handle if no more questions or no data found
                    const noMoreQuestions = document.createElement('p');
                    noMoreQuestions.innerText = "No more questions available.";
                    questionContainer.appendChild(noMoreQuestions);
                }
            }

            function showDescription(keyword) {
            const descriptions = {
                "textual data": "It refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
                "generate clinical surgery and nursing related text": "It refers to the ability to perform at least one similar task, such as the generation and simplification of surgical informed consent forms, the generation of clinical letters, etc.",
                "perform tasks related to disease prediction":"It refers to the ability to perform at least one similar task, such as the prediction of disease risk and survival, etc.",
                "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
                "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
                "extract information from clinical text":"It refers to the ability to perform at least one similar task, such as extracting ICD codes from clinical records, clinical named entity recognition, etc.",
                "answer medical questions":"medical questions includes multiple-choice and open-ended questions (and may also include visual questions, i.e., where there are medical images in the input) on tasks related to treatment and hospitalization. Open questions are questions that require a free-form text answer. These questions usually require more in-depth thinking and personalized answers rather than choosing from a limited number of options. These questions may relate to treatment options, surgical risks, medication choices, rehabilitation programs, and more.",
                "summarize clinical records":"includes summarizing the conversation between the doctor and the patient, summarizing the medical questions asked by the patient, etc."
            };
                const descriptionText = descriptions[keyword];
                if (descriptionText) {
                    document.getElementById('descriptionText').innerText = descriptionText;
                    document.getElementById('descriptionContainer').style.display = 'block';
                }
            }

            document.getElementById('closeButton').addEventListener('click', function () {
                document.getElementById('descriptionContainer').style.display = 'none';
            });

            function updateHistory(node, choice, questionNumber) {
                // Check if we are revisiting a question and modify history accordingly
                const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
                if (historyIndex !== -1) {
                    questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
                }

                // Add current question to history
                questionHistory.push({ node, choice, number: questionNumber });
            }

            // Start the first question
            displayQuestion(currentNode, 0);
                                `;
                                document.body.appendChild(script);
                            } else if (page === 's5.html') {
                                // Initialize Q&A logic after loading the HTML
                                const script = document.createElement('script');
                                script.innerHTML = `

            var currentNode = "0"; // Starting node
            var questionHistory = []; // Keeps track of answered questions

            // Function to display a question based on the node
            function displayQuestion(node, questionIndex) {
                const questionContainer = document.getElementById('questionContainer');
                // Clear questions beyond this index
                questionContainer.innerHTML = '';

                // Display history
                questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

                // Display the current question
                appendQuestion(node, null, questionIndex + 1);
            }

            function appendQuestion(node, selectedChoice, questionNumber) {
                const questionContainer = document.getElementById('questionContainer');
                const data = {
                "0": {
                    "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "1",
                    "no": "2"
                },
                "1": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "3",
                    "no": "4"
                },
                "2": {
                    "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
                    "yes": "5",
                    "no": "6"
                },
                "3": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "7",
                    "no": "8"
                },
                "4": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "9",
                    "no": "10"
                },
                "5": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "11",
                    "no": "12"
                },
                "6": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "13",
                    "no": "14"
                },
                "7": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 7",
                    "yes": "15",
                    "no": "16"
                },
                "8": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 8",
                    "yes": "17",
                    "no": "18"
                },
                "9": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 9",
                    "yes": "19",
                    "no": "20"
                },
                "10": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 10",
                    "yes": "21",
                    "no": "22"
                },
                "11": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 11",
                    "yes": "23",
                    "no": "24"
                },
                "12": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 12",
                    "yes": "25",
                    "no": "26"
                },
                "13": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 13",
                    "yes": "27",
                    "no": "28"
                },
                "14": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>? 14",
                    "yes": "29",
                    "no": "30"
                },
                "15": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 15",
                    "yes": "31",
                    "no": "32"
                },
                "16": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 16",
                    "yes": "33",
                    "no": "34"
                },
                "17": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 17",
                    "yes": "35",
                    "no": "36"
                },
                "18": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 18",
                    "yes": "37",
                    "no": "38"
                },
                "19": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 19",
                    "yes": "39",
                    "no": "40"
                },
                "20": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 20",
                    "yes": "41",
                    "no": "42"
                },
                "21": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 21",
                    "yes": "43",
                    "no": "44"
                },
                "22": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 22",
                    "yes": "45",
                    "no": "46"
                },
                "23": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 23",
                    "yes": "47",
                    "no": "48"
                },
                "24": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 24",
                    "yes": "49",
                    "no": "50"
                },
                "25": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 25",
                    "yes": "51",
                    "no": "52"
                },
                "26": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 26",
                    "yes": "53",
                    "no": "54"
                },
                "27": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 27",
                    "yes": "55",
                    "no": "56"
                },
                "28": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 28",
                    "yes": "57",
                    "no": "58"
                },
                "29": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 29",
                    "yes": "59",
                    "no": "60"
                },
                "30": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>? 30",
                    "yes": "61",
                    "no": "62"
                },
                "31": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 31",
                    "yes": "63",
                    "no": "64"
                },
                "32": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 32",
                    "yes": "65",
                    "no": "66"
                },
                "33": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 33",
                    "yes": "67",
                    "no": "68"
                },
                "34": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 34",
                    "yes": "69",
                    "no": "70"
                },
                "35": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 35",
                    "yes": "71",
                    "no": "72"
                },
                "36": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 36",
                    "yes": "73",
                    "no": "74"
                },
                "37": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 37",
                    "yes": "75",
                    "no": "76"
                },
                "38": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 38",
                    "yes": "77",
                    "no": "78"
                },
                "39": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 39",
                    "yes": "79",
                    "no": "80"
                },
                "40": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 40",
                    "yes": "81",
                    "no": "82"
                },
                "41": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 41",
                    "yes": "83",
                    "no": "84"
                },
                "42": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 42",
                    "yes": "85",
                    "no": "86"
                },
                "43": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 43",
                    "yes": "87",
                    "no": "88"
                },
                "44": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 44",
                    "yes": "89",
                    "no": "90"
                },
                "45": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 45",
                    "yes": "91",
                    "no": "92"
                },
                "46": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 46",
                    "yes": "93",
                    "no": "94"
                },
                "47": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 47",
                    "yes": "95",
                    "no": "96"
                },
                "48": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 48",
                    "yes": "97",
                    "no": "98"
                },
                "49": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 49",
                    "yes": "99",
                    "no": "100"
                },
                "50": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 50",
                    "yes": "101",
                    "no": "102"
                },
                "51": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 51",
                    "yes": "103",
                    "no": "104"
                },
                "52": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 52",
                    "yes": "105",
                    "no": "106"
                },
                "53": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 53",
                    "yes": "107",
                    "no": "108"
                },
                "54": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 54",
                    "yes": "109",
                    "no": "110"
                },
                "55": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 55",
                    "yes": "111",
                    "no": "112"
                },
                "56": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 56",
                    "yes": "113",
                    "no": "114"
                },
                "57": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 57",
                    "yes": "115",
                    "no": "116"
                },
                "58": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 58",
                    "yes": "117",
                    "no": "118"
                },
                "59": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 59",
                    "yes": "119",
                    "no": "120"
                },
                "60": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 60",
                    "yes": "121",
                    "no": "122"
                },
                "61": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 61",
                    "yes": "123",
                    "no": "124"
                },
                "62": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate discharge-related text'>generate discharge-related text</span>? 62",
                    "yes": "125",
                    "no": "126"
                },
                "63": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 63",
                    "yes": "127",
                    "no": "128"
                },
                "64": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 64",
                    "yes": "129",
                    "no": "130"
                },
                "65": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 65",
                    "yes": "131",
                    "no": "132"
                },
                "66": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 66",
                    "yes": "133",
                    "no": "134"
                },
                "67": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 67",
                    "yes": "135",
                    "no": "136"
                },
                "68": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 68",
                    "yes": "137",
                    "no": "138"
                },
                "69": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 69",
                    "yes": "139",
                    "no": "140"
                },
                "70": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 70",
                    "yes": "141",
                    "no": "142"
                },
                "71": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 71",
                    "yes": "143",
                    "no": "144"
                },
                "72": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 72",
                    "yes": "145",
                    "no": "146"
                },
                "73": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 73",
                    "yes": "147",
                    "no": "148"
                },
                "74": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 74",
                    "yes": "149",
                    "no": "150"
                },
                "75": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 75",
                    "yes": "151",
                    "no": "152"
                },
                "76": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 76",
                    "yes": "153",
                    "no": "154"
                },
                "77": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 77",
                    "yes": "155",
                    "no": "156"
                },
                "78": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 78",
                    "yes": "157",
                    "no": "158"
                },
                "79": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 79",
                    "yes": "159",
                    "no": "160"
                },
                "80": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 80",
                    "yes": "161",
                    "no": "162"
                },
                "81": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 81",
                    "yes": "163",
                    "no": "164"
                },
                "82": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 82",
                    "yes": "165",
                    "no": "166"
                },
                "83": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 83",
                    "yes": "167",
                    "no": "168"
                },
                "84": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 84",
                    "yes": "169",
                    "no": "170"
                },
                "85": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 85",
                    "yes": "171",
                    "no": "172"
                },
                "86": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 86",
                    "yes": "173",
                    "no": "174"
                },
                "87": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 87",
                    "yes": "175",
                    "no": "176"
                },
                "88": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 88",
                    "yes": "177",
                    "no": "178"
                },
                "89": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 89",
                    "yes": "179",
                    "no": "180"
                },
                "90": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 90",
                    "yes": "181",
                    "no": "182"
                },
                "91": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 91",
                    "yes": "183",
                    "no": "184"
                },
                "92": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 92",
                    "yes": "185",
                    "no": "186"
                },
                "93": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 93",
                    "yes": "187",
                    "no": "188"
                },
                "94": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 94",
                    "yes": "189",
                    "no": "190"
                },
                "95": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 95",
                    "yes": "191",
                    "no": "192"
                },
                "96": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 96",
                    "yes": "193",
                    "no": "194"
                },
                "97": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 97",
                    "yes": "195",
                    "no": "196"
                },
                "98": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 98",
                    "yes": "197",
                    "no": "198"
                },
                "99": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 99",
                    "yes": "199",
                    "no": "200"
                },
                "100": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 100",
                    "yes": "201",
                    "no": "202"
                },
                "101": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 101",
                    "yes": "203",
                    "no": "204"
                },
                "102": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 102",
                    "yes": "205",
                    "no": "206"
                },
                "103": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 103",
                    "yes": "207",
                    "no": "208"
                },
                "104": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 104",
                    "yes": "209",
                    "no": "210"
                },
                "105": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 105",
                    "yes": "211",
                    "no": "212"
                },
                "106": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 106",
                    "yes": "213",
                    "no": "214"
                },
                "107": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 107",
                    "yes": "215",
                    "no": "216"
                },
                "108": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 108",
                    "yes": "217",
                    "no": "218"
                },
                "109": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 109",
                    "yes": "219",
                    "no": "220"
                },
                "110": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 110",
                    "yes": "221",
                    "no": "222"
                },
                "111": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 111",
                    "yes": "223",
                    "no": "224"
                },
                "112": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 112",
                    "yes": "225",
                    "no": "226"
                },
                "113": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 113",
                    "yes": "227",
                    "no": "228"
                },
                "114": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 114",
                    "yes": "229",
                    "no": "230"
                },
                "115": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 115",
                    "yes": "231",
                    "no": "232"
                },
                "116": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 116",
                    "yes": "233",
                    "no": "234"
                },
                "117": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 117",
                    "yes": "235",
                    "no": "236"
                },
                "118": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 118",
                    "yes": "237",
                    "no": "238"
                },
                "119": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 119",
                    "yes": "239",
                    "no": "240"
                },
                "120": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 120",
                    "yes": "241",
                    "no": "242"
                },
                "121": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 121",
                    "yes": "243",
                    "no": "244"
                },
                "122": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 122",
                    "yes": "245",
                    "no": "246"
                },
                "123": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 123",
                    "yes": "247",
                    "no": "248"
                },
                "124": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 124",
                    "yes": "249",
                    "no": "250"
                },
                "125": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 125",
                    "yes": "251",
                    "no": "252"
                },
                "126": {
                    "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>? 126",
                    "yes": "253",
                    "no": "254"
                },
                "127": {
                    "answer": "Sorry, no matching LLMs were found. 127"
                },
                "128": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 128",
                    "yes": "257",
                    "no": "258"
                },
                "129": {
                    "answer": "Sorry, no matching LLMs were found. 129"
                },
                "130": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 130",
                    "yes": "261",
                    "no": "262"
                },
                "131": {
                    "answer": "Sorry, no matching LLMs were found. 131"
                },
                "132": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 132",
                    "yes": "265",
                    "no": "266"
                },
                "133": {
                    "answer": "Sorry, no matching LLMs were found. 133"
                },
                "134": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 134",
                    "yes": "269",
                    "no": "270"
                },
                "135": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 135",
                    "yes": "271",
                    "no": "272"
                },
                "136": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 136",
                    "yes": "273",
                    "no": "274"
                },
                "137": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 137",
                    "yes": "275",
                    "no": "276"
                },
                "138": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 138",
                    "yes": "277",
                    "no": "278"
                },
                "139": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 139",
                    "yes": "279",
                    "no": "280"
                },
                "140": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 140",
                    "yes": "281",
                    "no": "282"
                },
                "141": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 141",
                    "yes": "283",
                    "no": "284"
                },
                "142": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 142"
                },
                "143": {
                    "answer": "Sorry, no matching LLMs were found. 143"
                },
                "144": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 144",
                    "yes": "289",
                    "no": "290"
                },
                "145": {
                    "answer": "Sorry, no matching LLMs were found. 145"
                },
                "146": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 146",
                    "yes": "293",
                    "no": "294"
                },
                "147": {
                    "answer": "Sorry, no matching LLMs were found. 147"
                },
                "148": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 148",
                    "yes": "297",
                    "no": "298"
                },
                "149": {
                    "answer": "Sorry, no matching LLMs were found. 149"
                },
                "150": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 150",
                    "yes": "301",
                    "no": "302"
                },
                "151": {
                    "answer": "Sorry, no matching LLMs were found. 151"
                },
                "152": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 152",
                    "yes": "305",
                    "no": "306"
                },
                "153": {
                    "answer": "Sorry, no matching LLMs were found. 153"
                },
                "154": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 154",
                    "yes": "309",
                    "no": "310"
                },
                "155": {
                    "answer": "Sorry, no matching LLMs were found. 155"
                },
                "156": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 156",
                    "yes": "313",
                    "no": "314"
                },
                "157": {
                    "answer": "Sorry, no matching LLMs were found. 157"
                },
                "158": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 158"
                },
                "159": {
                    "answer": "Sorry, no matching LLMs were found. 159"
                },
                "160": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 160",
                    "yes": "321",
                    "no": "322"
                },
                "161": {
                    "answer": "Sorry, no matching LLMs were found. 161"
                },
                "162": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 162",
                    "yes": "325",
                    "no": "326"
                },
                "163": {
                    "answer": "Sorry, no matching LLMs were found. 163"
                },
                "164": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 164",
                    "yes": "329",
                    "no": "330"
                },
                "165": {
                    "answer": "Sorry, no matching LLMs were found. 165"
                },
                "166": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 166",
                    "yes": "333",
                    "no": "334"
                },
                "167": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 167",
                    "yes": "335",
                    "no": "336"
                },
                "168": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 168",
                    "yes": "337",
                    "no": "338"
                },
                "169": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 169",
                    "yes": "339",
                    "no": "340"
                },
                "170": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 170",
                    "yes": "341",
                    "no": "342"
                },
                "171": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 171",
                    "yes": "343",
                    "no": "344"
                },
                "172": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 172",
                    "yes": "345",
                    "no": "346"
                },
                "173": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 173",
                    "yes": "347",
                    "no": "348"
                },
                "174": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 174"
                },
                "175": {
                    "answer": "Sorry, no matching LLMs were found. 175"
                },
                "176": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 176",
                    "yes": "353",
                    "no": "354"
                },
                "177": {
                    "answer": "Sorry, no matching LLMs were found. 177"
                },
                "178": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 178",
                    "yes": "357",
                    "no": "358"
                },
                "179": {
                    "answer": "Sorry, no matching LLMs were found. 179"
                },
                "180": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 180",
                    "yes": "361",
                    "no": "362"
                },
                "181": {
                    "answer": "Sorry, no matching LLMs were found. 181"
                },
                "182": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 182",
                    "yes": "365",
                    "no": "366"
                },
                "183": {
                    "answer": "Sorry, no matching LLMs were found. 183"
                },
                "184": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 184",
                    "yes": "369",
                    "no": "370"
                },
                "185": {
                    "answer": "Sorry, no matching LLMs were found. 185"
                },
                "186": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 186",
                    "yes": "373",
                    "no": "374"
                },
                "187": {
                    "answer": "Sorry, no matching LLMs were found. 187"
                },
                "188": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 188",
                    "yes": "377",
                    "no": "378"
                },
                "189": {
                    "answer": "Sorry, no matching LLMs were found. 189"
                },
                "190": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 190"
                },
                "191": {
                    "answer": "Sorry, no matching LLMs were found. 191"
                },
                "192": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 192",
                    "yes": "385",
                    "no": "386"
                },
                "193": {
                    "answer": "Sorry, no matching LLMs were found. 193"
                },
                "194": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 194",
                    "yes": "389",
                    "no": "390"
                },
                "195": {
                    "answer": "Sorry, no matching LLMs were found. 195"
                },
                "196": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 196",
                    "yes": "393",
                    "no": "394"
                },
                "197": {
                    "answer": "Sorry, no matching LLMs were found. 197"
                },
                "198": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 198",
                    "yes": "397",
                    "no": "398"
                },
                "199": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 199",
                    "yes": "399",
                    "no": "400"
                },
                "200": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 200",
                    "yes": "401",
                    "no": "402"
                },
                "201": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 201",
                    "yes": "403",
                    "no": "404"
                },
                "202": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 202",
                    "yes": "405",
                    "no": "406"
                },
                "203": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 203",
                    "yes": "407",
                    "no": "408"
                },
                "204": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 204",
                    "yes": "409",
                    "no": "410"
                },
                "205": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 205",
                    "yes": "411",
                    "no": "412"
                },
                "206": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 206"
                },
                "207": {
                    "answer": "Sorry, no matching LLMs were found. 207"
                },
                "208": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 208",
                    "yes": "417",
                    "no": "418"
                },
                "209": {
                    "answer": "Sorry, no matching LLMs were found. 209"
                },
                "210": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 210",
                    "yes": "421",
                    "no": "422"
                },
                "211": {
                    "answer": "Sorry, no matching LLMs were found. 211"
                },
                "212": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 212",
                    "yes": "425",
                    "no": "426"
                },
                "213": {
                    "answer": "Sorry, no matching LLMs were found. 213"
                },
                "214": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 214",
                    "yes": "429",
                    "no": "430"
                },
                "215": {
                    "answer": "Sorry, no matching LLMs were found. 215"
                },
                "216": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 216",
                    "yes": "433",
                    "no": "434"
                },
                "217": {
                    "answer": "Sorry, no matching LLMs were found. 217"
                },
                "218": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 218",
                    "yes": "437",
                    "no": "438"
                },
                "219": {
                    "answer": "Sorry, no matching LLMs were found. 219"
                },
                "220": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 220",
                    "yes": "441",
                    "no": "442"
                },
                "221": {
                    "answer": "Sorry, no matching LLMs were found. 221"
                },
                "222": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 222"
                },
                "223": {
                    "answer": "Sorry, no matching LLMs were found. 223"
                },
                "224": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 224",
                    "yes": "449",
                    "no": "450"
                },
                "225": {
                    "answer": "Sorry, no matching LLMs were found. 225"
                },
                "226": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 226",
                    "yes": "453",
                    "no": "454"
                },
                "227": {
                    "answer": "Sorry, no matching LLMs were found. 227"
                },
                "228": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 228",
                    "yes": "457",
                    "no": "458"
                },
                "229": {
                    "answer": "Sorry, no matching LLMs were found. 229"
                },
                "230": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 230",
                    "yes": "461",
                    "no": "462"
                },
                "231": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 231",
                    "yes": "463",
                    "no": "464"
                },
                "232": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 232",
                    "yes": "465",
                    "no": "466"
                },
                "233": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 233",
                    "yes": "467",
                    "no": "468"
                },
                "234": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 234",
                    "yes": "469",
                    "no": "470"
                },
                "235": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 235",
                    "yes": "471",
                    "no": "472"
                },
                "236": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 236",
                    "yes": "473",
                    "no": "474"
                },
                "237": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 237",
                    "yes": "475",
                    "no": "476"
                },
                "238": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 238"
                },
                "239": {
                    "answer": "Sorry, no matching LLMs were found. 239"
                },
                "240": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 240",
                    "yes": "481",
                    "no": "482"
                },
                "241": {
                    "answer": "Sorry, no matching LLMs were found. 241"
                },
                "242": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 242",
                    "yes": "485",
                    "no": "486"
                },
                "243": {
                    "answer": "Sorry, no matching LLMs were found. 243"
                },
                "244": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 244",
                    "yes": "489",
                    "no": "490"
                },
                "245": {
                    "answer": "Sorry, no matching LLMs were found. 245"
                },
                "246": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 246",
                    "yes": "493",
                    "no": "494"
                },
                "247": {
                    "answer": "Sorry, no matching LLMs were found. 247"
                },
                "248": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 248",
                    "yes": "497",
                    "no": "498"
                },
                "249": {
                    "answer": "Sorry, no matching LLMs were found. 249"
                },
                "250": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 250",
                    "yes": "501",
                    "no": "502"
                },
                "251": {
                    "answer": "Sorry, no matching LLMs were found. 251"
                },
                "252": {
                    "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>? 252",
                    "yes": "505",
                    "no": "506"
                },
                "253": {
                    "answer": "Sorry, no matching LLMs were found. 253"
                },
                "254": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 254"
                },
                "257": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 257"
                },
                "258": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 258"
                },
                "261": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 261"
                },
                "262": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 262"
                },
                "265": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 265"
                },
                "266": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 266"
                },
                "269": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 269"
                },
                "270": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 270"
                },
                "271": {
                    "answer": "Sorry, no matching LLMs were found. 271"
                },
                "272": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 272"
                },
                "273": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  273"
                },
                "274": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 274"
                },
                "275": {
                    "answer": "Sorry, no matching LLMs were found. 275"
                },
                "276": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 276"
                },
                "277": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 277"
                },
                "278": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 278"
                },
                "279": {
                    "answer": "Sorry, no matching LLMs were found. 279"
                },
                "280": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 280"
                },
                "281": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 281"
                },
                "282": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.   282"
                },
                "283": {
                    "answer": "Sorry, no matching LLMs were found. 283"
                },
                "284": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 284"
                },
                "289": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 289"
                },
                "290": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 290"
                },
                "293": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 293"
                },
                "294": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 294"
                },
                "297": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 297"
                },
                "298": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 298"
                },
                "301": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 301"
                },
                "302": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 302"
                },
                "305": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 305"
                },
                "306": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 306"
                },
                "309": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 309"
                },
                "310": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 310"
                },
                "313": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 313"
                },
                "314": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 314"
                },
                "321": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 321"
                },
                "322": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 322"
                },
                "325": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 325"
                },
                "326": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 326"
                },
                "329": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 329"
                },
                "330": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 330"
                },
                "333": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 333"
                },
                "334": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 334"
                },
                "335": {
                    "answer": "Sorry, no matching LLMs were found. 335"
                },
                "336": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 336"
                },
                "337": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  337"
                },
                "338": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 338"
                },
                "339": {
                    "answer": "Sorry, no matching LLMs were found. 339"
                },
                "340": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 340"
                },
                "341": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 341"
                },
                "342": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 342"
                },
                "343": {
                    "answer": "Sorry, no matching LLMs were found. 343"
                },
                "344": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 344"
                },
                "345": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 345"
                },
                "346": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  346"
                },
                "347": {
                    "answer": "Sorry, no matching LLMs were found. 347"
                },
                "348": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 348"
                },
                "353": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  353"
                },
                "354": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 354"
                },
                "357": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 357"
                },
                "358": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 358"
                },
                "361": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 361"
                },
                "362": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 362"
                },
                "365": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 365"
                },
                "366": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 366"
                },
                "369": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 369"
                },
                "370": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 370"
                },
                "373": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 373"
                },
                "374": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 374"
                },
                "377": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 377"
                },
                "378": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 378"
                },
                "385": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 385"
                },
                "386": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 386"
                },
                "389": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 389"
                },
                "390": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 390"
                },
                "393": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 393"
                },
                "394": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 394"
                },
                "397": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 397"
                },
                "398": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 398"
                },
                "399": {
                    "answer": "Sorry, no matching LLMs were found.  399"
                },
                "400": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 400"
                },
                "401": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 401"
                },
                "402": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 402"
                },
                "403": {
                    "answer": "Sorry, no matching LLMs were found. 403"
                },
                "404": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 404"
                },
                "405": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 405"
                },
                "406": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 406"
                },
                "407": {
                    "answer": "Sorry, no matching LLMs were found. 407"
                },
                "408": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 408"
                },
                "409": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 409"
                },
                "410": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 410"
                },
                "411": {
                    "answer": "Sorry, no matching LLMs were found. 411"
                },
                "412": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 412"
                },
                "417": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 417"
                },
                "418": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 418"
                },
                "421": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 421"
                },
                "422": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 422"
                },
                "425": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 425"
                },
                "426": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 426"
                },
                "429": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 429"
                },
                "430": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 430"
                },
                "433": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  433"
                },
                "434": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  434"
                },
                "437": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 437"
                },
                "438": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 438"
                },
                "441": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 441"
                },
                "442": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 442"
                },
                "449": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 449"
                },
                "450": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 450"
                },
                "453": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 453"
                },
                "454": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  454"
                },
                "457": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  457"
                },
                "458": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 458"
                },
                "461": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  461"
                },
                "462": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 462"
                },
                "463": {
                    "answer": "Sorry, no matching LLMs were found.  463"
                },
                "464": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  464"
                },
                "465": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 465"
                },
                "466": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 466"
                },
                "467": {
                    "answer": "Sorry, no matching LLMs were found. 467"
                },
                "468": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 468"
                },
                "469": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 469"
                },
                "470": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.  470"
                },
                "471": {
                    "answer": "Sorry, no matching LLMs were found. 471"
                },
                "472": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 472"
                },
                "473": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 473"
                },
                "474": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  474"
                },
                "475": {
                    "answer": "Sorry, no matching LLMs were found.  475"
                },
                "476": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  476"
                },
                "481": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 481"
                },
                "482": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 482"
                },
                "485": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 485"
                },
                "486": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 486"
                },
                "489": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 489"
                },
                "490": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 490"
                },
                "493": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 493"
                },
                "494": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 494"
                },
                "497": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 497"
                },
                "498": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 498"
                },
                "501": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 501"
                },
                "502": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 502"
                },
                "505": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  505"
                },
                "506": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 506"
                }
            };
                if (data[node] && data[node].question) {
                    const questionDiv = document.createElement('div');
                    questionDiv.classList.add('question');

                    // Display fixed question number
                    const questionNumberElem = document.createElement('p');
                    questionNumberElem.classList.add('questionNumber');
                    questionNumberElem.innerText = \`Question \${questionNumber}:\`;
                    questionDiv.appendChild(questionNumberElem);

                    // Display question text (with clickable keywords)
                    const questionText = document.createElement('p');
                    questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
                    questionDiv.appendChild(questionText);

                    // Create Yes button
                    const yesButton = document.createElement('button');
                    yesButton.innerText = 'Yes';
                    yesButton.classList.toggle('selected', selectedChoice === 'yes');
                    yesButton.onclick = function () {
                        updateHistory(node, 'yes', questionNumber);
                        currentNode = data[node].yes;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(yesButton);

                    // Create No button
                    const noButton = document.createElement('button');
                    noButton.innerText = 'No';
                    noButton.classList.toggle('selected', selectedChoice === 'no');
                    noButton.onclick = function () {
                        updateHistory(node, 'no', questionNumber);
                        currentNode = data[node].no;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(noButton);

                    // Append the question block to the container
                    questionContainer.appendChild(questionDiv);

                    // Attach event listeners to keywords
                    const highlightElements = questionDiv.querySelectorAll('.highlight');
                    highlightElements.forEach(function (element) {
                        element.addEventListener('click', function () {
                            const keyword = element.getAttribute('data-keyword');
                            showDescription(keyword);
                        });
                    });

                } else if (data[node] && data[node].answer) {
                    // Create a div for the final answer
                    const answerDiv = document.createElement('div');
                    answerDiv.classList.add('answer');

                    const answerText = document.createElement('p');
                    answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
                    answerDiv.appendChild(answerText);

                    // Append the answer to the container
                    questionContainer.appendChild(answerDiv);
                } else {
                    // Handle if no more questions or no data found
                    const noMoreQuestions = document.createElement('p');
                    noMoreQuestions.innerText = "No more questions available.";
                    questionContainer.appendChild(noMoreQuestions);
                }
            }

            function showDescription(keyword) {
            const descriptions = {
                "textual data": "It refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
                "perform tasks related to disease prediction":"It refers to the ability to perform at least one similar task, such as the prediction of disease recurrence and prediction of disease diagnosis-related groups, etc.",
                "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
                "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
                "extract information from clinical text":"It refers to the ability to perform at least one similar task, such as extracting disease phenotypic information, disease feature information and other key information from clinical texts.",
                "answer medical questions":"It refers to open-ended questions related to hospital discharge. Open questions are questions that require a free-form text answer. These questions usually require more in-depth thinking and personalized answers rather than choosing from a limited number of options. These questions may pertain to discharge instructions, recovery recommendations, etc.",
                "generate discharge-related text":"The text may include discharge summaries, clinical records after de-identification, etc."
            };
                const descriptionText = descriptions[keyword];
                if (descriptionText) {
                    document.getElementById('descriptionText').innerText = descriptionText;
                    document.getElementById('descriptionContainer').style.display = 'block';
                }
            }

            document.getElementById('closeButton').addEventListener('click', function () {
                document.getElementById('descriptionContainer').style.display = 'none';
            });

            function updateHistory(node, choice, questionNumber) {
                // Check if we are revisiting a question and modify history accordingly
                const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
                if (historyIndex !== -1) {
                    questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
                }

                // Add current question to history
                questionHistory.push({ node, choice, number: questionNumber });
            }

            // Start the first question
            displayQuestion(currentNode, 0);
                                `;
                                document.body.appendChild(script);
                            }
                        } else {
                            document.getElementById('content').innerHTML = '<p>Error loading content.</p>';
                        }
                    }
                };
                
                xhr.onerror = function() {
                    document.getElementById('content').innerHTML = '<p>Error loading content.</p>';
                };
        
                xhr.send();
            }
        }
        
    </script>
</body>
</html>
