<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Hongyi Li">
	
	<title>Stage IV | Clinical LLM selector</title>

	<link rel="shortcut icon" href="assets/images/gt_favicon.png">
	
	<!-- Bootstrap -->
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	<!-- Icon font -->
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	<!-- Fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	<!-- Custom styles -->
	<link rel="stylesheet" href="assets/css/styles.css">

	<!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
</head>
<body>

    <header id="header">
        <div id="head" class="parallax" parallax-speed="2">
            <h1 id="logo" class="text-center">
                <img class="img-circle" src="assets/images/me.jpg" alt="">
                <span class="title">Hongyi Li</span>
                <span class="tagline">Zhejiang University<br>
                    <a href="">12135029@zju.edu.cn</a></span>
            </h1>
        </div>
    
        <nav class="navbar navbar-default navbar-sticky">
            <div class="container-fluid">
                
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
                </div>
                
                <div class="navbar-collapse collapse">
                    
                    <ul class="nav navbar-nav">
                        <li class="active"><a href="index.html">Home</a></li>
                        <li><a href="about.html">About</a></li>
                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Clinical LLM selector <b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="intro.html">Introduction</a></li>
                                <li><a href="guideline.html">Guideline</a></li>
                                <li><a href="stage_I.html">Stage_I</a></li>
                                <li><a href="stage_II.html">Stage_II</a></li>
                                <li><a href="stage_III.html">Stage_III</a></li>
                                <li><a href="stage_IV.html">Stage_IV</a></li>
                                <li><a href="stage_V.html">Stage_V</a></li>
                            </ul>
                        </li>
                        <!-- <li><a href="blog.html">Blog</a></li> -->
                    </ul>
                
                </div><!--/.nav-collapse -->			
            </div>	
        </nav>
    </header>

<main id="main">
    <div class="container">
		<div class="row topspace">
			<div class="col-sm-8 col-sm-offset-2">
															
				<article class="post">
					<header class="entry-header">
						<div class="entry-meta"> 
							<!-- <span class="posted-on"><time class="entry-date published" date="2024-10-29">October 29, 2024</time></span>			 -->
						</div>
						<h1 class="entry-title"><a href="single.html" rel="bookmark">Clinical LLM srlector for stage IV</a></h1>
					</header>
					<div class="entry-content">
						<p><b>Stage IV: treatment and hospitalization.</b>
                        <br>Next, answer the following questions to find the most suitable LLM for your use case. If an answer has an LLM followed by an `(*)' sign, this indicates that papers reported (not necessarily the same papers) that the LLM was the best performing LLM in the selected condition. e.g. there may be three different papers reporting the same LLM as the best performing model in three different clinical tasks, so when `Yes' is selected for all three clinical tasks, the answer will have the name of that LLM followed by the `(*)'. Those without the `(*)' are simply LLMs that satisfy the selected condition.<br>
                        <span style="color: blue;">Blue text</span> that appears in a question indicates that a specific explanation is available by clicking on that text. The explanation will appear at the bottom of the page and can be closed by clicking red `close' in the explanation.<br>
                        <span style="color: rgb(141, 40, 224);">Any links</span> present in the answer can be accessed by clicking on them.</p>
					</div>
                    <h5>Please answer the following questions on a case-by-case basis.</h5>
                    <!-- <div class="buttonContainer">
                        <button onclick="loadContent('LLM.html')">Return to the initial screen </button>
                    </div> -->
                
                    <div id="questionContainer"></div>
                    <div id="descriptionContainer">
                        <p id="descriptionText"></p>
                        <span id="closeButton">Close</span>
                    </div>
                    <p></p>
				</article>
			</div> 
		</div>


</main>

<footer id="footer">
	<div class="container">
		<div class="row">
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
						<a href="mailto:#">12135029@zju.edu.cn</a><br>
						<br>
						Zhejiang University, HangZhou, China
					</p>	
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
						<a href="https://github.com/williamlhy"><i class="fa fa-github fa-2"></i></a>
					</p>
				</div>
			</div>

			<!-- <div class="col-md-3 widget">
				<h3 class="widget-title">Text widget</h3>
				<div class="widget-body">
					<p>Require to fill</p>
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Form widget</h3>
				<div class="widget-body">
					<p>+234 23 9873237<br>
						<a href="mailto:#">some.email@somewhere.com</a><br>
						<br>
						234 Hidden Pond Road, Ashland City, TN 37015
					</p>	
				</div>
			</div> -->

		</div> <!-- /row of widgets -->
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">
			
			<div class="col-md-6 widget">
				<div class="widget-body">
					<p>Zhejiang University, HangZhou, China </p>
				</div>
			</div>

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2024, Hongyi Li</p>
				</div>
			</div>

		</div> <!-- /row of widgets -->
	</div>
</footer>



<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script>document.addEventListener('DOMContentLoaded', function () {
    const data = {
        "0": {
            "question": "Does your input include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "1",
            "no": "2"
        },
        "1": {
            "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "3",
            "no": "4"
        },
        "2": {
            "question": "Does your output include only <span class='highlight' data-keyword='textual data'>textual data</span>?",
            "yes": "5",
            "no": "6"
        },
        "3": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "7",
            "no": "8"
        },
        "4": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "9",
            "no": "10"
        },
        "5": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "11",
            "no": "12"
        },
        "6": {
            "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
            "yes": "13",
            "no": "14"
        },
        "7": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "15",
            "no": "16"
        },
        "8": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "17",
            "no": "18"
        },
        "9": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "19",
            "no": "20"
        },
        "10": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "21",
            "no": "22"
        },
        "11": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "23",
            "no": "24"
        },
        "12": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "25",
            "no": "26"
        },
        "13": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "27",
            "no": "28"
        },
        "14": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='generate clinical surgery and nursing related text'>generate clinical surgery and nursing related text</span>?",
            "yes": "29",
            "no": "30"
        },
        "15": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "31",
            "no": "32"
        },
        "16": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "33",
            "no": "34"
        },
        "17": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "35",
            "no": "36"
        },
        "18": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "37",
            "no": "38"
        },
        "19": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "39",
            "no": "40"
        },
        "20": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "41",
            "no": "42"
        },
        "21": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "43",
            "no": "44"
        },
        "22": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "45",
            "no": "46"
        },
        "23": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "47",
            "no": "48"
        },
        "24": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "49",
            "no": "50"
        },
        "25": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "51",
            "no": "52"
        },
        "26": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "53",
            "no": "54"
        },
        "27": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "55",
            "no": "56"
        },
        "28": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "57",
            "no": "58"
        },
        "29": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "59",
            "no": "60"
        },
        "30": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='perform tasks related to disease prediction'>perform tasks related to disease prediction</span>?",
            "yes": "61",
            "no": "62"
        },
        "31": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "63",
            "no": "64"
        },
        "32": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "65",
            "no": "66"
        },
        "33": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "67",
            "no": "68"
        },
        "34": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "69",
            "no": "70"
        },
        "35": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "71",
            "no": "72"
        },
        "36": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "73",
            "no": "74"
        },
        "37": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "75",
            "no": "76"
        },
        "38": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "77",
            "no": "78"
        },
        "39": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "79",
            "no": "80"
        },
        "40": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "81",
            "no": "82"
        },
        "41": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "83",
            "no": "84"
        },
        "42": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "85",
            "no": "86"
        },
        "43": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "87",
            "no": "88"
        },
        "44": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "89",
            "no": "90"
        },
        "45": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "91",
            "no": "92"
        },
        "46": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "93",
            "no": "94"
        },
        "47": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "95",
            "no": "96"
        },
        "48": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "97",
            "no": "98"
        },
        "49": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "99",
            "no": "100"
        },
        "50": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "101",
            "no": "102"
        },
        "51": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "103",
            "no": "104"
        },
        "52": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "105",
            "no": "106"
        },
        "53": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "107",
            "no": "108"
        },
        "54": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "109",
            "no": "110"
        },
        "55": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "111",
            "no": "112"
        },
        "56": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "113",
            "no": "114"
        },
        "57": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "115",
            "no": "116"
        },
        "58": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "117",
            "no": "118"
        },
        "59": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "119",
            "no": "120"
        },
        "60": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "121",
            "no": "122"
        },
        "61": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "123",
            "no": "124"
        },
        "62": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='extract information from clinical text'>extract information from clinical text</span>?",
            "yes": "125",
            "no": "126"
        },
        "63": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "127",
            "no": "128"
        },
        "64": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "129",
            "no": "130"
        },
        "65": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "131",
            "no": "132"
        },
        "66": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "133",
            "no": "134"
        },
        "67": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "135",
            "no": "136"
        },
        "68": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "137",
            "no": "138"
        },
        "69": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "139",
            "no": "140"
        },
        "70": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "141",
            "no": "142"
        },
        "71": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "143",
            "no": "144"
        },
        "72": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "145",
            "no": "146"
        },
        "73": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "147",
            "no": "148"
        },
        "74": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "149",
            "no": "150"
        },
        "75": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "151",
            "no": "152"
        },
        "76": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "153",
            "no": "154"
        },
        "77": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "155",
            "no": "156"
        },
        "78": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "157",
            "no": "158"
        },
        "79": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "159",
            "no": "160"
        },
        "80": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "161",
            "no": "162"
        },
        "81": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "163",
            "no": "164"
        },
        "82": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "165",
            "no": "166"
        },
        "83": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "167",
            "no": "168"
        },
        "84": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "169",
            "no": "170"
        },
        "85": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "171",
            "no": "172"
        },
        "86": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "173",
            "no": "174"
        },
        "87": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "175",
            "no": "176"
        },
        "88": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "177",
            "no": "178"
        },
        "89": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "179",
            "no": "180"
        },
        "90": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "181",
            "no": "182"
        },
        "91": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "183",
            "no": "184"
        },
        "92": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "185",
            "no": "186"
        },
        "93": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "187",
            "no": "188"
        },
        "94": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "189",
            "no": "190"
        },
        "95": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "191",
            "no": "192"
        },
        "96": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "193",
            "no": "194"
        },
        "97": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "195",
            "no": "196"
        },
        "98": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "197",
            "no": "198"
        },
        "99": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "199",
            "no": "200"
        },
        "100": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "201",
            "no": "202"
        },
        "101": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "203",
            "no": "204"
        },
        "102": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "205",
            "no": "206"
        },
        "103": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "207",
            "no": "208"
        },
        "104": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "209",
            "no": "210"
        },
        "105": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "211",
            "no": "212"
        },
        "106": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "213",
            "no": "214"
        },
        "107": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "215",
            "no": "216"
        },
        "108": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "217",
            "no": "218"
        },
        "109": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "219",
            "no": "220"
        },
        "110": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "221",
            "no": "222"
        },
        "111": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "223",
            "no": "224"
        },
        "112": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "225",
            "no": "226"
        },
        "113": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "227",
            "no": "228"
        },
        "114": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "229",
            "no": "230"
        },
        "115": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "231",
            "no": "232"
        },
        "116": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "233",
            "no": "234"
        },
        "117": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "235",
            "no": "236"
        },
        "118": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "237",
            "no": "238"
        },
        "119": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "239",
            "no": "240"
        },
        "120": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "241",
            "no": "242"
        },
        "121": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "243",
            "no": "244"
        },
        "122": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "245",
            "no": "246"
        },
        "123": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "247",
            "no": "248"
        },
        "124": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "249",
            "no": "250"
        },
        "125": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "251",
            "no": "252"
        },
        "126": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='answer medical questions'>answer medical questions</span>?",
            "yes": "253",
            "no": "254"
        },
        "127": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "255",
            "no": "256"
        },
        "128": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "257",
            "no": "258"
        },
        "129": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "259",
            "no": "260"
        },
        "130": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "261",
            "no": "262"
        },
        "131": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "263",
            "no": "264"
        },
        "132": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "265",
            "no": "266"
        },
        "133": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "267",
            "no": "268"
        },
        "134": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "269",
            "no": "270"
        },
        "135": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "271",
            "no": "272"
        },
        "136": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "273",
            "no": "274"
        },
        "137": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "275",
            "no": "276"
        },
        "138": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "277",
            "no": "278"
        },
        "139": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "279",
            "no": "280"
        },
        "140": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "281",
            "no": "282"
        },
        "141": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "283",
            "no": "284"
        },
        "142": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "285",
            "no": "286"
        },
        "143": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "287",
            "no": "288"
        },
        "144": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "289",
            "no": "290"
        },
        "145": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "291",
            "no": "292"
        },
        "146": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "293",
            "no": "294"
        },
        "147": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "295",
            "no": "296"
        },
        "148": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "297",
            "no": "298"
        },
        "149": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "299",
            "no": "300"
        },
        "150": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "301",
            "no": "302"
        },
        "151": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "303",
            "no": "304"
        },
        "152": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "305",
            "no": "306"
        },
        "153": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "307",
            "no": "308"
        },
        "154": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "309",
            "no": "310"
        },
        "155": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "311",
            "no": "312"
        },
        "156": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "313",
            "no": "314"
        },
        "157": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "315",
            "no": "316"
        },
        "158": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "317",
            "no": "318"
        },
        "159": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "319",
            "no": "320"
        },
        "160": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "321",
            "no": "322"
        },
        "161": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "323",
            "no": "324"
        },
        "162": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "325",
            "no": "326"
        },
        "163": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "327",
            "no": "328"
        },
        "164": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "329",
            "no": "330"
        },
        "165": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "331",
            "no": "332"
        },
        "166": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "333",
            "no": "334"
        },
        "167": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "335",
            "no": "336"
        },
        "168": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "337",
            "no": "338"
        },
        "169": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "339",
            "no": "340"
        },
        "170": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "341",
            "no": "342"
        },
        "171": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "343",
            "no": "344"
        },
        "172": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "345",
            "no": "346"
        },
        "173": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "347",
            "no": "348"
        },
        "174": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "349",
            "no": "350"
        },
        "175": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "351",
            "no": "352"
        },
        "176": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "353",
            "no": "354"
        },
        "177": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "355",
            "no": "356"
        },
        "178": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "357",
            "no": "358"
        },
        "179": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "359",
            "no": "360"
        },
        "180": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "361",
            "no": "362"
        },
        "181": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "363",
            "no": "364"
        },
        "182": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "365",
            "no": "366"
        },
        "183": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "367",
            "no": "368"
        },
        "184": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "369",
            "no": "370"
        },
        "185": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "371",
            "no": "372"
        },
        "186": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "373",
            "no": "374"
        },
        "187": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "375",
            "no": "376"
        },
        "188": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "377",
            "no": "378"
        },
        "189": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "379",
            "no": "380"
        },
        "190": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "381",
            "no": "382"
        },
        "191": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "383",
            "no": "384"
        },
        "192": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "385",
            "no": "386"
        },
        "193": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "387",
            "no": "388"
        },
        "194": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "389",
            "no": "390"
        },
        "195": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "391",
            "no": "392"
        },
        "196": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "393",
            "no": "394"
        },
        "197": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "395",
            "no": "396"
        },
        "198": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "397",
            "no": "398"
        },
        "199": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "399",
            "no": "400"
        },
        "200": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "401",
            "no": "402"
        },
        "201": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "403",
            "no": "404"
        },
        "202": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "405",
            "no": "406"
        },
        "203": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "407",
            "no": "408"
        },
        "204": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "409",
            "no": "410"
        },
        "205": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "411",
            "no": "412"
        },
        "206": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "413",
            "no": "414"
        },
        "207": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "415",
            "no": "416"
        },
        "208": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "417",
            "no": "418"
        },
        "209": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "419",
            "no": "420"
        },
        "210": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "421",
            "no": "422"
        },
        "211": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "423",
            "no": "424"
        },
        "212": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "425",
            "no": "426"
        },
        "213": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "427",
            "no": "428"
        },
        "214": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "429",
            "no": "430"
        },
        "215": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "431",
            "no": "432"
        },
        "216": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "433",
            "no": "434"
        },
        "217": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "435",
            "no": "436"
        },
        "218": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "437",
            "no": "438"
        },
        "219": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "439",
            "no": "440"
        },
        "220": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "441",
            "no": "442"
        },
        "221": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "443",
            "no": "444"
        },
        "222": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "445",
            "no": "446"
        },
        "223": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "447",
            "no": "448"
        },
        "224": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "449",
            "no": "450"
        },
        "225": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "451",
            "no": "452"
        },
        "226": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "453",
            "no": "454"
        },
        "227": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "455",
            "no": "456"
        },
        "228": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "457",
            "no": "458"
        },
        "229": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "459",
            "no": "460"
        },
        "230": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "461",
            "no": "462"
        },
        "231": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "463",
            "no": "464"
        },
        "232": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "465",
            "no": "466"
        },
        "233": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "467",
            "no": "468"
        },
        "234": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "469",
            "no": "470"
        },
        "235": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "471",
            "no": "472"
        },
        "236": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "473",
            "no": "474"
        },
        "237": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "475",
            "no": "476"
        },
        "238": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "477",
            "no": "478"
        },
        "239": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "479",
            "no": "480"
        },
        "240": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "481",
            "no": "482"
        },
        "241": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "483",
            "no": "484"
        },
        "242": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "485",
            "no": "486"
        },
        "243": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "487",
            "no": "488"
        },
        "244": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "489",
            "no": "490"
        },
        "245": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "491",
            "no": "492"
        },
        "246": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "493",
            "no": "494"
        },
        "247": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "495",
            "no": "496"
        },
        "248": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "497",
            "no": "498"
        },
        "249": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "499",
            "no": "500"
        },
        "250": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "501",
            "no": "502"
        },
        "251": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "503",
            "no": "504"
        },
        "252": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "505",
            "no": "506"
        },
        "253": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "507",
            "no": "508"
        },
        "254": {
            "question": "Do you need LLM to be able to <span class='highlight' data-keyword='summarize clinical records'>summarize clinical records</span>?",
            "yes": "509",
            "no": "510"
        },
        "255": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "511",
            "no": "512"
        },
        "256": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "513",
            "no": "514"
        },
        "257": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "515",
            "no": "516"
        },
        "258": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "517",
            "no": "518"
        },
        "259": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "519",
            "no": "520"
        },
        "260": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "521",
            "no": "522"
        },
        "261": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "523",
            "no": "524"
        },
        "262": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "525",
            "no": "526"
        },
        "263": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "527",
            "no": "528"
        },
        "264": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "529",
            "no": "530"
        },
        "265": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "531",
            "no": "532"
        },
        "266": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "533",
            "no": "534"
        },
        "267": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "535",
            "no": "536"
        },
        "268": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "537",
            "no": "538"
        },
        "269": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "539",
            "no": "540"
        },
        "270": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "541",
            "no": "542"
        },
        "271": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "543",
            "no": "544"
        },
        "272": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "545",
            "no": "546"
        },
        "273": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "547",
            "no": "548"
        },
        "274": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "549",
            "no": "550"
        },
        "275": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "551",
            "no": "552"
        },
        "276": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "553",
            "no": "554"
        },
        "277": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "555",
            "no": "556"
        },
        "278": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "557",
            "no": "558"
        },
        "279": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "559",
            "no": "560"
        },
        "280": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "561",
            "no": "562"
        },
        "281": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "563",
            "no": "564"
        },
        "282": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "565",
            "no": "566"
        },
        "283": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "567",
            "no": "568"
        },
        "284": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "569",
            "no": "570"
        },
        "285": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "571",
            "no": "572"
        },
        "286": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform.86"
        },
        "287": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "288": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "289": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "290": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "291": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "292": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "293": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "294": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "295": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "296": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "297": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "298": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "299": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "300": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "301": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "302": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "303": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "304": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "305": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "306": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "307": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "308": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "309": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "310": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "621",
            "no": "622"
        },
        "311": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "312": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "313": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "314": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "629",
            "no": "630"
        },
        "315": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "316": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "633",
            "no": "634"
        },
        "317": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "635",
            "no": "636"
        },
        "318": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform."
        },
        "319": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "639",
            "no": "640"
        },
        "320": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "641",
            "no": "642"
        },
        "321": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "643",
            "no": "644"
        },
        "322": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "645",
            "no": "646"
        },
        "323": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "647",
            "no": "648"
        },
        "324": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "649",
            "no": "650"
        },
        "325": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "651",
            "no": "652"
        },
        "326": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "653",
            "no": "654"
        },
        "327": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "655",
            "no": "656"
        },
        "328": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "657",
            "no": "658"
        },
        "329": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "659",
            "no": "660"
        },
        "330": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "661",
            "no": "662"
        },
        "331": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "663",
            "no": "664"
        },
        "332": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "665",
            "no": "666"
        },
        "333": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "667",
            "no": "668"
        },
        "334": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "669",
            "no": "670"
        },
        "335": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "671",
            "no": "672"
        },
        "336": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "673",
            "no": "674"
        },
        "337": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "675",
            "no": "676"
        },
        "338": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "677",
            "no": "678"
        },
        "339": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "679",
            "no": "680"
        },
        "340": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "681",
            "no": "682"
        },
        "341": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "683",
            "no": "684"
        },
        "342": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "685",
            "no": "686"
        },
        "343": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "687",
            "no": "688"
        },
        "344": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "689",
            "no": "690"
        },
        "345": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "691",
            "no": "692"
        },
        "346": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "693",
            "no": "694"
        },
        "347": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "695",
            "no": "696"
        },
        "348": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "697",
            "no": "698"
        },
        "349": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "699",
            "no": "700"
        },
        "350": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform.50"
        },
        "351": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "352": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "353": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "354": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "355": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "356": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "357": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "358": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "359": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "360": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "361": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "362": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "363": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "364": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "365": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "366": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "367": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "368": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "369": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "370": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "371": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "372": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "373": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "374": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "749",
            "no": "750"
        },
        "375": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "376": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "377": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "378": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "757",
            "no": "758"
        },
        "379": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "380": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "761",
            "no": "762"
        },
        "381": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "763",
            "no": "764"
        },
        "382": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform."
        },
        "383": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "767",
            "no": "768"
        },
        "384": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "769",
            "no": "770"
        },
        "385": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "771",
            "no": "772"
        },
        "386": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "773",
            "no": "774"
        },
        "387": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "775",
            "no": "776"
        },
        "388": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "777",
            "no": "778"
        },
        "389": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "779",
            "no": "780"
        },
        "390": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "781",
            "no": "782"
        },
        "391": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "783",
            "no": "784"
        },
        "392": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "785",
            "no": "786"
        },
        "393": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "787",
            "no": "788"
        },
        "394": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "789",
            "no": "790"
        },
        "395": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "791",
            "no": "792"
        },
        "396": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "793",
            "no": "794"
        },
        "397": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "795",
            "no": "796"
        },
        "398": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "797",
            "no": "798"
        },
        "399": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "799",
            "no": "800"
        },
        "400": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "801",
            "no": "802"
        },
        "401": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "803",
            "no": "804"
        },
        "402": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "805",
            "no": "806"
        },
        "403": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "807",
            "no": "808"
        },
        "404": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "809",
            "no": "810"
        },
        "405": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "811",
            "no": "812"
        },
        "406": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "813",
            "no": "814"
        },
        "407": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "815",
            "no": "816"
        },
        "408": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "817",
            "no": "818"
        },
        "409": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "819",
            "no": "820"
        },
        "410": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "821",
            "no": "822"
        },
        "411": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "823",
            "no": "824"
        },
        "412": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "825",
            "no": "826"
        },
        "413": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "827",
            "no": "828"
        },
        "414": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform.14"
        },
        "415": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "416": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "417": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "418": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "419": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "420": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "421": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "422": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "423": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "424": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "425": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "426": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "427": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "428": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "429": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "430": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "431": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "432": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "433": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "434": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "435": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "436": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "437": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "438": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "877",
            "no": "878"
        },
        "439": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "440": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "441": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "442": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "885",
            "no": "886"
        },
        "443": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "444": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "889",
            "no": "890"
        },
        "445": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "891",
            "no": "892"
        },
        "446": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform."
        },
        "447": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "895",
            "no": "896"
        },
        "448": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "897",
            "no": "898"
        },
        "449": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "899",
            "no": "900"
        },
        "450": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "901",
            "no": "902"
        },
        "451": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "903",
            "no": "904"
        },
        "452": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "905",
            "no": "906"
        },
        "453": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "907",
            "no": "908"
        },
        "454": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "909",
            "no": "910"
        },
        "455": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "911",
            "no": "912"
        },
        "456": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "913",
            "no": "914"
        },
        "457": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "915",
            "no": "916"
        },
        "458": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "917",
            "no": "918"
        },
        "459": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "919",
            "no": "920"
        },
        "460": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "921",
            "no": "922"
        },
        "461": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "923",
            "no": "924"
        },
        "462": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "925",
            "no": "926"
        },
        "463": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "927",
            "no": "928"
        },
        "464": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "929",
            "no": "930"
        },
        "465": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "931",
            "no": "932"
        },
        "466": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "933",
            "no": "934"
        },
        "467": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "935",
            "no": "936"
        },
        "468": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "937",
            "no": "938"
        },
        "469": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "939",
            "no": "940"
        },
        "470": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "941",
            "no": "942"
        },
        "471": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "943",
            "no": "944"
        },
        "472": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "945",
            "no": "946"
        },
        "473": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "947",
            "no": "948"
        },
        "474": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "949",
            "no": "950"
        },
        "475": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "951",
            "no": "952"
        },
        "476": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "953",
            "no": "954"
        },
        "477": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "955",
            "no": "956"
        },
        "478": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform.78"
        },
        "479": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "480": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "481": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "482": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "483": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "484": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "485": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "486": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "487": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "488": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "489": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "490": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "491": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "492": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "493": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "494": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "495": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "496": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "497": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "498": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "499": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "500": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "501": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "502": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "1005",
            "no": "1006"
        },
        "503": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "504": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "505": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "506": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "1013",
            "no": "1014"
        },
        "507": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "508": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "1017",
            "no": "1018"
        },
        "509": {
            "question": "Do you prefer to deploy LLM <span class='highlight' data-keyword='locally'>locally</span> rather than <span class='highlight' data-keyword='in the cloud'>in the cloud</span>?",
            "yes": "1019",
            "no": "1020"
        },
        "510": {
            "answer": "Sorry, there is not enough information to recommend LLMs, please select at least one clinical task that requires LLMs to be able to perform."
        },
        "511": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "512": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "513": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "514": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "515": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "516": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "517": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "518": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "519": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "520": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "521": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "522": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "523": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "524": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "525": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "526": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "527": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "528": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "529": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "530": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "531": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "532": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "533": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "534": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "535": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "536": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "537": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "538": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "539": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "540": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "541": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "542": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "543": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "544": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "545": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "546": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "547": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "548": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "549": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "550": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "551": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "552": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "553": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "554": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "555": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "556": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "557": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "558": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "559": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "560": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "561": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "562": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "563": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "564": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "565": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "566": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "567": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "568": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "569": {
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "570": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "571": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "572": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "621": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "622": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "629": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "630": {
            "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "633":{
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "634":{
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "635": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "636": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "639": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "640": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "641": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "642": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "643": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "644": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "645": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "646": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "647": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "648": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "649": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "650": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "651": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "652": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "653": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "654": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "655": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "656": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "657": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "658": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "659": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "660": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "661": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "662": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "663": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "664": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "665": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "666": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "667": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "668": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "669": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "670": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "671": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "672": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "673": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "674": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "675": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "676": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "677": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "678": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "679": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "680": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "681": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "682": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "683": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "684": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "685": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "686": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "687": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "688": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.88"
        },
        "689": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "690": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "691": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "692": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "693": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "694": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "695": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "696": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "697": {
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "698": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "699": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "700": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "749": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "750": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "757": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "758": {
            "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "761":{
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "762":{
            "answer": "The LLMs we recommend you use are: WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "763": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "764": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "767": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "768": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "769": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "770": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "771": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "772": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "773": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "774": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "775": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "776": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "777": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "778": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "779": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "780": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "781": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "782": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "783": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "784": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "785": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "786": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "787": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "788": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "789": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "790": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "791": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "792": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "793": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "794": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "795": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "796": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "797": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "798": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "799": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "800": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "801": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "802": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "803": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "804": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "805": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "806": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "807": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "808": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "809": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "810": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "811": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "812": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "813": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "814": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "815": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "816": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "817": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "818": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "819": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "820": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "821": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "822": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "823": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "824": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "825": {
            "answer": "The LLM we recommend you use is: OphGLM-6.2B (*), WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "826": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "827": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "828": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "877": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "878": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "885": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "886": {
            "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "889": {
            "answer": "The LLM we recommend you use is: OphGLM-6.2B (*), WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "890": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "891": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "892": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "895": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "896": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "897": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "898": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "899": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "900": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "901": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "902": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "903": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "904": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "905": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "906": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "907": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "908": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "909": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "910": {
            "answer": "The LLM we recommend you use is: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "911": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "912": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "913": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "914": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "915": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "916": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "917": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "918": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "919": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "920": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "921": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "922": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "923": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "924": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "925": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "926": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "927": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "928": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "929": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "930": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "931": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "932": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "933": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "934": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "935": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "936": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "937": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "938": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "939": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "940": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens."
        },
        "941": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "942": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "943": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "944": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "945": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "946": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "947": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "948": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "949": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "950": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), GPT-3.5, BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "951": {
            "answer": "Sorry, no matching LLMs were found."
        },
        "952": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens."
        },
        "953": {
            "answer": "The LLM we recommend you use is: OphGLM-6.2B (*), WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "954": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "955": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "956": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GatorTronGPT-20B (*), GPT-3.5, LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        },
        "1005": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 6000W and the total price is $167976. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $1000.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600."
        },
        "1006": {
            "answer": "The LLMs we recommend you use are: NYUTron-109M (*), BioBERT (*), BioLinkBERT (*), ClinicalBERT-110M (*). Here are the required resources:<br> <br>NYUTron-109M: pre-train requires 24 NVIDIA TESLA A100-40GB GPUs. The price of renting 24 this type of GPU using cloud services is $97.2/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $32.4/1h.<br><br>BioBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>BioLinkBERT: no specific resource requirements for this model, refer to resources for BERT. Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h."
        },
        "1013": {
            "answer": "The LLM we recommend you use is: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*), BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br><br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPU is 48000W and the total price is $411429. Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $51429. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPUs is 2400W and the total price is $360000. Inference requires 2 NVIDIA A40-48GB GPUs. The total thermal design power of the GPUs is 600W and the total price is $15598.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 8000W and the total price is $223968. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPU is 2000W and the total price is $55992.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: fine-tuning requires 8 H100 GPUs. The total thermal design power of the GPU is 2400W and the total price is $360000. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "1014": {
            "answer": "The LLMs we recommend you use are: BioClinicalBERT (*), Me LLaMA-Chat-70B (*), Me LLaMA-70B (*),  BERT, MedAlpaca-13B, LLaMA2-Chat-13B, LLaMA2-Chat-70B, PMC-LLaMA-Chat-13B, AlpaCare-13B, Me LLaMA-Chat-13B, Me LLaMA-13B. Here are the required resources:<br> <br>BioClinicalBERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Me LLaMA-Chat-70B and Me LLaMA-70B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h. Inference requires 1 NVIDIA RTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>MedAlpaca-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>LLaMA2-Chat-70B: no specific resource requirements for this model, refer to resources for LLMs of similar size (70B). Pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. Inference requires 2 NVIDIA A40-48GB GPUs. The price of renting GPUs with 96GB of memory using a cloud service is $7.5/1h.<br><br>PMC-LLaMA-Chat-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>AlpaCare-13B: no specific resource requirements for this model, refer to resources for LLMs of similar size (13B). Pre-train requires 32 NVIDIA TESLA A100-40GB GPUs. The price of renting 32 this type of GPU using cloud services is $129.6/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h.<br><br>Me LLaMA-Chat-13B and Me LLaMA-13B: pre-train requires 160 NVIDIA TESLA A100-80GB GPUs. The price of renting 160 this type of GPU using cloud services is $1000/1h. Fine-tuning requires 8 H100 GPUs. The price of renting 8 this type of GPU using cloud services is $88/1h. They can be accessed via link <a href='https://github.com/BIDS-Xu-Lab/Me-LLaMA' target='_blank' class='custom-link'>MeLLaMA GitHub</a>."
        },
        "1017": {
            "answer": "The LLM we recommend you use is: OphGLM-6.2B (*), WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPUs is 250W and the total price is $6999. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU."
        },
        "1018": {
            "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5 (*), OphGLM-6.2B (*), Google Bard, Bing Chat, WenZhong-3.5B, Chinese-Bloom-7B. Here are the required resources:<br> <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>OphGLM-6.2B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. Fine-tuning requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 32 this type of GPU using cloud services is $4.05/1h. It can be accessed via link <a href='https://github.com/ML-AILab/OphGLM' target='_blank' class='custom-link'>OphGLM GitHub</a>.<br><br>Google Bard: has been renamed to Gemini, the original version is not accessible, you can access the latest Gemini 1.5 Pro. The input price is $0.00125/1k tokens and the output price is $0.005/1k tokens.<br><br>Bing Chat: has been renamed to Copilot, the original version is not accessible, you can access the Copilot Advanced, averages $119 per month if paid annually.<br>WenZhong-3.5B: no specific resource requirements for this model, refer to resources for Flan-T5-2.7B. Fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. <br>Chinese-Bloom-7B: no specific resource requirements for this model, refer to resources for LLaMA-7B. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h."
        },
        "1019": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 80GB GPU.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPU is 295W and the total price is $4300.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The total thermal design power of the GPUs is 2400W and the total price is $144000.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 250W and the total price is $6999.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295W and the total price is $4300.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The total thermal design power of the GPUs is 295w and the total price is $4300."
        },
        "1020": {
            "answer": "The LLMs we recommend you use are: GatorTronGPT-20B (*), LLaMA-7B, Vicuna-7B, MedAlpaca-7B, Flan-T5-2.7B, GatorTronGPT-5B, T5-770M, Alpaca-7B, FLAN-UL2-20B. Here are the required resources:<br> <br>GatorTronGPT-20B: pre-train requires 560 NVIDIA TESLA A100-80GB GPUs. The price of renting 560 this type of GPU using cloud services is $3500/1h. Fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h. It can be accessed via link <a href='https://github.com/NVIDIA/Megatron-LM' target='_blank' class='custom-link'>GatorTron1 GitHub</a> and <a href='https://github.com/NVIDIA/NeMo' target='_blank' class='custom-link'>GatorTron2 GitHub</a>.<br><br>LLaMA-7B: fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 80GB GPU. The price of renting a GPU with 80GB of memory using a cloud service is $6.25/1h.<br><br>Vicuna-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>MedAlpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>GatorTronGPT-5B: fine-tuning requires 8 NVIDIA TESLA A100-80GB GPUs. The price of renting 8 this type of GPU using cloud services is $50/1h.<br><br>T5-770M: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPUs. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Alpaca-7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>FLAN-UL2-20B: fine-tuning requires 1 NVIDIA Quadro RTX 8000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h."
        }
    };
    
    const descriptions = {
        "textual data": "It refers to data presented in textual form, including medical records, radiology reports, patient histories, and other medically relevant textual information. Tabular data can also be used as textual data after being converted into textual form, such as a patient's laboratory indicators. However, data such as medical images, electrocardiograms, etc. are not textual data.",
        "generate clinical surgery and nursing related text": "It refers to the ability to perform at least one similar task, such as the generation and simplification of surgical informed consent forms, the generation of clinical letters, etc.",
        "perform tasks related to disease prediction":"It refers to the ability to perform at least one similar task, such as the prediction of disease risk and survival, etc.",
        "locally":"It refers to the way the LLMs is deployed and run in the local environment. It allows for better control and management of model operations, protects the privacy and security of patient data, and reduces reliance on external networks. Local environment refers to a specific computing environment that is operated and developed on a personal computer or specific device. It can be a server, desktop computer, laptop or mobile device within a healthcare organization, etc.",
        "in the cloud":"It refers to the deployment of LLMs in a cloud computing environment. Cloud computing is based on network providing computing resources and services through remote servers.  By deploying LLMs in the cloud, medical professionals can take full advantage of the high-performance computing resources provided by cloud computing. Training, reasoning, and management of models in the cloud can greatly reduce the burden on local devices and can flexibly adjust the scale of computing resources according to demand. Cloud computing also provides advanced data security and privacy protections that can ensure the safety and reliability of medical data. Accessing commercial LLMs like ChatGPT through an api or website also falls under the category of cloud deployment, but data privacy and security are not guaranteed.",
        "extract information from clinical text":"It refers to the ability to perform at least one similar task, such as extracting ICD codes from clinical records, clinical named entity recognition, etc.",
        "answer medical questions":"medical questions includes multiple-choice and open-ended questions (and may also include visual questions, i.e., where there are medical images in the input) on tasks related to treatment and hospitalization. Open questions are questions that require a free-form text answer. These questions usually require more in-depth thinking and personalized answers rather than choosing from a limited number of options. These questions may relate to treatment options, surgical risks, medication choices, rehabilitation programs, and more.",
        "summarize clinical records":"includes summarizing the conversation between the doctor and the patient, summarizing the medical questions asked by the patient, etc."
    };

    let currentNode = "0"; // Starting node
    let questionHistory = []; // Keeps track of answered questions

    // Function to display a question based on the node
    function displayQuestion(node, questionIndex) {
        const questionContainer = document.getElementById('questionContainer');
        // Clear questions beyond this index
        questionContainer.innerHTML = '';

        // Display history
        questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

        // Display the current question
        appendQuestion(node, null, questionIndex + 1);
    }

    function appendQuestion(node, selectedChoice, questionNumber) {
        const questionContainer = document.getElementById('questionContainer');

        if (data[node] && data[node].question) {
            const questionDiv = document.createElement('div');
            questionDiv.classList.add('question');

            // Display fixed question number
            const questionNumberElem = document.createElement('p');
            questionNumberElem.classList.add('questionNumber');
            questionNumberElem.innerText = `Question ${questionNumber}:`;
            questionDiv.appendChild(questionNumberElem);

            // Display question text (with clickable keywords)
            const questionText = document.createElement('p');
            questionText.innerHTML = data[node].question; // Allows inner HTML to handle <span> tags
            questionDiv.appendChild(questionText);

            // Create Yes button
            const yesButton = document.createElement('button');
            yesButton.innerText = 'Yes';
            yesButton.classList.toggle('selected', selectedChoice === 'yes');
            yesButton.onclick = function () {
                updateHistory(node, 'yes', questionNumber);
                currentNode = data[node].yes;
                displayQuestion(currentNode, questionNumber);
            };
            questionDiv.appendChild(yesButton);

            // Create No button
            const noButton = document.createElement('button');
            noButton.innerText = 'No';
            noButton.classList.toggle('selected', selectedChoice === 'no');
            noButton.onclick = function () {
                updateHistory(node, 'no', questionNumber);
                currentNode = data[node].no;
                displayQuestion(currentNode, questionNumber);
            };
            questionDiv.appendChild(noButton);

            // Append the question block to the container
            questionContainer.appendChild(questionDiv);

            // Attach event listeners to keywords
            const highlightElements = questionDiv.querySelectorAll('.highlight');
            highlightElements.forEach(function (element) {
                element.addEventListener('click', function () {
                    const keyword = element.getAttribute('data-keyword');
                    showDescription(keyword);
                });
            });

        } else if (data[node] && data[node].answer) {
            // Create a div for the final answer
            const answerDiv = document.createElement('div');
            answerDiv.classList.add('answer');

            const answerText = document.createElement('p');
            answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
            answerDiv.appendChild(answerText);

            // Append the answer to the container
            questionContainer.appendChild(answerDiv);
        } else {
            // Handle if no more questions or no data found
            const noMoreQuestions = document.createElement('p');
            noMoreQuestions.innerText = "No more questions available.";
            questionContainer.appendChild(noMoreQuestions);
        }
    }

    function showDescription(keyword) {
        const descriptionText = descriptions[keyword];
        if (descriptionText) {
            document.getElementById('descriptionText').innerText = descriptionText;
            document.getElementById('descriptionContainer').style.display = 'block';
        }
    }

    document.getElementById('closeButton').addEventListener('click', function () {
        document.getElementById('descriptionContainer').style.display = 'none';
    });

    function updateHistory(node, choice, questionNumber) {
        // Check if we are revisiting a question and modify history accordingly
        const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
        if (historyIndex !== -1) {
            questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
        }

        // Add current question to history
        questionHistory.push({ node, choice, number: questionNumber });
    }

    // Start the first question
    displayQuestion(currentNode, 0);
});</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
</body>
</html>
