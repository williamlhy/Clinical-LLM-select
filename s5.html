<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Questionnaire</title>
    <style>
        .questionNumber {
            font-weight: bold;
        }
        .selected {
            color: red;
        }
    </style>
</head>
<body>
    <!-- Instructions -->
    <div class="instruction">
        Next, answer the following questions to find the most suitable LLM for your use case:
    </div>
    <div class="buttonContainer">
        <button onclick="location.href='./'">Return to the initial screen </button>
    </div>

    <div id="questionContainer"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const data = {
                "0": {
                    "question": "Does your input include only textual data?",
                    "yes": "1",
                    "no": "2"
                },
                "1": {
                    "question": "Does your output include only textual data?",
                    "yes": "3",
                    "no": "4"
                },
                "2": {
                    "question": "Does your output include only textual data?",
                    "yes": "5",
                    "no": "6"
                },
                "3": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "7",
                    "no": "8"
                },
                "4": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "9",
                    "no": "10"
                },
                "5": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "11",
                    "no": "12"
                },
                "6": {
                    "question": "Do you agree that your data may be shared with third parties, published, or made generally available?",
                    "yes": "13",
                    "no": "14"
                },
                "7": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 7",
                    "yes": "15",
                    "no": "16"
                },
                "8": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 8",
                    "yes": "17",
                    "no": "18"
                },
                "9": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 9",
                    "yes": "19",
                    "no": "20"
                },
                "10": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 10",
                    "yes": "21",
                    "no": "22"
                },
                "11": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 11",
                    "yes": "23",
                    "no": "24"
                },
                "12": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 12",
                    "yes": "25",
                    "no": "26"
                },
                "13": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 13",
                    "yes": "27",
                    "no": "28"
                },
                "14": {
                    "question": "Do you need LLM to be able to accomplish tasks related to predicting disease? 14",
                    "yes": "29",
                    "no": "30"
                },
                "15": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 15",
                    "yes": "31",
                    "no": "32"
                },
                "16": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 16",
                    "yes": "33",
                    "no": "34"
                },
                "17": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 17",
                    "yes": "35",
                    "no": "36"
                },
                "18": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 18",
                    "yes": "37",
                    "no": "38"
                },
                "19": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 19",
                    "yes": "39",
                    "no": "40"
                },
                "20": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 20",
                    "yes": "41",
                    "no": "42"
                },
                "21": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 21",
                    "yes": "43",
                    "no": "44"
                },
                "22": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 22",
                    "yes": "45",
                    "no": "46"
                },
                "23": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 23",
                    "yes": "47",
                    "no": "48"
                },
                "24": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 24",
                    "yes": "49",
                    "no": "50"
                },
                "25": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 25",
                    "yes": "51",
                    "no": "52"
                },
                "26": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 26",
                    "yes": "53",
                    "no": "54"
                },
                "27": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 27",
                    "yes": "55",
                    "no": "56"
                },
                "28": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 28",
                    "yes": "57",
                    "no": "58"
                },
                "29": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 29",
                    "yes": "59",
                    "no": "60"
                },
                "30": {
                    "question": "Do you need LLM to be able to extract information from clinical text? 30",
                    "yes": "61",
                    "no": "62"
                },
                "31": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 31",
                    "yes": "63",
                    "no": "64"
                },
                "32": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 32",
                    "yes": "65",
                    "no": "66"
                },
                "33": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 33",
                    "yes": "67",
                    "no": "68"
                },
                "34": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 34",
                    "yes": "69",
                    "no": "70"
                },
                "35": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 35",
                    "yes": "71",
                    "no": "72"
                },
                "36": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 36",
                    "yes": "73",
                    "no": "74"
                },
                "37": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 37",
                    "yes": "75",
                    "no": "76"
                },
                "38": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 38",
                    "yes": "77",
                    "no": "78"
                },
                "39": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 39",
                    "yes": "79",
                    "no": "80"
                },
                "40": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 40",
                    "yes": "81",
                    "no": "82"
                },
                "41": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 41",
                    "yes": "83",
                    "no": "84"
                },
                "42": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 42",
                    "yes": "85",
                    "no": "86"
                },
                "43": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 43",
                    "yes": "87",
                    "no": "88"
                },
                "44": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 44",
                    "yes": "89",
                    "no": "90"
                },
                "45": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 45",
                    "yes": "91",
                    "no": "92"
                },
                "46": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 46",
                    "yes": "93",
                    "no": "94"
                },
                "47": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 47",
                    "yes": "95",
                    "no": "96"
                },
                "48": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 48",
                    "yes": "97",
                    "no": "98"
                },
                "49": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 49",
                    "yes": "99",
                    "no": "100"
                },
                "50": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 50",
                    "yes": "101",
                    "no": "102"
                },
                "51": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 51",
                    "yes": "103",
                    "no": "104"
                },
                "52": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 52",
                    "yes": "105",
                    "no": "106"
                },
                "53": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 53",
                    "yes": "107",
                    "no": "108"
                },
                "54": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 54",
                    "yes": "109",
                    "no": "110"
                },
                "55": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 55",
                    "yes": "111",
                    "no": "112"
                },
                "56": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 56",
                    "yes": "113",
                    "no": "114"
                },
                "57": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 57",
                    "yes": "115",
                    "no": "116"
                },
                "58": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 58",
                    "yes": "117",
                    "no": "118"
                },
                "59": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 59",
                    "yes": "119",
                    "no": "120"
                },
                "60": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 60",
                    "yes": "121",
                    "no": "122"
                },
                "61": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 61",
                    "yes": "123",
                    "no": "124"
                },
                "62": {
                    "question": "Do you need LLM to be able to generate clinically relevant text? 62",
                    "yes": "125",
                    "no": "126"
                },
                "63": {
                    "question": "Do you need LLM to be able to answer medical questions? 63",
                    "yes": "127",
                    "no": "128"
                },
                "64": {
                    "question": "Do you need LLM to be able to answer medical questions? 64",
                    "yes": "129",
                    "no": "130"
                },
                "65": {
                    "question": "Do you need LLM to be able to answer medical questions? 65",
                    "yes": "131",
                    "no": "132"
                },
                "66": {
                    "question": "Do you need LLM to be able to answer medical questions? 66",
                    "yes": "133",
                    "no": "134"
                },
                "67": {
                    "question": "Do you need LLM to be able to answer medical questions? 67",
                    "yes": "135",
                    "no": "136"
                },
                "68": {
                    "question": "Do you need LLM to be able to answer medical questions? 68",
                    "yes": "137",
                    "no": "138"
                },
                "69": {
                    "question": "Do you need LLM to be able to answer medical questions? 69",
                    "yes": "139",
                    "no": "140"
                },
                "70": {
                    "question": "Do you need LLM to be able to answer medical questions? 70",
                    "yes": "141",
                    "no": "142"
                },
                "71": {
                    "question": "Do you need LLM to be able to answer medical questions? 71",
                    "yes": "143",
                    "no": "144"
                },
                "72": {
                    "question": "Do you need LLM to be able to answer medical questions? 72",
                    "yes": "145",
                    "no": "146"
                },
                "73": {
                    "question": "Do you need LLM to be able to answer medical questions? 73",
                    "yes": "147",
                    "no": "148"
                },
                "74": {
                    "question": "Do you need LLM to be able to answer medical questions? 74",
                    "yes": "149",
                    "no": "150"
                },
                "75": {
                    "question": "Do you need LLM to be able to answer medical questions? 75",
                    "yes": "151",
                    "no": "152"
                },
                "76": {
                    "question": "Do you need LLM to be able to answer medical questions? 76",
                    "yes": "153",
                    "no": "154"
                },
                "77": {
                    "question": "Do you need LLM to be able to answer medical questions? 77",
                    "yes": "155",
                    "no": "156"
                },
                "78": {
                    "question": "Do you need LLM to be able to answer medical questions? 78",
                    "yes": "157",
                    "no": "158"
                },
                "79": {
                    "question": "Do you need LLM to be able to answer medical questions? 79",
                    "yes": "159",
                    "no": "160"
                },
                "80": {
                    "question": "Do you need LLM to be able to answer medical questions? 80",
                    "yes": "161",
                    "no": "162"
                },
                "81": {
                    "question": "Do you need LLM to be able to answer medical questions? 81",
                    "yes": "163",
                    "no": "164"
                },
                "82": {
                    "question": "Do you need LLM to be able to answer medical questions? 82",
                    "yes": "165",
                    "no": "166"
                },
                "83": {
                    "question": "Do you need LLM to be able to answer medical questions? 83",
                    "yes": "167",
                    "no": "168"
                },
                "84": {
                    "question": "Do you need LLM to be able to answer medical questions? 84",
                    "yes": "169",
                    "no": "170"
                },
                "85": {
                    "question": "Do you need LLM to be able to answer medical questions? 85",
                    "yes": "171",
                    "no": "172"
                },
                "86": {
                    "question": "Do you need LLM to be able to answer medical questions? 86",
                    "yes": "173",
                    "no": "174"
                },
                "87": {
                    "question": "Do you need LLM to be able to answer medical questions? 87",
                    "yes": "175",
                    "no": "176"
                },
                "88": {
                    "question": "Do you need LLM to be able to answer medical questions? 88",
                    "yes": "177",
                    "no": "178"
                },
                "89": {
                    "question": "Do you need LLM to be able to answer medical questions? 89",
                    "yes": "179",
                    "no": "180"
                },
                "90": {
                    "question": "Do you need LLM to be able to answer medical questions? 90",
                    "yes": "181",
                    "no": "182"
                },
                "91": {
                    "question": "Do you need LLM to be able to answer medical questions? 91",
                    "yes": "183",
                    "no": "184"
                },
                "92": {
                    "question": "Do you need LLM to be able to answer medical questions? 92",
                    "yes": "185",
                    "no": "186"
                },
                "93": {
                    "question": "Do you need LLM to be able to answer medical questions? 93",
                    "yes": "187",
                    "no": "188"
                },
                "94": {
                    "question": "Do you need LLM to be able to answer medical questions? 94",
                    "yes": "189",
                    "no": "190"
                },
                "95": {
                    "question": "Do you need LLM to be able to answer medical questions? 95",
                    "yes": "191",
                    "no": "192"
                },
                "96": {
                    "question": "Do you need LLM to be able to answer medical questions? 96",
                    "yes": "193",
                    "no": "194"
                },
                "97": {
                    "question": "Do you need LLM to be able to answer medical questions? 97",
                    "yes": "195",
                    "no": "196"
                },
                "98": {
                    "question": "Do you need LLM to be able to answer medical questions? 98",
                    "yes": "197",
                    "no": "198"
                },
                "99": {
                    "question": "Do you need LLM to be able to answer medical questions? 99",
                    "yes": "199",
                    "no": "200"
                },
                "100": {
                    "question": "Do you need LLM to be able to answer medical questions? 100",
                    "yes": "201",
                    "no": "202"
                },
                "101": {
                    "question": "Do you need LLM to be able to answer medical questions? 101",
                    "yes": "203",
                    "no": "204"
                },
                "102": {
                    "question": "Do you need LLM to be able to answer medical questions? 102",
                    "yes": "205",
                    "no": "206"
                },
                "103": {
                    "question": "Do you need LLM to be able to answer medical questions? 103",
                    "yes": "207",
                    "no": "208"
                },
                "104": {
                    "question": "Do you need LLM to be able to answer medical questions? 104",
                    "yes": "209",
                    "no": "210"
                },
                "105": {
                    "question": "Do you need LLM to be able to answer medical questions? 105",
                    "yes": "211",
                    "no": "212"
                },
                "106": {
                    "question": "Do you need LLM to be able to answer medical questions? 106",
                    "yes": "213",
                    "no": "214"
                },
                "107": {
                    "question": "Do you need LLM to be able to answer medical questions? 107",
                    "yes": "215",
                    "no": "216"
                },
                "108": {
                    "question": "Do you need LLM to be able to answer medical questions? 108",
                    "yes": "217",
                    "no": "218"
                },
                "109": {
                    "question": "Do you need LLM to be able to answer medical questions? 109",
                    "yes": "219",
                    "no": "220"
                },
                "110": {
                    "question": "Do you need LLM to be able to answer medical questions? 110",
                    "yes": "221",
                    "no": "222"
                },
                "111": {
                    "question": "Do you need LLM to be able to answer medical questions? 111",
                    "yes": "223",
                    "no": "224"
                },
                "112": {
                    "question": "Do you need LLM to be able to answer medical questions? 112",
                    "yes": "225",
                    "no": "226"
                },
                "113": {
                    "question": "Do you need LLM to be able to answer medical questions? 113",
                    "yes": "227",
                    "no": "228"
                },
                "114": {
                    "question": "Do you need LLM to be able to answer medical questions? 114",
                    "yes": "229",
                    "no": "230"
                },
                "115": {
                    "question": "Do you need LLM to be able to answer medical questions? 115",
                    "yes": "231",
                    "no": "232"
                },
                "116": {
                    "question": "Do you need LLM to be able to answer medical questions? 116",
                    "yes": "233",
                    "no": "234"
                },
                "117": {
                    "question": "Do you need LLM to be able to answer medical questions? 117",
                    "yes": "235",
                    "no": "236"
                },
                "118": {
                    "question": "Do you need LLM to be able to answer medical questions? 118",
                    "yes": "237",
                    "no": "238"
                },
                "119": {
                    "question": "Do you need LLM to be able to answer medical questions? 119",
                    "yes": "239",
                    "no": "240"
                },
                "120": {
                    "question": "Do you need LLM to be able to answer medical questions? 120",
                    "yes": "241",
                    "no": "242"
                },
                "121": {
                    "question": "Do you need LLM to be able to answer medical questions? 121",
                    "yes": "243",
                    "no": "244"
                },
                "122": {
                    "question": "Do you need LLM to be able to answer medical questions? 122",
                    "yes": "245",
                    "no": "246"
                },
                "123": {
                    "question": "Do you need LLM to be able to answer medical questions? 123",
                    "yes": "247",
                    "no": "248"
                },
                "124": {
                    "question": "Do you need LLM to be able to answer medical questions? 124",
                    "yes": "249",
                    "no": "250"
                },
                "125": {
                    "question": "Do you need LLM to be able to answer medical questions? 125",
                    "yes": "251",
                    "no": "252"
                },
                "126": {
                    "question": "Do you need LLM to be able to answer medical questions? 126",
                    "yes": "253",
                    "no": "254"
                },
                "127": {
                    "answer": "Sorry, no matching LLMs were found. 127"
                },
                "128": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 128",
                    "yes": "257",
                    "no": "258"
                },
                "129": {
                    "answer": "Sorry, no matching LLMs were found. 129"
                },
                "130": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 130",
                    "yes": "261",
                    "no": "262"
                },
                "131": {
                    "answer": "Sorry, no matching LLMs were found. 131"
                },
                "132": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 132",
                    "yes": "265",
                    "no": "266"
                },
                "133": {
                    "answer": "Sorry, no matching LLMs were found. 133"
                },
                "134": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 134",
                    "yes": "269",
                    "no": "270"
                },
                "135": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 135",
                    "yes": "271",
                    "no": "272"
                },
                "136": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 136",
                    "yes": "273",
                    "no": "274"
                },
                "137": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 137",
                    "yes": "275",
                    "no": "276"
                },
                "138": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 138",
                    "yes": "277",
                    "no": "278"
                },
                "139": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 139",
                    "yes": "279",
                    "no": "280"
                },
                "140": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 140",
                    "yes": "281",
                    "no": "282"
                },
                "141": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 141",
                    "yes": "283",
                    "no": "284"
                },
                "142": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 142"
                },
                "143": {
                    "answer": "Sorry, no matching LLMs were found. 143"
                },
                "144": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 144",
                    "yes": "289",
                    "no": "290"
                },
                "145": {
                    "answer": "Sorry, no matching LLMs were found. 145"
                },
                "146": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 146",
                    "yes": "293",
                    "no": "294"
                },
                "147": {
                    "answer": "Sorry, no matching LLMs were found. 147"
                },
                "148": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 148",
                    "yes": "297",
                    "no": "298"
                },
                "149": {
                    "answer": "Sorry, no matching LLMs were found. 149"
                },
                "150": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 150",
                    "yes": "301",
                    "no": "302"
                },
                "151": {
                    "answer": "Sorry, no matching LLMs were found. 151"
                },
                "152": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 152",
                    "yes": "305",
                    "no": "306"
                },
                "153": {
                    "answer": "Sorry, no matching LLMs were found. 153"
                },
                "154": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 154",
                    "yes": "309",
                    "no": "310"
                },
                "155": {
                    "answer": "Sorry, no matching LLMs were found. 155"
                },
                "156": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 156",
                    "yes": "313",
                    "no": "314"
                },
                "157": {
                    "answer": "Sorry, no matching LLMs were found. 157"
                },
                "158": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 158"
                },
                "159": {
                    "answer": "Sorry, no matching LLMs were found. 159"
                },
                "160": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 160",
                    "yes": "321",
                    "no": "322"
                },
                "161": {
                    "answer": "Sorry, no matching LLMs were found. 161"
                },
                "162": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 162",
                    "yes": "325",
                    "no": "326"
                },
                "163": {
                    "answer": "Sorry, no matching LLMs were found. 163"
                },
                "164": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 164",
                    "yes": "329",
                    "no": "330"
                },
                "165": {
                    "answer": "Sorry, no matching LLMs were found. 165"
                },
                "166": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 166",
                    "yes": "333",
                    "no": "334"
                },
                "167": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 167",
                    "yes": "335",
                    "no": "336"
                },
                "168": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 168",
                    "yes": "337",
                    "no": "338"
                },
                "169": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 169",
                    "yes": "339",
                    "no": "340"
                },
                "170": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 170",
                    "yes": "341",
                    "no": "342"
                },
                "171": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 171",
                    "yes": "343",
                    "no": "344"
                },
                "172": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 172",
                    "yes": "345",
                    "no": "346"
                },
                "173": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 173",
                    "yes": "347",
                    "no": "348"
                },
                "174": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 174"
                },
                "175": {
                    "answer": "Sorry, no matching LLMs were found. 175"
                },
                "176": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 176",
                    "yes": "353",
                    "no": "354"
                },
                "177": {
                    "answer": "Sorry, no matching LLMs were found. 177"
                },
                "178": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 178",
                    "yes": "357",
                    "no": "358"
                },
                "179": {
                    "answer": "Sorry, no matching LLMs were found. 179"
                },
                "180": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 180",
                    "yes": "361",
                    "no": "362"
                },
                "181": {
                    "answer": "Sorry, no matching LLMs were found. 181"
                },
                "182": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 182",
                    "yes": "365",
                    "no": "366"
                },
                "183": {
                    "answer": "Sorry, no matching LLMs were found. 183"
                },
                "184": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 184",
                    "yes": "369",
                    "no": "370"
                },
                "185": {
                    "answer": "Sorry, no matching LLMs were found. 185"
                },
                "186": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 186",
                    "yes": "373",
                    "no": "374"
                },
                "187": {
                    "answer": "Sorry, no matching LLMs were found. 187"
                },
                "188": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 188",
                    "yes": "377",
                    "no": "378"
                },
                "189": {
                    "answer": "Sorry, no matching LLMs were found. 189"
                },
                "190": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 190"
                },
                "191": {
                    "answer": "Sorry, no matching LLMs were found. 191"
                },
                "192": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 192",
                    "yes": "385",
                    "no": "386"
                },
                "193": {
                    "answer": "Sorry, no matching LLMs were found. 193"
                },
                "194": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 194",
                    "yes": "389",
                    "no": "390"
                },
                "195": {
                    "answer": "Sorry, no matching LLMs were found. 195"
                },
                "196": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 196",
                    "yes": "393",
                    "no": "394"
                },
                "197": {
                    "answer": "Sorry, no matching LLMs were found. 197"
                },
                "198": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 198",
                    "yes": "397",
                    "no": "398"
                },
                "199": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 199",
                    "yes": "399",
                    "no": "400"
                },
                "200": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 200",
                    "yes": "401",
                    "no": "402"
                },
                "201": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 201",
                    "yes": "403",
                    "no": "404"
                },
                "202": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 202",
                    "yes": "405",
                    "no": "406"
                },
                "203": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 203",
                    "yes": "407",
                    "no": "408"
                },
                "204": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 204",
                    "yes": "409",
                    "no": "410"
                },
                "205": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 205",
                    "yes": "411",
                    "no": "412"
                },
                "206": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 206"
                },
                "207": {
                    "answer": "Sorry, no matching LLMs were found. 207"
                },
                "208": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 208",
                    "yes": "417",
                    "no": "418"
                },
                "209": {
                    "answer": "Sorry, no matching LLMs were found. 209"
                },
                "210": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 210",
                    "yes": "421",
                    "no": "422"
                },
                "211": {
                    "answer": "Sorry, no matching LLMs were found. 211"
                },
                "212": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 212",
                    "yes": "425",
                    "no": "426"
                },
                "213": {
                    "answer": "Sorry, no matching LLMs were found. 213"
                },
                "214": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 214",
                    "yes": "429",
                    "no": "430"
                },
                "215": {
                    "answer": "Sorry, no matching LLMs were found. 215"
                },
                "216": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 216",
                    "yes": "433",
                    "no": "434"
                },
                "217": {
                    "answer": "Sorry, no matching LLMs were found. 217"
                },
                "218": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 218",
                    "yes": "437",
                    "no": "438"
                },
                "219": {
                    "answer": "Sorry, no matching LLMs were found. 219"
                },
                "220": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 220",
                    "yes": "441",
                    "no": "442"
                },
                "221": {
                    "answer": "Sorry, no matching LLMs were found. 221"
                },
                "222": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 222"
                },
                "223": {
                    "answer": "Sorry, no matching LLMs were found. 223"
                },
                "224": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 224",
                    "yes": "449",
                    "no": "450"
                },
                "225": {
                    "answer": "Sorry, no matching LLMs were found. 225"
                },
                "226": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 226",
                    "yes": "453",
                    "no": "454"
                },
                "227": {
                    "answer": "Sorry, no matching LLMs were found. 227"
                },
                "228": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 228",
                    "yes": "457",
                    "no": "458"
                },
                "229": {
                    "answer": "Sorry, no matching LLMs were found. 229"
                },
                "230": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 230",
                    "yes": "461",
                    "no": "462"
                },
                "231": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 231",
                    "yes": "463",
                    "no": "464"
                },
                "232": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 232",
                    "yes": "465",
                    "no": "466"
                },
                "233": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 233",
                    "yes": "467",
                    "no": "468"
                },
                "234": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 234",
                    "yes": "469",
                    "no": "470"
                },
                "235": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 235",
                    "yes": "471",
                    "no": "472"
                },
                "236": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 236",
                    "yes": "473",
                    "no": "474"
                },
                "237": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 237",
                    "yes": "475",
                    "no": "476"
                },
                "238": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 238"
                },
                "239": {
                    "answer": "Sorry, no matching LLMs were found. 239"
                },
                "240": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 240",
                    "yes": "481",
                    "no": "482"
                },
                "241": {
                    "answer": "Sorry, no matching LLMs were found. 241"
                },
                "242": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 242",
                    "yes": "485",
                    "no": "486"
                },
                "243": {
                    "answer": "Sorry, no matching LLMs were found. 243"
                },
                "244": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 244",
                    "yes": "489",
                    "no": "490"
                },
                "245": {
                    "answer": "Sorry, no matching LLMs were found. 245"
                },
                "246": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 246",
                    "yes": "493",
                    "no": "494"
                },
                "247": {
                    "answer": "Sorry, no matching LLMs were found. 247"
                },
                "248": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 248",
                    "yes": "497",
                    "no": "498"
                },
                "249": {
                    "answer": "Sorry, no matching LLMs were found. 249"
                },
                "250": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 250",
                    "yes": "501",
                    "no": "502"
                },
                "251": {
                    "answer": "Sorry, no matching LLMs were found. 251"
                },
                "252": {
                    "question": "Do you prefer to deploy LLM locally rather than in the cloud? 252",
                    "yes": "505",
                    "no": "506"
                },
                "253": {
                    "answer": "Sorry, no matching LLMs were found. 253"
                },
                "254": {
                    "answer": "Sorry, there is not enough information to recommend LLM, please reselect the option. 254"
                },
                "257": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 257"
                },
                "258": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 258"
                },
                "261": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 261"
                },
                "262": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 262"
                },
                "265": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 265"
                },
                "266": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 266"
                },
                "269": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 269"
                },
                "270": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 270"
                },
                "271": {
                    "answer": "Sorry, no matching LLMs were found. 271"
                },
                "272": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 272"
                },
                "273": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  273"
                },
                "274": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 274"
                },
                "275": {
                    "answer": "Sorry, no matching LLMs were found. 275"
                },
                "276": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 276"
                },
                "277": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 277"
                },
                "278": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 278"
                },
                "279": {
                    "answer": "Sorry, no matching LLMs were found. 279"
                },
                "280": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 280"
                },
                "281": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 281"
                },
                "282": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.   282"
                },
                "283": {
                    "answer": "Sorry, no matching LLMs were found. 283"
                },
                "284": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 284"
                },
                "289": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 289"
                },
                "290": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 290"
                },
                "293": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 293"
                },
                "294": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 294"
                },
                "297": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 297"
                },
                "298": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 298"
                },
                "301": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 301"
                },
                "302": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 302"
                },
                "305": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 305"
                },
                "306": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 306"
                },
                "309": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 309"
                },
                "310": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 310"
                },
                "313": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 313"
                },
                "314": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 314"
                },
                "321": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 321"
                },
                "322": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 322"
                },
                "325": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 325"
                },
                "326": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 326"
                },
                "329": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 329"
                },
                "330": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 330"
                },
                "333": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 333"
                },
                "334": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 334"
                },
                "335": {
                    "answer": "Sorry, no matching LLMs were found. 335"
                },
                "336": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 336"
                },
                "337": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  337"
                },
                "338": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 338"
                },
                "339": {
                    "answer": "Sorry, no matching LLMs were found. 339"
                },
                "340": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 340"
                },
                "341": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 341"
                },
                "342": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 342"
                },
                "343": {
                    "answer": "Sorry, no matching LLMs were found. 343"
                },
                "344": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 344"
                },
                "345": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 345"
                },
                "346": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  346"
                },
                "347": {
                    "answer": "Sorry, no matching LLMs were found. 347"
                },
                "348": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 348"
                },
                "353": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  353"
                },
                "354": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 354"
                },
                "357": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 357"
                },
                "358": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 358"
                },
                "361": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 361"
                },
                "362": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 362"
                },
                "365": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 365"
                },
                "366": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 366"
                },
                "369": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 369"
                },
                "370": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 370"
                },
                "373": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 373"
                },
                "374": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 374"
                },
                "377": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 377"
                },
                "378": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 378"
                },
                "385": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 385"
                },
                "386": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 386"
                },
                "389": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 389"
                },
                "390": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 390"
                },
                "393": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 393"
                },
                "394": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 394"
                },
                "397": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 397"
                },
                "398": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 398"
                },
                "399": {
                    "answer": "Sorry, no matching LLMs were found.  399"
                },
                "400": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 400"
                },
                "401": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 401"
                },
                "402": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 402"
                },
                "403": {
                    "answer": "Sorry, no matching LLMs were found. 403"
                },
                "404": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 404"
                },
                "405": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 405"
                },
                "406": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 406"
                },
                "407": {
                    "answer": "Sorry, no matching LLMs were found. 407"
                },
                "408": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 408"
                },
                "409": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 409"
                },
                "410": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 410"
                },
                "411": {
                    "answer": "Sorry, no matching LLMs were found. 411"
                },
                "412": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 412"
                },
                "417": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 417"
                },
                "418": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 418"
                },
                "421": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 421"
                },
                "422": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 422"
                },
                "425": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 425"
                },
                "426": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 426"
                },
                "429": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 429"
                },
                "430": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 430"
                },
                "433": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  433"
                },
                "434": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  434"
                },
                "437": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 437"
                },
                "438": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 438"
                },
                "441": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 441"
                },
                "442": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 442"
                },
                "449": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 449"
                },
                "450": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 450"
                },
                "453": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 453"
                },
                "454": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  454"
                },
                "457": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  457"
                },
                "458": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 458"
                },
                "461": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  461"
                },
                "462": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 462"
                },
                "463": {
                    "answer": "Sorry, no matching LLMs were found.  463"
                },
                "464": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  464"
                },
                "465": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 465"
                },
                "466": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), GPT-3.5, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 466"
                },
                "467": {
                    "answer": "Sorry, no matching LLMs were found. 467"
                },
                "468": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 468"
                },
                "469": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 469"
                },
                "470": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.  470"
                },
                "471": {
                    "answer": "Sorry, no matching LLMs were found. 471"
                },
                "472": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens. 472"
                },
                "473": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 473"
                },
                "474": {
                    "answer": "The LLMs we recommend you use are: GPT-4 (*), DeID-GPT (*), GPT-3.5, BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>DeID-GPT: using GPT-4 api and can be accessed via link <a href='https://github.com/yhydhx/ChatGPT-API' target='_blank'>DeIDGPT GitHub</a>. The input price is $0.01/1k tokens and the output price is $0.03/1k tokens.<br><br>GPT-3.5: the input price is $0.0005/1k tokens and the output price is $0.0015/1k tokens.<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.  474"
                },
                "475": {
                    "answer": "Sorry, no matching LLMs were found.  475"
                },
                "476": {
                    "answer": "The LLM we recommend you use is: GPT-4. Here are the required resources:<br>  <br>GPT-4: the input price is $0.01/1k tokens and the output price is $0.03/1k tokens.  476"
                },
                "481": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 481"
                },
                "482": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 482"
                },
                "485": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 485"
                },
                "486": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 486"
                },
                "489": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 489"
                },
                "490": {
                    "answer": "The LLM we recommend you use is: ClinicalBERT-110M. Here are the required resources:<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 490"
                },
                "493": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The total thermal design power of the GPUs is 1500W and the total price is $12588.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The total thermal design power of the GPUs is 300W and the total price is $4680.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 2000W and the total price is $55992. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The total thermal design power of the GPUs is 500W and the total price is $13998. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 493"
                },
                "494": {
                    "answer": "The LLMs we recommend you use are: Clinical-longformer-150M (*), DRG-LLaMA-13B (*), DRG-LLaMA-7B, ClinicalBERT-110M. Here are the required resources:<br> <br>Clinical-longformer-150M: pre-train requires 6 NVIDIA TESLA V100-32GB GPUs. The price of renting 6 this type of GPU using cloud services is $29.76/1h.<br><br>DRG-LLaMA-13B: fine-tuning requires 1 NVIDIA RTX A6000-48GB GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h.<br><br>DRG-LLaMA-7B: no specific resource requirements for this model, refer to resources for LLMs of similar size (7B). Pre-train requires 8 NVIDIA TESLA A100-40GB GPUs. The price of renting 8 this type of GPU using cloud services is $32.4/1h. Fine-tuning requires 2 NVIDIA TESLA A100-40GB GPUs. The price of renting 2 this type of GPU using cloud services is $8.1/1h. Inference requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 494"
                },
                "497": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600. 497"
                },
                "498": {
                    "answer": "The LLMs we recommend you use are: ClinicalBERT-110M. Here are the required resources:<br><br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 498"
                },
                "501": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The total thermal design power of the GPU is 140W and the total price is $1499.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The total thermal design power of the GPU is 295W and the total price is $4300. 501"
                },
                "502": {
                    "answer": "The LLMs we recommend you use are: Flan-T5-11B (*), BioClinRoBERTa-345M (*), Flan-T5-2.7B (*). Here are the required resources:<br><br>Flan-T5-11B: Inference requires 1 NVIDIA RTX A4000-16GB GPU. The price of renting a GPU with 16GB of memory using a cloud service is $1.25/1h.<br><br>BioClinRoBERTa-345M: no specific resource requirements for this model, refer to resources for LLMs of similar size (406M). Fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br><br>Flan-T5-2.7B: fine-tuning requires 1 NVIDIA Quadro RTX 8000 GPU. The price of renting a GPU with 48GB of memory using a cloud service is $3.75/1h. 502"
                },
                "505": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999. Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The total thermal design power of the GPU is 250W and the total price is $6999.<br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The total thermal design power of the GPU is 350W and the total price is $1600.  505"
                },
                "506": {
                    "answer": "The LLMs we recommend you use are: BERT, RoBERTa, ClinicalBERT-110M. Here are the required resources:<br><br>BERT: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br><br>RoBERTa: fine-tuning requires 1 NVIDIA TESLA A100-40GB GPU. The price of renting 1 this type of GPU using cloud services is $4.05/1h.<br> <br>Inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h.<br> <br>ClinicalBERT-110M: inference requires 1 NVIDIA GTX 3090-24GB GPU. The price of renting a GPU with 24GB of memory using a cloud service is $1.88/1h. 506"
                }
            }
            ;

            let currentNode = "0"; // Starting node
            let questionHistory = []; // Keeps track of answered questions

            // Function to display a question based on the node
            function displayQuestion(node, questionIndex) {
                const questionContainer = document.getElementById('questionContainer');
                // Clear questions beyond this index
                questionContainer.innerHTML = '';

                // Display history
                questionHistory.slice(0, questionIndex).forEach(q => appendQuestion(q.node, q.choice, q.number));

                // Display the current question
                appendQuestion(node, null, questionIndex + 1);
            }

            function appendQuestion(node, selectedChoice, questionNumber) {
                const questionContainer = document.getElementById('questionContainer');

                if (data[node] && data[node].question) {
                    const questionDiv = document.createElement('div');
                    questionDiv.classList.add('question');

                    // Display fixed question number
                    const questionNumberElem = document.createElement('p');
                    questionNumberElem.classList.add('questionNumber');
                    questionNumberElem.innerText = `Question ${questionNumber}:`;
                    questionDiv.appendChild(questionNumberElem);

                    // Display question text
                    const questionText = document.createElement('p');
                    questionText.innerText = data[node].question;
                    questionDiv.appendChild(questionText);

                    // Create Yes button
                    const yesButton = document.createElement('button');
                    yesButton.innerText = 'Yes';
                    yesButton.classList.toggle('selected', selectedChoice === 'yes');
                    yesButton.onclick = function () {
                        updateHistory(node, 'yes', questionNumber);
                        currentNode = data[node].yes;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(yesButton);

                    // Create No button
                    const noButton = document.createElement('button');
                    noButton.innerText = 'No';
                    noButton.classList.toggle('selected', selectedChoice === 'no');
                    noButton.onclick = function () {
                        updateHistory(node, 'no', questionNumber);
                        currentNode = data[node].no;
                        displayQuestion(currentNode, questionNumber);
                    };
                    questionDiv.appendChild(noButton);

                    // Append the question block to the container
                    questionContainer.appendChild(questionDiv);

                } else if (data[node] && data[node].answer) {
                    // Create a div for the final answer
                    const answerDiv = document.createElement('div');
                    answerDiv.classList.add('answer');

                    const answerText = document.createElement('p');
                    answerText.innerHTML = data[node].answer;  // Allow HTML content for line breaks and links
                    answerDiv.appendChild(answerText);

                    // Append the answer to the container
                    questionContainer.appendChild(answerDiv);
                } else {
                    // Handle if no more questions or no data found
                    const noMoreQuestions = document.createElement('p');
                    noMoreQuestions.innerText = "No more questions or answers.";
                    questionContainer.appendChild(noMoreQuestions);
                }
            }

            function updateHistory(node, choice, questionNumber) {
                // Check if we are revisiting a question and modify history accordingly
                const historyIndex = questionHistory.findIndex(q => q.number === questionNumber);
                if (historyIndex !== -1) {
                    questionHistory = questionHistory.slice(0, historyIndex); // Remove subsequent questions
                }

                // Add current question to history
                questionHistory.push({ node, choice, number: questionNumber });
            }

            // Start the first question
            displayQuestion(currentNode, 0);
        });
    </script>
</body>
</html>
